Language of Original Document,Document Type,Year,Authors,Title,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Source,EID
English,Conference Paper,2019,"Anu H., Chen J., Shi W., Hou J., Liang B., Qin B.",An Approach to Recommendation of Verbosity Log Levels Based on Logging Intention,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",,,8919094,125,134,,2,10.1109/ICSME.2019.00022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077183129&doi=10.1109%2fICSME.2019.00022&partnerID=40&md5=da69162b617f17c6a06097724bed0318,"Verbosity levels of logs are designed to discriminate highly diverse runtime events, which facilitates system failure identification through simple keyword search (e.g., fatal, error). Verbosity levels should be properly assigned to logging statements, as inappropriate verbosity levels would confuse users and cause a lot of redundant maintenance effort. However, to achieve such a goal is not an easy task due to the lack of practical specifications and guidelines towards verbosity log level usages. The existing research has built a classification model on log related quantitative metrics such as log density to improve logging level practice. Though such quantitative metrics can reveal logging characteristics, their contributions on logging level decision are limited, since valuable logging intention information buried in logging code context can not be captured. In this paper, we propose an automatic approach to help developers determine the appropriate verbosity log levels. More specially, our approach discriminates different verbosity log level usages based on code context features that contain underlying logging intention. To validate our approach, we implement a prototype tool, VerbosityLevelDirector, and perform a case study to measure its effectiveness on four well-known open source software projects. Evaluation results show that VerbosityLevelDirector achieves high performance on verbosity level discrimination and outperforms the baseline approaches on all those projects. Furthermore, through applying noise handling technique, our approach can detect previously unknown inappropriate verbosity level configurations in the code repository. We have reported 21 representative logging level errors with modification advice to issue tracking platforms of the examined software projects and received positive feedback from their developers. The above results confirm that our work can help developers make a better logging level decision in real-world engineering. © 2019 IEEE.",Scopus,2-s2.0-85077183129
English,Article,2021,"Duan X., Ying S., Yuan W., Cheng H., Yin X.",QLLog: A log anomaly detection method based on Q-learning algorithm,Information Processing and Management,58,3,102540,,,,,10.1016/j.ipm.2021.102540,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100679368&doi=10.1016%2fj.ipm.2021.102540&partnerID=40&md5=79b97a1a76a88de241fd50e64eb24abb,"Most of the existing log anomaly detection methods suffer from scalability and numerous false positives. Besides, they cannot rank the severity level of abnormal events. This paper proposes a log anomaly detection based on Q-learning, namely QLLog, which can detect multiple types of system anomalies and rank the severity level of abnormal events. We first build a mathematical model of log anomaly detection, proving that log anomaly detection is a sequential decision problem. Second, we use the Q-learning algorithm to build the core of the anomaly detection model. This allows QLLog to automatically learn directed acyclic graph log patterns from normal execution and adjust the training model according to the reward value. Then, QLLog combines the advantages of the Q-learning algorithm and the specially designed rules to detect anomalies when log patterns deviate from the model trained from log data under normal execution. Besides, we provide a feedback mechanism and build an abnormal level table. Therefore, QLLog can adapt to new log states and log patterns. Experiments on real datasets show that the method can quickly and effectively detect system anomalies. Compared with the state of the art, QLLog can detect numerous real problems with high accuracy 95%, and its scalability outperforms other existing log-based anomaly detection methods. © 2021 Elsevier Ltd",Scopus,2-s2.0-85100679368
English,Article,2015,"Tak S.H., Zhang H., Hong S.H.",Preferred computer activities among individuals with dementia: A pilot study,Journal of Gerontological Nursing,41,3,,50,57,,6,10.3928/00989134-20141029-01,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927784660&doi=10.3928%2f00989134-20141029-01&partnerID=40&md5=dd23421825dcb7f3cf621e4a6f7cd009,"Computers offer new activities that are easily accessible, cognitively stimulating, and enjoyable for individuals with dementia. The current descriptive study examined preferred computer activities among nursing home residents with different severity levels of dementia. A secondary data analysis was conducted using activity observation logs from 15 study participants with dementia (severe = 115 logs, moderate = 234 logs, and mild = 124 logs) who participated in a computer activity program. Significant differences existed in preferred computer activities among groups with different severity levels of dementia. Participants with severe dementia spent significantly more time watching slide shows with music than those with both mild and moderate dementia (F [2,12] = 9.72, p = 0.003). Preference in playing games also differed significantly across the three groups. It is critical to consider individuals' interests and functional abilities when computer activities are provided for individuals with dementia. A practice guideline for tailoring computer activities is detailed. © SLACK Incorporated.",Scopus,2-s2.0-84927784660
Chinese,Article,2009,"Jing X., Huang W.-J., Wang J.-H., Wang J.-D., Wang K.-R.",Hyperspectral inversion models on verticillium wilt severity of cotton leaf,Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis,29,12,,3348,3352,,4,10.3964/j.issn.1000-0593(2009)12-3348-05,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449203129&doi=10.3964%2fj.issn.1000-0593%282009%2912-3348-05&partnerID=40&md5=e30da53fd979f22ce8dca0c9542e6636,"The correlation of cotton leaf verticillium wilt severity level with raw hyperspectral reflectance, first derivative hyperspectral reflectance, and hyperspectral characteristic parameters was analyzed. Using linear and nonlinear regression methods, the hyperspectral remote sensing retrieval models of verticillium wilt severity level with remote sensing parameters as independent variables were constructed and validated. The result showed that spectral reflectance increased significantly in visible and short infrared wave band with the increase in the severity level, and this is especially significant in visible band. The raw spectral reflectance has the maximum coefficient of determination at 694 nm (R2=0.4616) with severity level and the logarithm model constructed with reflectance at this point is the better one as compared to linear model. By the precision evaluation of retrieval models, the linear model with the first derivative reflectance at 717 nm as independent variable was proved to be the best, with R2=0.4889, RMSE=0.2571, and relative error=12.74%, for the estimation of verticllium wilt severity level of cotton leaf. The results provide a good basis for further studying monitoring mechanism of cotton verticillium wilt by remote sensing data, and have an important application in acquiring cotton disease information using hyperspectral remote sensing.",Scopus,2-s2.0-72449203129
English,Conference Paper,2019,"Rochim A.F., Aziz M.A., Fauzi A.",Design Log Management System of Computer Network Devices Infrastructures Based on ELK Stack,"ICECOS 2019 - 3rd International Conference on Electrical Engineering and Computer Science, Proceeding",,,8984494,338,342,,4,10.1109/ICECOS47637.2019.8984494,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084291687&doi=10.1109%2fICECOS47637.2019.8984494&partnerID=40&md5=b807f9dfb015114bcf29a02a3812ecaf,"Device monitoring is an important thing to manage networks. Information-related network state or condition can be gathered through the device monitoring for administrators to take decisions regarding occurred events. Logs can be useful information to monitor network devices. Network administrator of Diponegoro University needs a centralized the logs, so that can receive, manage, and analyze logs. This research identifies functional requirements of the log management system. DSR Method was used to design topology and software of the log management system. The next step is implementation of the topology, software, and application. The last step is testing the system and log management application. The results show that collecting centralized logs and processing these logs into information in the form of dashboards using ELK Stack application successfully implemented. The dashboard resulted by ELK Stack Application will be implemented on the web application using PHP programming language and Code Igniter framework. The test results show that system can receive logs and group the log according to the device location and the severity level of the log. © 2019 IEEE.",Scopus,2-s2.0-85084291687
English,Conference Paper,2017,"Tran H.M., Van Nguyen S., Le S.T., Vu Q.T.",Applying data analytic techniques for fault detection,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10140 LNCS,,,30,46,,,10.1007/978-3-662-54173-9_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009734030&doi=10.1007%2f978-3-662-54173-9_2&partnerID=40&md5=cd8e18d4984ad55743117de7223aee0f,"Monitoring events in communication and computing systems becomes more and more challenging due to the increasing complexity and diversity of these systems. Several supporting tools have been created to assist system administrators in monitoring an enormous number of events daily. The main function of these tools is to filter as many as possible events and present highly suspected events to the administrators for fault analysis, detection and report. While these suspected events appear regularly on large and complex systems, such as cloud computing systems, analyzing them consumes much time and effort. In this study, we propose an approach for evaluating the severity level of events using a classification decision tree. The approach exploits existing fault datasets and features, such as bug reports and log events to construct a decision tree that can be used to classify the severity level of other events. The administrators refer to the result of classification to determine proper actions for the suspected events with a high severity level. We have implemented and experimented the approach for various bug report and log event datasets. The experimental results reveal that the accuracy of classifying severity levels by using the decision trees is above 80%, and some detailed analyses are also provided. © Springer-Verlag GmbH Germany 2017.",Scopus,2-s2.0-85009734030
English,Conference Paper,2020,"Kim T., Kim S., Park S., Park Y.",Automatic recommendation to appropriate log levels,Software - Practice and Experience,50,3,,189,209,,,10.1002/spe.2771,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075740067&doi=10.1002%2fspe.2771&partnerID=40&md5=e9fbcad602c176e3ce952b84f6b60531,"A log statement is one of the key tactics for a developer to record and monitor important run-time behaviors of our system in a development phase and a maintenance phase. It composes of a message for stating log contents, and a log level (eg, debug or warn) to denote the severity of a message and controlling its visibility at run time. In spite of its usefulness, a developer does not tend to deeply consider which log level is appropriate in writing source code, which causes the system to be unmaintainable. To address this issue, this paper proposes an automatic approach to validating the appropriateness of the log level in consideration of the semantic and syntactic features and recommending a proper alternative log level. We first build the semantic feature vector to quantify the semantic similarity among application log messages using the word vector space, and the syntactic feature vector to capture the application context that surrounds the log statement. Based on the feature vectors and machine learning techniques, the log level is automatically validated, and an alternative log level is recommended if the log level is invalid. For the evaluation, we collected 22 open-source projects from three application domains, and obtained the 77% of precision and 75% of recall in validating the log levels. Also, our approach showed 6% higher accuracy than that of the developer group who has 7 to 8 years of work experience, and 72% of the developers accepted our recommendation. © 2019 John Wiley & Sons, Ltd.",Scopus,2-s2.0-85075740067
English,Article,2016,"Bradley C.M., Hanson C.T., DellaSala D.A.",Does increased forest protection correspond to higher fire severity in frequent-fire forests of the western United States?,Ecosphere,7,10,e01492,,,,12,10.1002/ecs2.1492,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993981966&doi=10.1002%2fecs2.1492&partnerID=40&md5=fb377e19fa584a6fd089d0a78a1ee184,"There is a widespread view among land managers and others that the protected status of many forestlands in the western United States corresponds with higher fire severity levels due to historical restrictions on logging that contribute to greater amounts of biomass and fuel loading in less intensively managed areas, particularly after decades of fire suppression. This view has led to recent proposals-both administrative and legislative-to reduce or eliminate forest protections and increase some forms of logging based on the belief that restrictions on active management have increased fire severity. We investigated the relationship between protected status and fire severity using the Random Forests algorithm applied to 1500 fires affecting 9.5 million hectares between 1984 and 2014 in pine (Pinus ponderosa, Pinus jeffreyi) and mixed-conifer forests of western United States, accounting for key topographic and climate variables. We found forests with higher levels of protection had lower severity values even though they are generally identified as having the highest overall levels of biomass and fuel loading. Our results suggest a need to reconsider current overly simplistic assumptions about the relationship between forest protection and fire severity in fire management and policy. © 2016 Bradley et al.",Scopus,2-s2.0-84993981966
English,Book Chapter,2013,Ruvalcaba C.,Security information and event management systems ... a need in the real world,Advances in Security Information Management: Perceptions and Outcomes,,,,1,26,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891962783&partnerID=40&md5=ff432420267448c45b914e61eccfa9a4,"Security Information Event Management (SIEM) systems are becoming morecommonplace in the corporate world. Generally, they are deployed in a plug-n-playfashion and used for fulfilling auditory requirements. Though each organization is different,there are common points and processes that can be followed in order to ensurethe proper use and maximum return on investment in the acquired tools, being theSIEM devices central points for the analytical tasks.Basically, SIEM devices collect logs from a series of the security appliances andapplications deployed over the organization's network. Amongst their numerous advantages,SIEM devices have the ability to aggregate the reported logs in correlationwith each other in order to assist the analyst in determining which of the securityevents should take a higher priority. The establishment of this priority is mostly basedon the nature, source and severity level of the events. Apart from the logs reportedfrom dedicated security appliances, logs from any other system and application arealso sent to SIEM collectors to reach a more complete snapshot of the current state ofthe environment. The correlation of these logs provides higher reliability and accuracyof the events, and potentially decrease the number of security incidents. © 2013 Nova Science Publishers, Inc.",Scopus,2-s2.0-84891962783
English,Conference Paper,2018,"Kim T., Kim S., Yoo C.-J., Cho S., Park S.",An Automatic Approach to Validating Log Levels in Java,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2018-December,,8719571,623,627,,3,10.1109/APSEC.2018.00078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066785090&doi=10.1109%2fAPSEC.2018.00078&partnerID=40&md5=2df919270f524c63387aa64a25095511,"A log statement is used to record important runtime behavior of software systems for diverse reasons, which is inevitable to develop most of the software systems. However, developers do not tend to deeply consider an appropriate log level in their source code. In order to address the issues, this paper proposes an automatic approach to validating log levels in Java in consideration of the syntactic as well as semantic features. We first build up the Word2Vec model and generate semantic and syntactic log feature vectors, then train the machine learning classifiers to automatically validate the log levels. For the evaluation, we collected six open source projects of the message-oriented middleware domain, and obtained the 88% precision and the 87% recall respectively. © 2018 IEEE.",Scopus,2-s2.0-85066785090
English,Article,2017,"Li H., Shang W., Hassan A.E.",Which log level should developers choose for a new logging statement?,Empirical Software Engineering,22,4,,1684,1716,,31,10.1007/s10664-016-9456-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991112488&doi=10.1007%2fs10664-016-9456-2&partnerID=40&md5=43f22548181d349dca9e572f05850a5a,"Logging statements are used to record valuable runtime information about applications. Each logging statement is assigned a log level such that users can disable some verbose log messages while allowing the printing of other important ones. However, prior research finds that developers often have difficulties when determining the appropriate level for their logging statements. In this paper, we propose an approach to help developers determine the appropriate log level when they add a new logging statement. We analyze the development history of four open source projects (Hadoop, Directory Server, Hama, and Qpid), and leverage ordinal regression models to automatically suggest the most appropriate level for each newly-added logging statement. First, we find that our ordinal regression model can accurately suggest the levels of logging statements with an AUC (area under the curve; the higher the better) of 0.75 to 0.81 and a Brier score (the lower the better) of 0.44 to 0.66, which is better than randomly guessing the appropriate log level (with an AUC of 0.50 and a Brier score of 0.80 to 0.83) or naively guessing the log level based on the proportional distribution of each log level (with an AUC of 0.50 and a Brier score of 0.65 to 0.76). Second, we find that the characteristics of the containing block of a newly-added logging statement, the existing logging statements in the containing source code file, and the content of the newly-added logging statement play important roles in determining the appropriate log level for that logging statement. © 2016, Springer Science+Business Media New York.",Scopus,2-s2.0-84991112488
English,Article,2009,"Aguero-Valverde J., Jovanis P.P.",Bayesian multivariate Poisson lognormal models for crash severity modeling and site ranking,Transportation Research Record,,2136,,82,91,,101,10.3141/2136-10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76749111608&doi=10.3141%2f2136-10&partnerID=40&md5=f59adb6d247ee2b30d2f598465cb793c,"Traditionally, highway safety analyses have used univariate Poisson or negative binomial distributions to model crash counts for different levels of crash severity. Because unobservables or omitted variables are shared across severity levels, however, crash counts are multivariate in nature. This research uses full Bayes multivariate Poisson lognormal models to estimate the expected crash frequency for different levels of crash severity and then compares those estimates to independent or univariate Poisson lognormal estimates. The multivariate Poisson lognormal model fits better than the univariate model and improves the precision in crash-frequency estimates. The covariances and correlations among crash severities are high (correlations range from 0.47 to 0.97), with the highest values found between contiguous severity levels. Considering this correlation between severity levels improves the precision of the expected number of crashes. The multivariate estimates are used with cost data from the Pennsylvania Department of Transportation to develop the expected crash cost (and excess expected cost) per segment, which is then used to rank sites for safety improvements. The multivariate-based top-ranked segments are found to have consistently higher costs and excess costs than the univariate estimates, which is due to higher multivariate estimates of fatalities and major injuries (due to the random effects parameter). These higher estimated frequencies, in turn, produce different rankings for the multivariate and independent models. The finding of a high correlation between contiguous severity levels is consistent with some of the literature, but additional tests of multivariate models are recommended. The improved precision has important implications for the identification of sites with promise (SWiPs), because one formulation includes the standard deviation of crash frequencies for similar sites as part of the assessment of SWiPs.",Scopus,2-s2.0-76749111608
English,Conference Paper,2018,"Li H., Shang W., Hassan A.E.",Which log level should developers choose for a new logging statement? (journal-first abstract),"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings",2018-March,,8330234,468,,,,10.1109/SANER.2018.8330234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050964552&doi=10.1109%2fSANER.2018.8330234&partnerID=40&md5=c683abc445b36b80d5db7f78e5a02ba0,"This is an extended abstract of a paper published in the Empirical Software Engineering journal. The original paper is communicated by Mark Grechanik. The paper empirically studied how developers assign log levels to their logging statements and proposed an automated approach to help developers determine the most appropriate log level when they add a new logging statement. We analyzed the development history of four open source projects (Hadoop, Directory Server, Hama, and Qpid). We found that our automated approach can accurately suggest the levels of logging statements with an AUC of 0.75 to 0.81. We also found that the characteristics of the containing block of a newly-added logging statement, the existing logging statements in the containing source code file, and the content of the newly-added logging statement play important roles in determining the appropriate log level for that logging statement. © 2018 IEEE.",Scopus,2-s2.0-85050964552
English,Conference Paper,1983,"Yow Jr. Jesse L., Wilder Dale G.",PLANNING EXPLORATORY DRILLING: THE EFFECT OF BLIND ZONES AND LEVEL OF LOGGING EFFORT.,Proceedings - Symposium on Rock Mechanics,,,,807,812,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020504483&partnerID=40&md5=cdcf5ba4cf688f7486ea4594d0eafb26,[No abstract available],Scopus,2-s2.0-0020504483
English,Article,2012,"Lan B., Persaud B.",Evaluation of Multivariate Poisson Log Normal Bayesian Methods for Before-After Road Safety Evaluations,Journal of Transportation Safety and Security,4,3,,193,210,,13,10.1080/19439962.2011.649194,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864647768&doi=10.1080%2f19439962.2011.649194&partnerID=40&md5=4950f24b6aaa4eb60e87057b002efc51,"The multivariate Poisson log normal (MVPLN) Bayesian method has recently been introduced in road safety analysis mainly for network screening and using different crash severity levels. However, there is little or no research applying MVPLN to different crash types. Besides, only one model structure for the expected crashes of a given severity was investigated in previous MVPLN studies. Another knowledge gap is that this method has not yet been evaluated for before-after treatment effect analysis. The objective of this study was to evaluate the application of MVPLN Bayesian method for before-after road safety evaluation studies. Two groups of unsignalized California intersections, for which a naive before-after comparison shows a significant change in the crash frequency after a hypothetical treatment was assigned, were used to conduct the study. It was found that the crash reduction rates are sensitive to the function form of expected crashes. For each model structure, MVPLN, univariate Poisson log normal (PLN) and Poisson-gamma models provided comparable results while PLN was seen to be superior. Finally, models that consider temporal effects of unobserved latent variables were found to be superior to those that don't. © 2012 Copyright Taylor and Francis Group, LLC.",Scopus,2-s2.0-84864647768
English,Article,2017,"Zhou A., Wang K., Zhang H.",Human factor risk control for oil and gas drilling industry,Journal of Petroleum Science and Engineering,159,,,581,587,,6,10.1016/j.petrol.2017.09.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041527013&doi=10.1016%2fj.petrol.2017.09.034&partnerID=40&md5=28aa6ebfd569a0567c8557c027e5a9ab,"To control the human factor risk for oil and gas drilling industry, the types of work activities involved in drilling were identified, the major accident risk types of human errors were selected, the probability and severity levels of the risks associated with each activity were quantified, and the human factor risk levels of major work activities involved in drilling processes were calculated. Quantitative results demonstrate that the highest risks during the drilling process include being struck by an object, injury by machine, and overexertion; conversely, the lowest risks include exposure to harmful substances or environments and fire and explosion. The highest risk activities are associated with, for instance, penetration drilling, the trip in and out, and the hoisting and lifting operations. Conversely, the relative lowest risk activities include auxiliary operations, well logging and mud logging, equipment inspection and maintenance. The total risk value of all main drilling activities is 11.3947 S/w-h. Twelve risk control measures were selected, and the Delphi method was taken to quantify the human factor risk mitigation capacity resulting from the selected risk control measures. The results demonstrate that the human risk control measures exist in three tiers of effectiveness, with each tier being separated by nearly an order of magnitude. The types of most risk mitigation are risk codes, such as injury by machine, fall from height, and contact with harmful material. The most minimal risk mitigation types include muscle strain and fire and explosion. Finally, these data will be valuable for drilling companies to strategically allocate limited resources to their safety management plan, and they can evaluate the expected effectiveness resulting from risk control measures. © 2017",Scopus,2-s2.0-85041527013
English,Conference Paper,2013,"Zhou A.T., Wang K., Zhang H.",Quantitative study on the risk level of human factor for drilling process,Advanced Materials Research,774-776,,,2021,2024,,,10.4028/www.scientific.net/AMR.774-776.2021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884955203&doi=10.4028%2fwww.scientific.net%2fAMR.774-776.2021&partnerID=40&md5=335284b1b5e36ae64eea775daf15708f,"In order to quantifying human factor initial risk level in drilling process, the types of work activities involved in drilling were identified, the major accident risk types of human errors were selected, the probability and severity levels of risks associated with each activity were quantified, and the human factor risk level of major work activities involved in drilling process were calculated. Quantitative results show that the relatively highest risk types during drilling process are struck by object (risk value is 3.171 S/w-h), injury by machine (risk value is 2.816 S/w-h), overexertion (risk value is 1.501 S/w-h), etc. Contrarily, the relatively lowest risk types are exposure to harmful substances or environments (0.737 S/w-h), fire and explosion (0.2750S/w-h), and others (0.2261 S/w-h), etc. The relatively highest risk activities are associated with penetration drilling (risk value is 2.7475S/w-h), trip in and out (risk value is 2.0206 S/w-h), and hoisting and lifting operation (risk value is 1.7064 S/w-h), etc. Contrarily, the relatively lowest risk activities are auxiliary operations (risk value is 0.3706S/w-h), well logging and mud logging (risk value is 0.205 S/w-h), equipment inspection and maintenance (risk value is 0.1510 S/w-h), etc. The total risk value of all main drilling activities is 11.347 S/w-h. © (2013) Trans Tech Publications, Switzerland.",Scopus,2-s2.0-84884955203
English,Conference Paper,2019,"Mizouchi T., Shimari K., Ishio T., Inoue K.",PADLA: A dynamic log level adapter using online phase detection,IEEE International Conference on Program Comprehension,2019-May,,8813251,135,138,,1,10.1109/ICPC.2019.00029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072306623&doi=10.1109%2fICPC.2019.00029&partnerID=40&md5=72dbcc22209d2cd3b3e34ccc5a6a560a,"Logging is an important feature for a software system to record its run-time information. Although detailed logs are helpful to identify the cause of a failure in a program execution, constantly recording detailed logs of a long-running system is challenging because of its performance overhead and storage cost. To solve the problem, we propose PADLA (Phase-Aware Dynamic Log Level Adapter) that dynamically adjusts the log level of a running system so that the system can record irregular events such as performance anomalies in detail while recording regular events concisely. PADLA is an extension of Apache Log4j, one of the most popular logging framework for Java. It employs an online phase detection algorithm to recognize irregular events. It monitors run-time performance of a system and learns regular execution phases of a program. If it recognizes a performance anomalies, it automatically changes the log level of a system to record the detailed behavior. In the case study, PADLA successfully recorded a detailed log for performance analysis of a server system under high load while suppressing the amount of log data and performance overhead. © 2019 IEEE.",Scopus,2-s2.0-85072306623
English,Conference Paper,2018,"Blakely W.F., Bolduc D.L., Debad J., Sigal G., Port M., Abend M., Valente M., Drouet M., Hérodin F.",Use of Proteomic and Hematology Biomarkers for Prediction of Hematopoietic Acute Radiation Syndrome Severity in Baboon Radiation Models,Health Physics,115,1,,29,36,,15,10.1097/HP.0000000000000819,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048030537&doi=10.1097%2fHP.0000000000000819&partnerID=40&md5=7b39a13f93daa53afc863a3200ee1f65,"Use of plasma proteomic and hematological biomarkers represents a promising approach to provide useful diagnostic information for assessment of the severity of hematopoietic acute radiation syndrome. Eighteen baboons were evaluated in a radiation model that underwent total-body and partial-body irradiations at doses of 60Co gamma rays from 2.5 to 15 Gy at dose rates of 6.25 cGy min-1 and 32 cGy min-1. Hematopoietic acute radiation syndrome severity levels determined by an analysis of blood count changes measured up to 60 d after irradiation were used to gauge overall hematopoietic acute radiation syndrome severity classifications. A panel of protein biomarkers was measured on plasma samples collected at 0 to 28 d after exposure using electrochemiluminescence-detection technology. The database was split into two distinct groups (i.e., ""calibration,"" n = 11; ""validation,"" n = 7). The calibration database was used in an initial stepwise regression multivariate model-fitting approach followed by down selection of biomarkers for identification of subpanels of hematopoietic acute radiation syndrome-responsive biomarkers for three time windows (i.e., 0-2 d, 2-7 d, 7-28 d). Model 1 (0-2 d) includes log C-reactive protein (p &lt; 0.0001), log interleukin-13 (p &lt; 0.0054), and procalcitonin (p &lt; 0.0316) biomarkers; model 2 (2-7 d) includes log CD27 (p &lt; 0.0001), log FMS-related tyrosine kinase 3 ligand (p &lt; 0.0001), log serum amyloid A (p &lt; 0.0007), and log interleukin-6 (p &lt; 0.0002); and model 3 (7-28 d) includes log CD27 (p &lt; 0.0012), log serum amyloid A (p &lt; 0.0002), log erythropoietin (p &lt; 0.0001), and log CD177 (p &lt; 0.0001). The predicted risk of radiation injury categorization values, representing the hematopoietic acute radiation syndrome severity outcome for the three models, produced least squares multiple regression fit confidences of R2 = 0.73, 0.82, and 0.75, respectively. The resultant algorithms support the proof of concept that plasma proteomic biomarkers can supplement clinical signs and symptoms to assess hematopoietic acute radiation syndrome risk severity. © 2018 Lippincott Williams & Wilkins.",Scopus,2-s2.0-85048030537
English,Article,2006,"Nutter Jr. F.W., Esker P.D.",The role of psychophysics in phytopathology: The Weber-Fechner law revisited,European Journal of Plant Pathology,114,2,,199,213,,63,10.1007/s10658-005-4732-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646452125&doi=10.1007%2fs10658-005-4732-9&partnerID=40&md5=3fa5343e5396993538dcaf279c595389,"The accuracy and precision of disease severity assessment data might be improved if there was a better understanding of how the laws of psychophysics actually relate to the theory and practice of phytopathometry. In this regard, we utilized a classical method developed in the field of psychophysics (the method of comparison stimuli) to test Horsfall and Barratt's claim that raters cannot accurately discriminate disease severity levels between 25% and 50% because, according to the Weber-Fechner law, visual acuity is proportional to the logarithm of the intensity of the stimulus. We show for two pathosystems, wheat leaf rust and grapevine downy mildew, that raters can accurately discriminate disease severity levels between 25% and 50%, and that although Weber's law appears to hold true, Fechner's law does not. Furthermore, based upon our results, the relationship between actual (true) disease severity (X) and disease severity estimated by raters (F) is linear, not logarithmic as proposed by Horsfall and Barratt. © Springer 2006.",Scopus,2-s2.0-33646452125
English,Conference Paper,2020,"Haji A., Tisdale C., Muslem M., Abouzaid A.",Quantification and correction of lateral motion effects on NMR logging while drilling,"Society of Petroleum Engineers - Abu Dhabi International Petroleum Exhibition and Conference 2020, ADIP 2020",,,,,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097533840&partnerID=40&md5=25abf14b56c6a5dfc81b3d4269fa53fc,"Monitoring downhole drilling dynamics is an essential element to quality check the measurements provided in logging while drilling (LWD). LWD nuclear magnetic resonance (NMR) is sensitive to the motion induced from the bottom-hole assembly (BHA) during drilling, therefore, quantifying the motion effects becomes important to understand how to correct the measurements. Quantification and correction of lateral motion effects on NMR LWD are the objectives of this paper. Data was collected from various BHA combinations with LWD NMR to model the responses, which were compared with actual downhole conditions to assess the need for the lateral motion correction (LMC). Vibrational assessment criteria are utilized to assign a severity level, which dictates the level of the LMC. The LMC applies an algorithm to differentiate true formation signal responses from the vibration signal response. Specifically, the motion effect function was integrated into the forward matrix of the NMR joint inversion, and a nonlinear optimization algorithm was used to determine the four motion parameters, and if present, compensate for lateral motion effects. In wellbores with severe motion vibration there were large discrepancies between real-time and memory data, which resulted in mismatches with the measured partial porosities. Investigations were conducted on the BHA design, well trajectories, and wellbore environment to quantify the lateral motion effect on the NMR measurement. This information was then compiled to incorporate all aspects of BHA design techniques to mitigate the lateral motion effects on the NMR measurement. The LMC algorithm gives added confidence to ensure all data collected is consistent and reliable even in more challenging wellbore environments, which could be subjected to unanticipated lateral motion. This paper highlights an approach to integrate BHA simulation principles to anticipate severe motion effects during drilling. This knowledge, coupled with the LMC, creates a platform to enhance NMR data quality. © 2020, Society of Petroleum Engineers",Scopus,2-s2.0-85097533840
English,Article,2016,"Kluch S.P., Vaux A.",The non-random nature of terrorism: An exploration of where and how global trends of terrorism have developed over 40 years,Studies in Conflict and Terrorism,39,12,,1031,1049,,5,10.1080/1057610X.2016.1159070,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964555199&doi=10.1080%2f1057610X.2016.1159070&partnerID=40&md5=6e31114dd2d22e15ab963fe9e49e5135,"We examined the geographic concentration and persistence of terrorism using the Global Terrorism Database (GTD). The GTD logs all terrorist incidents worldwide using open-source media, and, for 1970-2013, includes over 125,000 incidents from over 200 countries and territories. We examined regional and country-level data; different terrorism forms, severity levels, and timeframes (entire period, five-year periods, and annual); and multiple definitions of ""elevated"" terrorism. The findings reveal that terrorism is concentrated geographically and temporally. Most countries experience peace or very low levels of terrorism; only a few experience substantial outbreaks; very few experience prolonged terrorism; and even fewer, prolonged severe terrorism. © Gallup.",Scopus,2-s2.0-84964555199
English,Conference Paper,2014,"Huang Y., Cui W., Rui Y.",Mobile atmospheric sensing using vision approach,IOP Conference Series: Earth and Environmental Science,17,1,12016,,,,,10.1088/1755-1315/17/1/012016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902208895&doi=10.1088%2f1755-1315%2f17%2f1%2f012016&partnerID=40&md5=a822fef77296e92b383e74ce88de2b8b,"Air quality monitoring, especially the atmospheric phenomenon of thick haze, has been an acute problem in most countries and a hot topic in the atmospheric sensing. Recently thick haze occurs more frequently in most cities of China due to the rapid growth of traffic, farming, wildfires, and industrial development. It forms a low-hanging shroud that impairs visibility and becomes a respiratory health threat. Traditionally the dust, smoke, and other particles in relatively dry sky are reported at fixed meteorological stations. The coverage of these sampling stations is limited and cannot accommodate with the emergent incidence of thick haze from industrial pollution. In addition, the visual effect of thick haze is not yet investigated in the current practice. Thick haze appears colorful veil (e.g., yellowish, brownish-grey, etc) in video log images and results in a loss of contrast in the subject due to the light scattering through haze particles. This paper proposes an intuitive and mobile atmospheric sensing using vision approach. Based on the video log images collected by a mobile sensing vehicle, a Haze Veil Index (HVI) is proposed to identify the type and severity level of thick haze from the color and texture perspective. HVI characterizes the overall veil effect of haze spatially. HVI first identifies the haze color from the color deviation histogram of the white-balanced hazy image. The white-balancing is conducted with the most haze-opaque pixels in the dark channel and seed growing strategy. Then pixel-wise haze severity level of atmospheric veil is inferred by approximating the upper veil limit with the dark color of each pixel in a hazy image. The proposed method is tested on a diverse set of actual hazy video log images under varying atmospheric conditions and backgrounds in Wuhan City, China. Experimental results show the proposed HVI is effective for visually atmospheric sensing. The proposed method is promising for haze monitoring and prediction in UAV and satellite remote-sensing images.",Scopus,2-s2.0-84902208895
English,Article,2021,"Xia B., Bai Y., Yin J., Li Y., Xu J.",LogGAN: a Log-level Generative Adversarial Network for Anomaly Detection using Permutation Event Modeling,Information Systems Frontiers,23,2,,285,298,,1,10.1007/s10796-020-10026-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086718325&doi=10.1007%2fs10796-020-10026-3&partnerID=40&md5=9c5941b56c2f9fe516aa0824c22069ef,"System logs that trace system states and record valuable events comprise a significant component of any computer system in our daily life. Each log contains sufficient information (i.e., normal and abnormal instances) that assist administrators in diagnosing and maintaining the operation of systems. If administrators cannot detect and eliminate diverse and complex anomalies (i.e., bugs and failures) efficiently, running workflows and transactions, even systems, would break down. Therefore, the technique of anomaly detection has become increasingly significant and attracted a lot of research attention. However, current approaches concentrate on the anomaly detection analyzing a high-level granularity of logs (i.e., session) instead of detecting log-level anomalies which weakens the efficiency of responding anomalies and the diagnosis of system failures. To overcome the limitation, we propose an LSTM-based generative adversarial network for anomaly detection based on system logs using permutation event modeling named LogGAN, which detects log-level anomalies based on patterns (i.e., combinations of latest logs). On the one hand, the permutation event modeling mitigates the strong sequential characteristics of LSTM for solving the out-of-order problem caused by the arrival delays of logs. On the other hand, the generative adversarial network-based model mitigates the impact of imbalance between normal and abnormal instances to improve the performance of detecting anomalies. To evaluate LogGAN, we conduct extensive experiments on two real-world datasets, and the experimental results show the effectiveness of our proposed approach on the task of log-level anomaly detection. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Scopus,2-s2.0-85086718325
English,Article,2020,"Blessinger T.D., Euling S.Y., Wang L., Hogan K.A., Cai C., Klinefelter G., Saillenfait A.-M.",Ordinal dose-response modeling approach for the phthalate syndrome,Environment International,134,,105287,,,,1,10.1016/j.envint.2019.105287,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075506868&doi=10.1016%2fj.envint.2019.105287&partnerID=40&md5=bdb9816b465dc1d4102f633136360607,"Background: The phthalate syndrome (PS) is a collection of related male reproductive developmental effects, ranging in severity, that have been observed in rats after gestational exposure to developmentally-toxic phthalates. For statistical purposes, the PS is defined as a single endpoint and one dose-response analysis is conducted, rather than conducting multiple analyses on each individual endpoint. Objective: To improve dose-response modeling approaches for the PS and other syndromes of effects by accounting for differing severity levels among the endpoints. Methods: Ordinal dose-response modeling was performed on PS data from a published study of diisobutyl phthalate (DIBP) gestational exposure to male Sprague-Dawley rats. To incorporate PS endpoint severity, the endpoints were categorized into ordinal levels based on the expected impact of male developmental endpoint's on fertility. Then, a benchmark dose was estimated for each ordinal level. A bootstrap procedure was used to account for the nested nature of the data, and a sensitivity analysis was performed to assess the bootstrap results. A comparison of the estimates between the ordinal and the dichotomous model was performed. Results: The ordinal version of the log-logistic model applied to the data categorized by PS endpoint severity level provided benchmark dose estimates that were closer to each other in value and had lower variability than the traditional dichotomous application. The sensitivity analysis confirmed the validity of the bootstrap results. Conclusion: The ordinal dose-response modeling method accounts for severity differences among dichotomous PS endpoints, can be expanded in the future to include more severity levels, and can be used in both single and cumulative phthalate risk assessments. © 2019",Scopus,2-s2.0-85075506868
English,Article,2017,"Kitali A.E., Kidando E., Sando T., Moses R., Ozguven E.E.",Evaluating Aging Pedestrian Crash Severity with Bayesian Complementary Log–Log Model for Improved Prediction Accuracy,Transportation Research Record,2659,1,,155,163,,8,10.3141/2659-17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049426841&doi=10.3141%2f2659-17&partnerID=40&md5=fb7095c1edc0dfa6622d567df414f3e1,"Reliable prediction accuracy is an essential attribute for crash prediction models. Generally, more severe injury outcomes, such as fatalities, are rarer than less severe crashes, such as property damage only or minor injury crashes. The complementary log–log (cloglog) model, commonly used in epidemiological research, is known for its accuracy in predicting rare events. This study implemented the cloglog model in analyzing pedestrian injury severity and compared its performance with the two conventional models used in injury severity research: the probit and logit models. The three models were developed with data from 1,397 crashes involving aging pedestrians that occurred in Florida from 2009 through 2013. The response variable, injury severity level, was binary and categorized as either fatal or severe injury or minor or no injury. The study used three accuracy metrics (deviance information criteria, prediction accuracy, and receiver operating characteristics curves) to compare the performance of the models. The cloglog model outperformed the probit and logit models in overall goodness of fit and prediction accuracy. More important, the cloglog model outperformed the other two models considerably for predicting fatal and severe crashes according to the recall metric (72% accuracy versus 43% and 41% for probit and logit models, respectively). However, the other two models outperformed the cloglog model in predicting crashes with no or minor injuries. Of predictor variables included in the model, six were found to significantly influence fatal or severe injuries for aging pedestrians at 95% Bayesian credible interval. These variables included pedestrian age, alcohol involvement, first harmful event, vehicle movement, shoulder type, and posted speed. © 2017 National Academy of Sciences.",Scopus,2-s2.0-85049426841
English,Article,2013,"Benjamin J.G., Seymour R.S., Meacham E., Wilson J.",Impact of whole-tree and cut-to-length harvesting on postharvest condition and logging costs for early commercial thinning in maine,Northern Journal of Applied Forestry,30,4,,149,155,,14,10.5849/njaf.13-016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890935075&doi=10.5849%2fnjaf.13-016&partnerID=40&md5=15def5b60bbfb7520950735357bd014a,"Regenerating clearcuts dating from the spruce budworm outbreak of the 1980s are beginning to reach merchantable size throughout northern Maine and would benefit from commercial thinning, but there is no consensus among foresters and the logging industry about how such stands can be efficiently thinned. This study investigated silviculturally effective, operational solutions to implement early commercial thinning treatments. Comparisons between two wholetree (WT) and two cut-to-length (CTL) systems were made in terms of residual stem damage, retention of downed woody material, product utilization, and unit cost of production. Results show significantly more crop trees removed (P = 0.030) and more high severity wound area per plot (P = 0.011) for the WT systems. There was no difference in the number of stems wounded per plot at the high severity level between harvest methods (P = 0.312). Harvest-generated and retained downed wood material volume averaged 511.9 and 205.6 ft3/ac for CTL and WT plots, respectively. Round wood production was the same for CTL and WT plots (average 30 tons/ac), but more than four times more biomass was produced from the WT operations. Production costs were not significantly different between harvest methods due in part to high machine productivity and increased biomass production for the WT systems. © 2013 by the Society of American Foresters.",Scopus,2-s2.0-84890935075
English,Article,2019,"Franceschini M.H.D., Bartholomeus H., van Apeldoorn D.F., Suomalainen J., Kooistra L.",Feasibility of unmanned aerial vehicle optical imagery for early detection and severity assessment of late blight in Potato,Remote Sensing,11,3,224,,,,20,10.3390/rs11030224,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061357547&doi=10.3390%2frs11030224&partnerID=40&md5=068bc3a5922c0bb2375c4b4e0c19cc22,"Assessment of disease incidence and severity at farm scale or in agronomic trials is frequently performed based on visual crop inspection, which is a labor intensive task prone to errors associated with its subjectivity. Therefore, alternative methods to relate disease incidence and severity with changes in crop traits are of great interest. Optical imagery in the visible and near-infrared (Vis-NIR) can potentially be used to detect changes in crop traits caused by pathogen development. Also, cameras on-board of Unmanned Aerial Vehicles (UAVs) have flexible data collection capabilities allowing adjustments considering the trade-off between data throughput and its resolution. However, studies focusing on the use of UAV imagery to describe changes in crop traits related to disease infection are still lacking. More specifically, evaluation of late blight (Phytophthora infestans) incidence in potato concerning early discrimination of different disease severity levels has not been extensively reported. In this article, the description of spectral changes related to the development of potato late blight under low disease severity levels is performed using sub-decimeter UAV optical imagery. The main objective was to evaluate the sensitivity of the data acquired regarding early changes in crop traits related to disease incidence. For that, UAV images were acquired on four dates during the growing season (from 37 to 78 days after planting), before and after late blight was detected in the field. The spectral variability observed in each date was summarized using Simplex Volume Maximization (SiVM), and its relationship with experimental treatments (different crop systems) and disease severity levels (evaluated by visual assessment) was determined based on pixel-wise log-likelihood ratio (LLR) calculation. Using this analytical framework it was possible to identify considerable spectral changes related to late blight incidence in different treatments and also to disease severity level as low as between 2.5 and 5.0% of affected leaf area. Comparison of disease incidence and spectral information acquired using UAV (with 4-5 cm of spatial resolution) and ground-based imagery (with 0.1-0.2 cm of spatial resolution) indicate that UAV data allowed identification of patterns comparable to those described by ground-based images, despite some differences concerning the distribution of affected areas detected within the sampling units and an attenuation in the signal measured. Finally, although aggregated information at sampling unit level provided discriminative potential for higher levels of disease development, focusing on spectral information related to disease occurrence increased the discriminative potential of the data acquired. © 2019 by the authors.",Scopus,2-s2.0-85061357547
English,Article,2006,"Bae Y., Kakkar V., Ogaki M.",Money demand in Japan and nonlinear cointegration,"Journal of Money, Credit and Banking",38,6,,1659,1667,,18,10.1353/mcb.2006.0076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748571943&doi=10.1353%2fmcb.2006.0076&partnerID=40&md5=aa5b04ea60d8a4db21a691accfdcac42,"We estimate the long-run Japanese money demand function in a cointegration framework with two nonlinear functional forms that allow for the liquidity trap, and compare the results with the standard log-level functional form. In addition to the conventional linear cointegration techniques, we also use a recently developed procedure for nonlinear cointegration that allows the estimation of alternative functional forms under the same assumption regarding the trend properties of the nominal interest rate. The nonlinear functional forms outperform the log-level functional form based on out-of-sample prediction performance. Copyright 2006 by The Ohio State University.",Scopus,2-s2.0-33748571943
English,Article,1983,"Fackler J.S., Douglas McMillin W.",Specification and Goldfeld money demand function,Journal of Macroeconomics,5,4,,437,459,,8,10.1016/0164-0704(83)90033-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48749144267&doi=10.1016%2f0164-0704%2883%2990033-2&partnerID=40&md5=ae74f5b5c0d3d369463e710d7fa61dec,"Despite intensive investigation of the temporal stability of the Goldfield formulation of the money demand function, a clear consensus on its stability has yet to emerge. This paper builds a statistical case supporting the first difference of log-levels specification, as opposed to the more commonly used log-levels specification, of the Goldfeld equation and then examines the stability of both specifications. Formal stability tests proposed by Cooley and Prescott, Farley and Hinich, and Brown, Durbin, and Evans are employed; the out-of-sample predictive performance is examined as well. These tests strongly support the first difference specification over thelog-levels specification. © 1984.",Scopus,2-s2.0-48749144267
English,Article,2017,"Safe M., Faradmal J., Poorolajal J., Mahjub H.",Model-based recursive partitioning for survival of Iranian female breast cancer patients: Comparing with parametric survival models,Iranian Journal of Public Health,46,1,,35,43,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010932208&partnerID=40&md5=73aff7106a87a2e195cebe42a1c77c0e,"Background: Precise diagnosis of disease risk factors via efficient statistical models is the primary step for reducing the heavy costs of breast cancer, as one of the most highly prevalent cancer throughout the world. Therefore, the aim of this study was to present a recently introduced statistical model in order to assess its proficiency for model fitting. Methods: The information of 1465 eligible Iranian women with breast cancer was used for this retrospective cohort study. The statistical performances of exponential, Weibull, Log-logistic and Lognormal, as the most proper parametric survival models, were evaluated and compared with ‘Model-based Recursive Partitioning’ in order to survey their capability of more relevant risk factor detection. Results: ‘Model-based Recursive Partitioning’ recognized the largest number of significant affective risk factors, whereas, all four parametric models agreed and unable to detect the effectiveness of ‘Progesterone Receptor’ as an indicator; ‘Log-Normal-based Recursive Partitioning’ could provide the paramount fit. Conclusion: The superiority of ‘Model-based Recursive Partitioning’ was ascertained; not only by its excellent fitness but also by its susceptibility for classification of individuals to homogeneous severity levels and its impressive visual intuition potentiality. © 2017, Iranian Journal of Public Health. All rights reserved.",Scopus,2-s2.0-85010932208
English,Conference Paper,2019,"Xia B., Yin J., Xu J., Li Y.",LogGAN: A Sequence-Based Generative Adversarial Network for Anomaly Detection Based on System Logs,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11933 LNCS,,,61,76,,5,10.1007/978-3-030-34637-9_5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078462311&doi=10.1007%2f978-3-030-34637-9_5&partnerID=40&md5=a2e779d884c5111e07394a4f9c26f248,"System logs which trace system states and record valuable events comprise a significant component of any computer system in our daily life. There exist abundant information (i.e., normal and abnormal instances) involved in logs which assist administrators in diagnosing and maintaining the operation of the system. If diverse and complex anomalies (i.e., bugs and failures) cannot be detected and eliminated efficiently, the running workflows and transactions, even the system, would break down. Therefore, anomaly detection has become increasingly significant and attracted a lot of research attention. However, current approaches concentrate on the anomaly detection in a high-level granularity of logs (i.e., session) instead of detecting log-level anomalies which weakens the efficiency of responding anomalies and the diagnosis of system failures. To overcome the limitation, we propose a sequence-based generative adversarial network for anomaly detection based on system logs named LogGAN which detects log-level anomalies based on the patterns (i.e., the combination of latest logs). In addition, the generative adversarial network-based model relieves the effect of imbalance between normal and abnormal instances to improve the performance of capturing anomalies. To evaluate LogGAN, we conduct extensive experiments on two real-world datasets, and the experimental results show the effectiveness of our proposed approach to log-level anomaly detection. © Springer Nature Switzerland AG 2019.",Scopus,2-s2.0-85078462311
English,Article,2021,"Wang K., Bhowmik T., Zhao S., Eluru N., Jackson E.",Highway safety assessment and improvement through crash prediction by injury severity and vehicle damage using Multivariate Poisson-Lognormal model and Joint Negative Binomial-Generalized Ordered Probit Fractional Split model,Journal of Safety Research,76,,,44,55,,3,10.1016/j.jsr.2020.11.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098227868&doi=10.1016%2fj.jsr.2020.11.005&partnerID=40&md5=531101c6f7ee41a9aab92d739b4b31bb,"Introduction: Predicting crash counts by severity plays a dominant role in identifying roadway sites that experience overrepresented crashes, or an increase in the potential for crashes with higher severity levels. Valid and reliable methodologies for predicting highway accidents by severity are necessary in assessing contributing factors to severe highway crashes, and assisting the practitioners in allocating safety improvement resources. Methods: This paper uses urban and suburban intersection data in Connecticut, along with two sophisticated modeling approaches, i.e. a Multivariate Poisson-Lognormal (MVPLN) model and a Joint Negative Binomial-Generalized Ordered Probit Fractional Split (NB-GOPFS) model to assess the methodological rationality and accuracy by accommodating for the unobserved factors in predicting crash counts by severity level. Furthermore, crash prediction models based on vehicle damage level are estimated using the same two methodologies to supplement the injury severity in estimating crashes by severity when the sample mean of severe injury crashes (e.g., fatal crashes) is very low. Results: The model estimation results highlight the presence of correlations of crash counts among severity levels, as well as the crash counts in total and crash proportions by different severity levels. A comparison of results indicates that injury severity and vehicle damage are highly consistent. Conclusions: Crash severity counts are significantly correlated and should be accommodated in crash prediction models. Practical application: The findings of this research could help select sound and reliable methodologies for predicting highway accidents by injury severity. When crash data samples have challenges associated with the low observed sampling rates for severe injury crashes, this research also confirmed that vehicle damage can be appropriate as an alternative to injury severity in crash prediction by severity. © 2020 National Safety Council and Elsevier Ltd",Scopus,2-s2.0-85098227868
English,Article,1999,"Dolmas J., Raj B., Slottje D.J.",The U.S. Productivity slowdown: A peak through the structural break window,Economic Inquiry,37,2,,226,241,,12,10.1111/j.1465-7295.1999.tb01427.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033408571&doi=10.1111%2fj.1465-7295.1999.tb01427.x&partnerID=40&md5=2545a950b65640dde3f370d10e9e6282,"This paper provides a formal test of the null hypothesis of a unit root in the log-level of labor productivity against the alternative of linear trend stationarity with a one-time structural break in the level and slope of the trend at an a priori unknown date. Using some newly developed time series tests, we show that the log-level of productivity is more accurately modeled as following a deterministic trend with a regime shift rather than as a unit root process. Some implications of the results for detrending and for testing cointegration relationships between productivity and other variables are discussed.",Scopus,2-s2.0-0033408571
English,Article,1993,"Arize A.C., Shwiff S.S.","Cointegration, real exchange rate and modelling the demand for broad money in Japan",Applied Economics,25,6,,717,726,,18,10.1080/00036849300000124,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039318819&doi=10.1080%2f00036849300000124&partnerID=40&md5=ce3275ad03252283d6332208dec37e67,"The literature on the demand for the Japan's broad money addresses two controversial issues: the form (log level or log difference) in which variables enter the money demand function and the question of whether financial innovation and deregulation caused shift(s) in the money demand relationship. Log-level specifications of the money demand function have been shown to exhibit large shifts, whereas log-first-differences and error-correction specifications do not. Our paper demonstrates that the appropriate specification for the Japanese money demand function is that which uses cointegration and error-correction procedures. © 1993, Taylor & Francis Group, LLC. All rights reserved.",Scopus,2-s2.0-0039318819
English,Article,1989,Gassmann H.I.,Optimal harvest of a forest in the presence of uncertainty,Canadian Journal of Forest Research,19,10,,1267,1274,,50,10.1139/x89-193,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024816102&doi=10.1139%2fx89-193&partnerID=40&md5=74081420ad25d8a14834d4cee7f6262d,"A method is described for finding logging levels to maximize harvest in a finite horizon type II model. Uncertainty is considered in the form of the risk of forest fires and other environmental hazards, which may destroy a random fraction of the existing forest. Numerical results include upper and lower bound approximations to the original problem. -Author",Scopus,2-s2.0-0024816102
English,Article,1996,Han H.-L.,Cointegration and tests of a present value model in the stock market,Applied Economics,28,2,,267,272,,9,10.1080/000368496328902,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2342431793&doi=10.1080%2f000368496328902&partnerID=40&md5=6e9242c40fea267fd8c7376e61c56230,"The long-run relation between stock price and dividend is reinvestigated by applying the Canonical Cointegrating Regression. It is shown that the present value model implies either the levels or the log levels of stock price and dividend are cointegrated, when there are no rational bubbles. Both the deterministic and the stochastic components of stock price and dividend are examined in this study to determine the validity of the present value model. It is found that neither the levels nor the log levels of stock price and dividend are cointegrated. Rational bubbles may exist in the deterministic component of stock price and cannot be eliminated by the cointegrating vector.",Scopus,2-s2.0-2342431793
English,Article,2009,Nazarov A.,Log-level comparison principle for small ball probabilities,Statistics and Probability Letters,79,4,,481,486,,10,10.1016/j.spl.2008.09.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58549099251&doi=10.1016%2fj.spl.2008.09.021&partnerID=40&md5=eaa79193bb101198f71da789fd451e71,"We prove a new variant of comparison principle for logarithmic L2-small ball probabilities of Gaussian processes. As an application, we obtain logarithmic small ball asymptotics for some well-known processes with smooth covariances. © 2008 Elsevier B.V. All rights reserved.",Scopus,2-s2.0-58549099251
English,Article,2018,"Zhao M., Liu C., Li W., Sharma A.",Multivariate Poisson-lognormal model for analysis of crashes on urban signalized intersections approach,Journal of Transportation Safety and Security,10,3,,251,265,,12,10.1080/19439962.2017.1323059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021447764&doi=10.1080%2f19439962.2017.1323059&partnerID=40&md5=1507b11cb083f81820f96e67b5c0ef04,"Many studies investigate contributing factors of intersection crashes, but very limited studies focus on cashes on the intersection approach. It is important to address the characteristics of intersection-approach crashes to better understand intersection safety. This article analyzes the crashes on signalized intersection approach on urban arterials with a focus on traffic and geometric elements. The intersection approach is defined as the segment between stop bar and the location 200 ft upstream from the stop bar. The multivariate Poisson log-normal (MVPLN) model is used to model crash counts by severity. Ten-year crash data collected from 643 signalized intersections in Nebraska are used for analysis. One-way road is found to be negatively related to all three severity levels (light crash, moderate crash, and severe crash) of crashes. Compared to the 12 ft lane width, narrower lane widths generally lead to fewer crashes. The intersection approaches on urban arterials are expected to have more crashes than collector roads. The numbers of right-turn, left-turn, and through lanes, as well as the annual average daily traffic on the intersection approach and its crossing approach are statistically significant factors increasing crash frequency. The MVPLN model is compared to univariate and zero-inflated Poisson models. The results reveal that the MVPLN model provides a superior fit over the univariate Poisson model. © 2018 Taylor & Francis Group, LLC and The University of Tennessee.",Scopus,2-s2.0-85021447764
English,Article,2016,"Sharma A., Amarnath M., Kankar P.K.",Feature extraction and fault severity classification in ball bearings,JVC/Journal of Vibration and Control,22,1,,176,192,,67,10.1177/1077546314528021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952882177&doi=10.1177%2f1077546314528021&partnerID=40&md5=299efd78e505e8d167fc748ac5deb775,"The present study attempts to diagnose severity of faults in ball bearings using various machine learning techniques, like support vector machine (SVM) and artificial neural network (ANN). Various features are extracted from raw vibration signals which include statistical features such as skewness, kurtosis, standard deviation and measures of uncertainty such as Shannon entropy, log energy entropy, sure entropy, etc. The calculated features are examined for their sensitivity towards fault of different severity in bearings. The proposed methodology incorporates extraction of most appropriate features from raw vibration signals. Results revealed that apart from statistical features uncertainty measures like log energy entropy and sure entropy are also good indicators of variation in fault severity. This work attempts to classify faults of different severity level in each bearing component which is not considered in most of the previous studies. Classification efficiency achieved by proposed methodology is compared to the other methodologies available in the literature. Comparative study shows the potential application of proposed methodology with machine learning techniques for the development of real time system to diagnose fault and it's severity in ball bearings. © 2014 The Author(s).",Scopus,2-s2.0-84952882177
English,Article,2021,"Finch A.P., Brazier J., Mukuria C.","Selecting Bolt-on Dimensions for the EQ-5D: Testing the Impact of Hearing, Sleep, Cognition, Energy, and Relationships on Preferences Using Pairwise Choices",Medical Decision Making,41,1,,89,99,,3,10.1177/0272989X20969686,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096971325&doi=10.1177%2f0272989X20969686&partnerID=40&md5=731c3c1db739750d165eb12f7e328154,"Background: Generic preference-based measures (GPBMs) such as the EQ-5D are valid across many conditions, but in some cases, “bolting on” additional dimensions may improve validity. The selection of “bolt-ons” has been based on the psychometric impact of individual dimensions, but preferences provide another important way to select them. This study aims to test the potential of using pairwise choices to inform the selection of bolt-ons for the EQ-5D-5L. Methods: General population preferences were collected using an online survey of 1040 UK residents. Three EQ-5D-5L health state pairs were selected based on pairs that had a 50:50 split in respondent preferences from a previous pairwise survey. Participants were presented with pairwise choices of EQ-5D-5L health states without and with bolt-ons of hearing, sleep, cognition, energy, and relationships, each added individually. Logistic models were used to assess the impact of bolt-ons, as well as bolt-ons at different severity levels, on the log odds of responders choosing between health states. Results: Preferences varied according to the bolt-ons and their severity level (only levels 1, 3, and 5 were used). Additions of bolt-ons at level 1 generally resulted in nonstatistically significant differences while additions of bolt-ons at level 3 and level 5 produced a negative and statistically significant impact on preferences for the health state with the bolt-on. At level 5, hearing had the largest impact, followed by cognition, relationships, energy, and sleep. At level 3, cognition produced the largest impact, followed by hearing and sleep with similar impacts, energy, and relationships. This ordering offers information for bolt-on selection, with hearing and cognition appearing as the most important. The weight placed on the different health problems is not constant across severity levels between bolt-ons. Conclusions: Pairwise choices provide a cost-effective approach of generating information on preferences to support bolt-on selection. © The Author(s) 2020.",Scopus,2-s2.0-85096971325
English,Article,2017,"Dabbour E., Easa S., Haider M.",Using fixed-parameter and random-parameter ordered regression models to identify significant factors that affect the severity of drivers’ injuries in vehicle-train collisions,Accident Analysis and Prevention,107,,,20,30,,13,10.1016/j.aap.2017.07.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025657792&doi=10.1016%2fj.aap.2017.07.017&partnerID=40&md5=724fc2c97af43992b0fa6b90761e2471,"This study attempts to identify significant factors that affect the severity of drivers’ injuries when colliding with trains at railroad-grade crossings by analyzing the individual-specific heterogeneity related to those factors over a period of 15 years. Both fixed-parameter and random-parameter ordered regression models were used to analyze records of all vehicle-train collisions that occurred in the United States from January 1, 2001 to December 31, 2015. For fixed-parameter ordered models, both probit and negative log–log link functions were used. The latter function accounts for the fact that lower injury severity levels are more probable than higher ones. Separate models were developed for heavy and light-duty vehicles. Higher train and vehicle speeds, female, and young drivers (below the age of 21 years) were found to be consistently associated with higher severity of drivers’ injuries for both heavy and light-duty vehicles. Furthermore, favorable weather, light-duty trucks (including pickup trucks, panel trucks, mini-vans, vans, and sports-utility vehicles), and senior drivers (above the age of 65 years) were found be consistently associated with higher severity of drivers’ injuries for light-duty vehicles only. All other factors (e.g. air temperature, the type of warning devices, darkness conditions, and highway pavement type) were found to be temporally unstable, which may explain the conflicting findings of previous studies related to those factors. © 2017 Elsevier Ltd",Scopus,2-s2.0-85025657792
English,Article,1999,"Forshaw J.R., Sabio Vera A., Webber B.R.",Final states in small-x deep inelastic scattering,Journal of Physics G: Nuclear and Particle Physics,25,7,,1511,1514,,11,10.1088/0954-3899/25/7/337,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033412825&doi=10.1088%2f0954-3899%2f25%2f7%2f337&partnerID=40&md5=4b8c9f6b0035084c98a510ec927ff453,This talk summarizes our work on the calculation of small-x jet rates within the BFKL and CCFM approaches. The two approaches are proven to yield the same results at the leading logarithm level to order ᾱ3S. The proof is then extended to all orders.,Scopus,2-s2.0-0033412825
English,Article,1979,Giovannini A.,QCD jets as Markov branching processes,"Nuclear Physics, Section B",161,2-3,,429,448,,82,10.1016/0550-3213(79)90222-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36749029176&doi=10.1016%2f0550-3213%2879%2990222-0&partnerID=40&md5=4311ff23fa312058b51395e49a079110,"Working at the leading log level, within a simplified treatment of IR divergences, the evolution of quark and gluon jets has been studied in terms of Markov branching processes and Janossy G-equations. The results previously found by KUV and AP are recovered in this approach. © 1979.",Scopus,2-s2.0-36749029176
English,Review,2015,"Tilsch M.K., O'Donnell M.S.",Considerations on automation of coating machines,Advanced Optical Technologies,4,2,,209,219,,,10.1515/aot-2015-0015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941775378&doi=10.1515%2faot-2015-0015&partnerID=40&md5=a77d5d7d292ba43e37f3eb413f8496c8,"Most deposition chambers sold into the optical coating market today are outfitted with an automated control system. We surveyed several of the larger equipment providers, and nine of them responded with information about their hardware architecture, data logging, level of automation, error handling, user interface, and interfacing options. In this paper, we present a summary of the results of the survey and describe commonalities and differences together with some considerations of tradeoffs, such as between capability for high customization and simplicity of operation. © 2015 Thoss Media and De Gruyter.",Scopus,2-s2.0-84941775378
English,Article,2016,"Shepherd M.M., Mejias R.J.",Nontechnical Deterrence Effects of Mild and Severe Internet Use Policy Reminders in Reducing Employee Internet Abuse,International Journal of Human-Computer Interaction,32,7,,557,567,,7,10.1080/10447318.2016.1183862,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969833555&doi=10.1080%2f10447318.2016.1183862&partnerID=40&md5=e886ceb0d0bfd43e245321275dd644f1,"ABSTRACT: This two-stage longitudinal study examines how employee Internet abuse may be reduced by nontechnical deterrence methods, specifically via organizational acceptable use policies (AUPs). This study used actual employee usage and audit logs (not self-reporting survey measures) to monitor the web activity of employees. In stage 1, a mild AUP reminder sent to company employees resulted in a 12% decrease in employee Internet abuse. In stage 2, a more severe AUP reminder resulted in a 33% decrease in employee Internet abuse. For both stages, the AUP warning (regardless of severity level) resulted in an immediate and significant decrease in employee nonwork Internet use. Results indicate that the severe AUP treatment was more effective in reducing and maintaining lower levels of employee nonwork Internet use than the mild AUP treatment. Under the mild AUP treatment, employee nonwork Internet use levels returned to their pretreatment levels after only one week. However, under the severe AUP treatment, employee nonwork Internet use levels were lower than the mild AUP treatment and remained consistently lower than their pretreatment levels even after three weeks. These results suggest that nontechnical deterrence methods in the form of organizational IT use policies may constitute an effective approach to reducing employee Internet abuse, particularly if AUP policies are clear with regard to related sanctions and penalties for employee noncompliance. © 2016 Taylor & Francis.",Scopus,2-s2.0-84969833555
English,Article,2013,"Aziz H.M.A., Ukkusuri S.V., Hasan S.",Exploring the determinants of pedestrian-vehicle crash severity in New York City,Accident Analysis and Prevention,50,,,1298,1309,,109,10.1016/j.aap.2012.09.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870298763&doi=10.1016%2fj.aap.2012.09.034&partnerID=40&md5=4b9bdf50984605226462e2669f0fad93,"Pedestrian-vehicle crashes remain a major concern in New York City due to high percentage of fatalities. This study develops random parameter logit models for explaining pedestrian injury severity levels of New York City accounting for unobserved heterogeneity in the population and across the boroughs. A log-likelihood ratio test for joint model suitability suggests that separate models for each of the boroughs should be estimated. Among many variables, road characteristics (e.g.; number of lanes, grade, light condition, road surface, etc.), traffic attributes (e.g.; presence of signal control, type of vehicle, etc.), and land use (e.g.; parking facilities, commercial and industrial land use, etc.) are found to be statistically significant in the estimated model. The study also suggests that the set of counter measures should be different for different boroughs in the New York City and the priority ranks of countermeasures should be different as well. © 2012 Elsevier Ltd.",Scopus,2-s2.0-84870298763
English,Article,2007,"Gao F., Li W.V.",Logarithmic level comparison for small deviation probabilities,Journal of Theoretical Probability,20,1,,1,23,,3,10.1007/s10959-006-0027-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847329227&doi=10.1007%2fs10959-006-0027-0&partnerID=40&md5=813872c95d46db9fd4f22d733ebf0379,"Log-level comparisons of the small deviation probabilities are studied in three different but related settings: Gaussian processes under the L2 norm, multiple sums motivated by tensor product of Gaussian processes, and various integrated fractional Brownian motions under the sup-norm. © Springer Science+Business Media, LLC 2007.",Scopus,2-s2.0-33847329227
English,Article,2006,"Gao F., Li W.V.",Logarithmic level comparison for small deviation probabilities,Journal of Theoretical Probability,19,3,,535,556,,5,10.1007/s10959-006-0026-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845646743&doi=10.1007%2fs10959-006-0026-1&partnerID=40&md5=8e01933661b3fdb4f46811fdcc155354,"Log-level comparisons of the small deviation probabilities are studied in three different but related settings: Gaussian processes under the L 2 norm, multiple sums motivated by tensor product of Gaussian processes, and various integrated fractional Brownian motions under the sup-norm. © 2006 Springer Science+Business Media, Inc.",Scopus,2-s2.0-33845646743
English,Article,2015,"Sar N., Chatterjee S., Das Adhikari M.","Integrated remote sensing and GIS based spatial modelling through analytical hierarchy process (AHP) for water logging hazard, vulnerability and risk assessment in Keleghai river basin, India",Modeling Earth Systems and Environment,1,4,31,,,,24,10.1007/s40808-015-0039-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086722568&doi=10.1007%2fs40808-015-0039-9&partnerID=40&md5=6a6ae86623159584f461ab33ccd6b17b,"Water-logging disaster is a most important environmental as well as socio-economic problem which directly associated with the utilization of soil and land resources in agricultural command areas. Water resource management and conservation is a crucial approach for agricultural development in a basin area. To identify the maximum extent of waterlogged area in a basin during the pre-monsoon, monsoon and post monsoon season necessitated a multidisciplinary approach that integrates the spatial and non spatial attributes on Geographical Information System (GIS) that can be used by the decision makers for implement strategy of the problematic area in term of waterlogged and flood prone region. The main objective of the present investigation is to identify and mapping of the waterlogged disaster areas and it associated risk using an Analytical Hieratical Process and GIS model through ArcGIS model maker in the Keleghai river basin, India. For this purpose, the post monsoon multi-temporal (1976, 1979, 1987, 1990, 1996, 2000, 2005 and 2009) Landsat™ satellite imagery, topographical maps, ASTER Digital Elevation Model, ground water table and population data have been used to identify the severity level of waterlogged areas and the associated vulnerability level in the basin. We applied digital remote sensing techniques like normalised differences water index, normalised differences vegetation index and normalised differences moisture index to identify the water content pixel from the multi-temporal Landsat™ data. On the other hand, we have considered topographic variation, distance from river and post monsoon groundwater depth as a proxy for waterlogged hazard zoning. In the present study, we applied spatial modelling through Analytic Hierarchy Process (AHP) and GIS to demarcate the hazard and risk level in terms of severe, high, moderate and low. The basic model approach is the conversion of all the thematic layer attributes into a normalised weighted raster as per the water holding capacity through AHP and Multi Criteria Decision Support System. Thereafter, we used combined operation through ArcGIS raster modelling to mapping the severity level of waterlogged hazard area. The assessment of socio-economic impact, we have calculated risk by considering different vulnerability exposures viz. population distribution, settlement density and landuse/landcover. The study demonstrates the integrated remote sensing and GIS based spatial modelling to detect the waterlogged hazard and it associated risk level that occurred due to excessive accumulation of rain and floodwaters. The result depicts the waterlogged hazard zones viz. severe, high, moderate and low and its associated risk levels in the study area which can be helpful for better planning and management of both the drainage system and agriculture activities. © 2015, Springer International Publishing Switzerland.",Scopus,2-s2.0-85086722568
English,Article,2012,"Rosi A., Segoni S., Catani F., Casagli N.",Statistical and environmental analyses for the definition of a regional rainfall threshold system for landslide triggering in Tuscany (Italy),Journal of Geographical Sciences,22,4,,617,629,,62,10.1007/s11442-012-0951-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862223033&doi=10.1007%2fs11442-012-0951-0&partnerID=40&md5=33c47022c492f0e8909a738b6cea00c2,"The aim of this work is the determination of regional-scale rainfall thresholds for the triggering of landslides in the Tuscany Region (Italy). The critical rainfall events related to the occurrence of 593 past landslides were characterized in terms of duration (D) and intensity (I). I and D values were plotted in a log-log diagram and a lower boundary was clearly noticeable: it was interpreted as a threshold representing the rainfall conditions associated to landsliding. That was also confirmed by a comparison with many literature thresholds, but at the same time it was clear that a similar threshold would be affected by a too large approximation to be effectively used for a regional warning system. Therefore, further analyses were performed differentiating the events on the basis of seasonality, magnitude, location, land use and lithology. None of these criteria led to discriminate among all the events different groups to be characterized by a specific and more effective threshold. This outcome could be interpreted as the demonstration that at regional scale the best results are obtained by the simplest approach, in our case an empirical black box model which accounts only for two rainfall parameters (I and D). So a set of thresholds could be conveniently defined using a statistical approach: four thresholds corresponding to four severity levels were defined by means of the prediction interval technique and we developed a prototype warning system based on rainfall recordings or weather forecasts. © 2012 Science China Press and Springer-Verlag Berlin Heidelberg.",Scopus,2-s2.0-84862223033
English,Conference Paper,2017,Väisänen T.,Categorization of cyber security deception events for measuring the severity level of advanced targeted breaches,ACM International Conference Proceeding Series,Part F130530,,,125,131,,,10.1145/3129790.3129805,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037746770&doi=10.1145%2f3129790.3129805&partnerID=40&md5=f0c0cf19da60fe4ede3b4b2462b584eb,"Advanced attackers have become more sophisticated in their target selection, evasion of detection and monetization of breached data. Cyber deception is used for gathering information about botnets and spreading worms, and to detect persistent external attackers hidden into the systems as well as insider threats. Decoys are resources that should not be normally accessed. They raise alerts and provide information when systems have been compromised. Decoys can be used for learning about automated malicious tools and behavior of the adversaries, as well as to slow down the attacks. This paper tries to solve the following challenges. Deception tools usually raise only certain severity level alerts, which have been selected manually or hard coded into implementations. This means that telling the difference in severity between two alerts coming from different decoys may be difficult. However, on the other hand the second challenge is that alerts coming from decoys may tell too much information for malicious administrators (insider threats). In fact, many times it would be not necessary to tell the type or actual location of decoys at all. Third challenge is difficulty of monitoring the attack phases during time. For giving solutions for all three challenges, this paper proposes an automated categorization for severity of information coming from decoys. The proposed categorization can be used together with existing cyber security deception tools (such as honeypots, honeynets or honeytokens) to provide addition information for alerts. The categorization uses a decoy severity level, which is calculated from the criticality of locations of the actual decoy, a bait leading to it and a key enabling the access to the bait or the decoy. Usually external attacks start against the easiest targets, but insider threat may in fact access the most critical information right away. In addition to this, presented categorization wants to improve the situational awareness by giving more information for measuring the level of the adversaries in advanced targeted attacks, and thus helping with the third challenge. The proposed approach and categorization have been tested with propotype including a combination of webpage type of honeytokens, URL type of baits leading to them, and encryption keys and user credentials enabling access to the baits. Two different implementation approaches have been demonstrated. The results show that combining additional severity measurement information together with security alerts indeed improves the situational awareness. The results of the research can be used to improve existing deception tools and ways of logging of events, or to create new deception tools, as well as to improve information that would be shown in various visualization tools. © 2017 ACM.",Scopus,2-s2.0-85037746770
English,Article,2002,Hautmann F.,QQ̄g contribution to diffractive J/ψ electroproduction,Journal of High Energy Physics,6,4,,915,926,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044456146&partnerID=40&md5=9e55164c26bf319aa0e25682797e8556,We study the diffractive electroproduction of quarkonia from quark-antiquarkgluon states in the photon wave function. We show that these states contribute to the leading-power and leading-logarithm level. We suggest the measurement of J/ψ production via inelastic diffraction to study color-transparency and color-opacity effects in the diffractive gluon distribution. © SISSA/ISAS 2002.,Scopus,2-s2.0-23044456146
English,Conference Paper,2015,"Hones K., Stangl F., Sift M., Hessling M.",Visible optical radiation generates bactericidal effect applicable for inactivation of health care associated germs demonstrated by inactivation of E. coli and B. subtilis using 405-nm and 460-nm light emitting diodes,Progress in Biomedical Optics and Imaging - Proceedings of SPIE,9540,,95400T,,,,3,10.1117/12.2183903,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939818335&doi=10.1117%2f12.2183903&partnerID=40&md5=423b1e8563583a5ab7ea7487cd2565c1,"The Ulm University of Applied Sciences is investigating a technique using visible optical radiation (405 nm and 460 nm) to inactivate health-hazardous bacteria in water. A conceivable application could be point-of-use disinfection implementations in developing countries for safe drinking water supply. Another possible application field could be to provide sterile water in medical institutions like hospitals or dental surgeries where contaminated pipework or long-term disuse often results in higher germ concentrations. Optical radiation for disinfection is presently mostly used in UV wavelength ranges but the possibility of bacterial inactivation with visible light was so far generally disregarded. One of the advantages of visible light is, that instead of mercury arc lamps, light emitting diodes could be used, which are commercially available and therefore cost-efficient concerning the visible light spectrum. Furthermore they inherit a considerable longer life span than UV-C LEDs and are non-hazardous in contrast to mercury arc lamps. Above all there are specific germs, like Bacillus subtilis, which show an inactivation resistance to UV-C wavelengths. Due to the totally different deactivation mechanism even higher disinfection rates are reached, compared to Escherichia coli as a standard laboratory germ. By 460 nm a reduction of three log-levels appeared with Bacillus subtilis and a half log-level with Escherichia coli both at a dose of about 300 J/cm2;. By the more efficient wavelength of 405 nm four and a half log-levels are reached with Bacillus subtilis and one and a half log-level with Escherichia coli also both at a dose of about 300 J/cm2;. In addition the employed optical setup, which delivered a homogeneous illumination and skirts the need of a stirring technique to compensate irregularities, was an important improvement compared to previous published setups. Evaluated by optical simulation in ZEMAX® the designed optical element provided proven homogeneity distributions with maximum variation of ± 10 %. © 2015 SPIE.",Scopus,2-s2.0-84939818335
English,Article,2020,"Brice M.-H., Vissault S., Vieira W., Gravel D., Legendre P., Fortin M.-J.",Moderate disturbances accelerate forest transition dynamics under climate change in the temperate–boreal ecotone of eastern North America,Global Change Biology,26,8,,4418,4435,,7,10.1111/gcb.15143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085615140&doi=10.1111%2fgcb.15143&partnerID=40&md5=91b4137e159f9fefbb489ee62aa6838c,"Several temperate tree species are expected to migrate northward and colonize boreal forests in response to climate change. Tree migrations could lead to transitions in forest types, but these could be influenced by several non-climatic factors, such as disturbances and soil conditions. We analysed over 10,000 forest inventory plots, sampled from 1970 to 2018 in meridional Québec, Canada, to identify what environmental conditions promote or prevent regional-scale forest transitions. We used a continuous-time multi-state Markov model to quantify the probabilities of transitions between forest states (temperate, boreal, mixed, pioneer) as a function of climate (mean temperature and climate moisture index during the growing season), soil conditions (pH and drainage) and disturbances (severity levels of natural disturbances and logging). We further investigate how different disturbance types and severities impact forests' short-term transient dynamics and long-term equilibrium using properties of Markov transition matrices. The most common transitions observed during the study period were from mixed to temperate states, as well as from pioneer to boreal forests. In our study, transitions were mainly driven by natural and anthropogenic disturbances and secondarily by climate, whereas soil characteristics exerted relatively minor constraints. While major disturbances only promoted transitions to the pioneer state, moderate disturbances increased the probability of transition from mixed to temperate states. Long-term projections of our model under the current environmental conditions indicate that moderate disturbances would promote a northward shift of the temperate forest. Moreover, disturbances reduced turnover and convergence time for all transitions, thereby accelerating forest dynamics. Contrary to our expectation, mixed to temperate transitions were not driven by temperate tree recruitment but by mortality and growth. Overall, our results suggest that moderate disturbances could catalyse rapid forest transitions and accelerate broad-scale biome shifts. © 2020 John Wiley & Sons Ltd",Scopus,2-s2.0-85085615140
English,Conference Paper,2018,"Rong G., Gu S., Zhang H., Shao D., Liu W.",How is logging practice implemented in open source software projects? A preliminary exploration,"Proceedings - 25th Australasian Software Engineering Conference, ASWEC 2018",,,8587302,171,180,,2,10.1109/ASWEC.2018.00031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061030161&doi=10.1109%2fASWEC.2018.00031&partnerID=40&md5=37e474ad5fb25d1d3bf8c1e53e96277b,"Background: Logs are the footprints that software systems produce during runtime, which can be used to understand the dynamic behavior of these software systems. To generate logs, logging practice is accepted by developers to place logging statements in the source code of software systems. Compared to the great number of studies on log analysis, the research on logging practice is relatively scarce, which raises a very critical question, i.e. as the original intention, can current logging practice support capturing the behavior of software systems effectively? Aims: To answer this question, we first need to understand how logging practices are implemented these software projects. Method: In this paper, we carried out an empirical study to explore the logging practice in open source software projects so as to establish a basic understanding on how logging practice is applied in real world software projects. The density, log level (what to log?) and context (where to log?) are measured for our study. Results: Based on the evidence we collected in 28 top open source projects, we find the logging practice is adopted highly inconsistently among different developers both across projects and even within one project in terms of the density and log levels of logging statements. However, the choice of what context the logging statements to place is consistent to a fair degree. Conclusion: Both the inconsistency in density and log level and the convergence of context have forced us to question whether it is a reliable means to understand the runtime behavior of software systems via analyzing the logs produced by the current logging practice. © 2018 IEEE.",Scopus,2-s2.0-85061030161
English,Article,2016,"Harrasser N., Jüssen S., Obermeir A., Kmeth R., Stritzker B., Gollwitzer H., Burgkart R.",Antibacterial potency of different deposition methods of silver and copper containing diamond-like carbon coated polyethylene,Biomaterials Research,20,1,17,,,,23,10.1186/s40824-016-0062-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031018732&doi=10.1186%2fs40824-016-0062-6&partnerID=40&md5=d2877d6069c13ca1c95b811134ed7098,"Background: Antibacterial coatings of medical devices have been introduced as a promising approach to reduce the risk of infection. In this context, diamond-like carbon coated polyethylene (DLC-PE) can be enriched with bactericidal ions and gain antimicrobial potency. So far, influence of different deposition methods and ions on antimicrobial effects of DLC-PE is unclear. Methods: We quantitatively determined the antimicrobial potency of different PE surfaces treated with direct ion implantation (II) or plasma immersion ion implantation (PIII) and doped with silver (Ag-DLC-PE) or copper (Cu-DLC-PE). Bacterial adhesion and planktonic growth of various strains of S. epidermidis were evaluated by quantification of bacterial growth as well as semiquantitatively by determining the grade of biofilm formation by scanning electron microscopy (SEM). Additionally silver release kinetics of PIII-samples were detected. Results: (1) A significant (p<0.05) antimicrobial effect on PE-surface could be found for Ag- and Cu-DLC-PE compared to untreated PE. (2) The antimicrobial effect of Cu was significantly lower compared to Ag (reduction of bacterial growth by 0.8 (Ag) and 0.3 (Cu) logarithmic (log)-levels). (3) PIII as a deposition method was more effective in providing antibacterial potency to PE-surfaces than II alone (reduction of bacterial growth by 2.2 (surface) and 1.1 (surrounding medium) log-levels of PIII compared to 1.2 (surface) and 0.6 (medium) log-levels of II). (4) Biofilm formation was more decreased on PIII-surfaces compared to II-surfaces. (5) A silver-concentration-dependent release was observed on PIII-samples. Conclusion: The results obtained in this study suggest that PIII as a deposition method and Ag-DLC-PE as a surface have high bactericidal effects. © 2016 The Author(s).",Scopus,2-s2.0-85031018732
English,Article,2015,"Shibasaki Y., Seki Y., Tanaka T., Miyakoshi S., Fuse K., Kozakai T., Kobayashi H., Ushiki T., Abe T., Yano T., Moriyama M., Kuroha T., Isahai N., Takizawa J., Narita M., Koyama S., Furukawa T., Sone H., Masuko M.",The association of level of reduction of Wilms' tumor gene 1 mRNA transcript in bone marrow and outcome in acute myeloid leukemia patients,Leukemia Research,39,6,,667,671,,7,10.1016/j.leukres.2015.03.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929502960&doi=10.1016%2fj.leukres.2015.03.021&partnerID=40&md5=22e7d78ae29bef07df490de0a9597f25,"We focused on the level of reduction of Wilms' tumor gene 1 (WT1) mRNA in bone marrow as minimal residual disease during chemotherapies in adult acute myeloid leukemia (AML) patients. Forty-eight patients were enrolled in this study. Log levels of reduction of WT1 mRNA transcript after induction therapy compared with those at diagnosis were associated with disease-free survival (DFS) (. P=. 0.0066) and overall survival (OS) (. P=. 0.0074) in patients who achieved complete remission. Also log levels of reduction of WT1 mRNA transcript after final consolidation therapy compared with those at diagnosis were associated with DFS (. P=. 0.015) and OS (. P=. 0.012). By multivariate analysis, log levels of reduction of WT1 mRNA transcript after induction therapy and after final consolidation therapy compared with those at diagnosis were extracted as risk factors for outcome. Our results suggest that early and deep reduction of tumor burden may be important for the outcome of AML patients. In addition, it may be useful for the decision to proceed with allogeneic SCT as post-remission therapy. © 2015 Elsevier Ltd.",Scopus,2-s2.0-84929502960
English,Article,2009,"Abdel Kawy W.A.M., Belal A.A.","""Impact of soil degradation on land qualities of some cultivated areas at East Nile Delta-Egypt.""",Australian Journal of Basic and Applied Sciences,3,3,,2054,2063,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952265093&partnerID=40&md5=e4575bc2ca7b2c8f320c9eb6c70ccccc,"Most forms of degradation are man-made problems, although there are some physical environmental factors evolved, but misuse and mismanagement are important factors. Quantitative assessment of human induced land degradation and monitoring the changes in land qualities in El Dakhlyia Governorate during the period of 1976 to 2008 are the main objective of this study. Geometrically corrected physiographic-soils map of scale 1: 100.000 reduced to the attached map of scale 1: 250.000 were produced for the studied area. The comparison between the data extracted from the RISW reports, (1976) and the data of this study were carried out in the year 2008 for 13 selective profiles in 9 representative districts of EL Dakhlyia Governorat to determine the rate of land degradation. Aerial photo-interpretation, fieldwork and laboratory analysis data were used to produce the physiographic - soil ma of EL Dakhlyia Governorat. Land degradation rate, relative extent, degree, and severity level in the study area were assessed. The results indicate that the most active land degradation features are; water logging salinization, alkalinization and compaction. The main causative factors of human induced land degradation types in the studied area are over irrigation, human intervention in natural drainage, improper time use of heavy machinery and the absence of conservation measurements. © 2009, INSInet Publication.",Scopus,2-s2.0-77952265093
English,Article,1995,"Paul S., Chowdhury K.",Export-led growth hypothesis: Some empirical testing,Applied Economics Letters,2,6,,177,179,,13,10.1080/135048595357384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001329979&doi=10.1080%2f135048595357384&partnerID=40&md5=3e13e49b9bb6eae71dc3223c574583f9,"The hypothesis of export-led growth is tested using annual time series data for Australia. The analysis reveals that both exports and the GDP series in log levels have unit roots but they do not cointegrate. However, there is evidence of Granger causality running from exports to GDP growth implying that expansion of exports promotes economic growth in Australia. © 1995, Taylor & Francis Group, LLC. All rights reserved.",Scopus,2-s2.0-0001329979
English,Article,2009,"Park Y., Choi J., Hong J., Lee S., Yoo M., Cho J.",Accurate systematic hot-spot scoring method and score-based fixing guidance generation,"IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences",E92-A,12,,3082,3085,,,10.1587/transfun.E92.A.3082,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884269858&doi=10.1587%2ftransfun.E92.A.3082&partnerID=40&md5=894354b4e0034844784bd9fc1d470740,"The researches on predicting and removing of lithographic hot-spots have been prevalent in recent semiconductor industries, and known to be one of the most difficult challenges to achieve high quality detection coverage. To provide physical design implementation with designer's favors on fixing hot-spots, in this paper, we present a noble and accurate hot-spot detection method, so-called ""leveling and scoring"" algorithm based on weighted combination of image quality parameters (i.e., normalized image log-slope (NILS), mask error enhancement factor (MEEF), and depth of focus (DOF)) from lithography simulation. In our algorithm, firstly, hot-spot scoring function considering severity level is calibrated with process window qualification, and then least-square regression method is used to calibrate weighting coefficients for each image quality parameter. In this way, after we obtain the scoring function with wafer results, our method can be applied to future designs of using the same process. Using this calibrated scoring function, we can successfully generate fixing guidance and rule to detect hot-spot area by locating edge bias value which leads to a hot-spot-free score level. Finally, we integrate the hotspot fixing guidance information into layout editor to facilitate the user-favorable design environment. Applying our method to memory devices of 60 nm node and below, we could successfully attain sufficient process window margin to yield high mass production. Copyright © 2009 The Institute of Electronics, Information and Communication Engineers.",Scopus,2-s2.0-84884269858
English,Article,1994,"Kasran B., Nik A.R.",Suspended sediment yield resulting from selective logging practices in a small watershed in Peninsular Malaysia,Journal of Tropical Forest Science,7,2,,286,295,,21,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028682488&partnerID=40&md5=0e7545f2478257af1eda394c3cf67c06,"Selective logging was carried out in a 28.4 ha forested catchment of Jengka Experimental Basin. After six years of the calibration period the catchment was logged by commercial selective logging method. Significant increases in suspended sediment yield were observed. In the first year after logging, the suspended sediment yield increased from 100 to 277 kg ha-1 y-1 (177%) and further increased to 397 kg ha-1 y-1 (297%) in the following year. In the fourth year the yield recovered to the pre-logging level. -Authors",Scopus,2-s2.0-0028682488
English,Article,2020,"Shalkamy A., El-Basyouny K.",Multivariate models to investigate the relationship between collision risk and reliability outcomes on horizontal curves,Accident Analysis and Prevention,147,,105745,,,,2,10.1016/j.aap.2020.105745,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090841277&doi=10.1016%2fj.aap.2020.105745&partnerID=40&md5=605333a77e285532a73884d3f6dcb272,"This paper proposes a reliability-based framework to address the risk associated with limitations in the Available Sight Distance (ASD) on curved highway segments considering a three-dimensional (3D) sight distance computation approach. To facilitate this assessment, the ASD on horizontal curves was evaluated and an accurate inventory of curve attribute information was generated using LiDAR (Light Detection and Ranging) data in an automated and efficient manner. These datasets were then used to estimate the risk (probability of noncompliance, Pnc) associated with sight distance insufficiencies. Full Bayes multivariate Poisson log-normal safety performance functions were developed to relate the Pnc to the expected number of collisions. The results show that there was a statistically significant relationship between Pnc and collision frequency. There was also a significant correlation of 0.444 to 0.452 across collision severity levels indicating that curves with high Property-Damage-Only (PDO) collisions could be associated with higher injury and fatal (I + F) collisions. It was also found that Pnc had a greater impact on increasing PDO collisions than I + F collisions, suggesting that collisions associated with insufficient sight distance are likely to be less severe. The results of this analysis are expected to improve our understanding of the risks associated with deviations from design guidelines and quantitatively assess the safety margins due to these variations. The framework presented in this paper can be used to compare different design alternatives and investigate the influence of design deficiencies on collision occurrence across various severity levels. © 2020 Elsevier Ltd",Scopus,2-s2.0-85090841277
English,Article,2000,"Ciafaloni M., Rodrigo G.",Heavy quark impact factor at next-to-leading level,Journal of High Energy Physics,4,5 PART B,,1,16,,11,10.1088/1126-6708/2000/05/042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-7244228039&doi=10.1088%2f1126-6708%2f2000%2f05%2f042&partnerID=40&md5=bc1e7644e56945489adf39a2c5922e02,"We further analyze the definition and the calculation of the heavy quark impact factor at next-to-leading (NL) logs level, and we provide its analytical expression in a previously proposed k-factorization scheme. Our results indicate that k-factorization holds at NL level with a properly chosen energy scale, and with the same gluonic Green's function previously found in the massless probe case.",Scopus,2-s2.0-7244228039
English,Article,1999,"Forte S., Altarelli G., Ridolfi G.",Are parton distributions positive?,Nuclear Physics B - Proceedings Supplements,74,1-3,,138,141,,3,10.1016/S0920-5632(99)00150-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033245352&doi=10.1016%2fS0920-5632%2899%2900150-4&partnerID=40&md5=2041d4eb25031475b1afe9b4255b8a1f,"We show that the naive positivity conditions on polarized parton distributions which follow from their probabilistic interpretation in the naive parton model are reproduced in perturbative QCD at the leading log level if the quark and gluon distribution are defined in terms of physical processes. We show how these conditions are modified at the next-to-leading level, and discuss their phenomenological implications, in particular in view of the determination of the polarized gluon distribution.",Scopus,2-s2.0-0033245352
English,Article,1980,"Banerjee H., Sengupta M.",Transverse momentum cut-off hypothesis in high energy scattering in perturbation theory,Physics Letters B,93,3,,277,280,,,10.1016/0370-2693(80)90513-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49149144012&doi=10.1016%2f0370-2693%2880%2990513-4&partnerID=40&md5=9b4605526bc442847c18e9c205116c46,The hypothesis that transverse components of loop momenta are limited in high energy scattering is shown to imply that enhanced contributions from singular scaling in Feynman parameter space analysis should all cancel. Cancellation of singular scaling contributions is verified at the next to the leading-log level in the sixth order for fermion-fermion scattering in a Yang-Mills theory with SU(N)-gauge symmetry. © 1980.,Scopus,2-s2.0-49149144012
English,Article,2014,"Khan T., Westin J., Dougherty M.",Cepstral separation difference: A novel approach for speech impairment quantification in Parkinson's disease,Biocybernetics and Biomedical Engineering,34,1,,25,34,,16,10.1016/j.bbe.2013.06.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894594187&doi=10.1016%2fj.bbe.2013.06.001&partnerID=40&md5=455435730cb9d3696426db003d0e4311,"This paper introduces a novel approach, Cepstral Separation Difference (CSD), for quantification of speech impairment in Parkinson's disease (PD). CSD represents a ratio between the magnitudes of glottal (source) and supra-glottal (filter) log-spectrums acquired using the source-filter speech model. The CSD-based features were tested on a database consisting of 240 clinically rated running speech samples acquired from 60 PD patients and 20 healthy controls. The Guttmann (μ2) monotonic correlations between the CSD features and the speech symptom severity ratings were strong (up to 0.78). This correlation increased with the increasing textual difficulty in different speech tests. CSD was compared with some non-CSD speech features (harmonic ratio, harmonic-to-noise ratio and Mel-frequency cepstral coefficients) for speech symptom characterization in terms of consistency and reproducibility. The high intra-class correlation coefficient (&gt;0.9) and analysis of variance indicates that CSD features can be used reliably to distinguish between severity levels of speech impairment. Results motivate the use of CSD in monitoring speech symptoms in PD. © 2013 Nałȩcz Institute of Biocybernetics and Biomedical Engineering.",Scopus,2-s2.0-84894594187
English,Article,2007,Marginean P.,Logging in C++,Dr. Dobb's Journal,32,10,,50,56,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34948830551&partnerID=40&md5=6f29a1bd04582864258c0e1827903294,Logging is a critical technique for troubleshooting and maintaining software systems and provides information without requiring knowledge of programming language. Good logging mechanisms can save long debugging sessions and can increase the maintainability of applications. It is believed that indentation makes the logging more readable. Logging will have a cost only if it actually produces output. This lets the user to control the trade-off between fast execution and detailed logging. User can add logging liberally to code without serious efficiency concerns. The only thing to remember is to pass higher logging levels to code that is more heavily executed. Using policy-based design for logging is justified as the communication can be done in an efficient manner.,Scopus,2-s2.0-34948830551
English,Article,1998,"Forshaw J.R., Sabio Vera A.",QCD coherence and jet rates in small x deep inelastic scattering,"Physics Letters, Section B: Nuclear, Elementary Particle and High-Energy Physics",440,1-2,,141,150,,30,10.1016/S0370-2693(98)01090-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347570106&doi=10.1016%2fS0370-2693%2898%2901090-9&partnerID=40&md5=8cfe894947a144d1962a5515e1837eae,"The contributions to the deep inelastic scattering structure function which arise from emission of zero, one, two or three resolvable gluons and any number of unresolvable ones are computed to order ᾱ3S. Coherence effects are taken into account via angular ordering and are demonstrated to yield (at the leading logarithm level) the identical results to those obtained assuming the multi-Regge kinematics of BFKL. © 1998 Elsevier Science B.V. All rights reserved.",Scopus,2-s2.0-0347570106
English,Article,1979,"De Rújula A., Petronzio R., Savoy-Navarro A.",Radiative corrections to high-energy neutrino scattering,"Nuclear Physics, Section B",154,3,,394,426,,102,10.1016/0550-3213(79)90039-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000175869&doi=10.1016%2f0550-3213%2879%2990039-7&partnerID=40&md5=beedc0ee1f4627c2cc11455258e18f59,"Motivated by precise neutrino experiments, we reconsider the electromagnetic radiative corrections to the data. We investigate the usefulness and demonstrate the simplicity of the ""leading log"" approximation: the calculation to order α ln (Q/μ), α ln (Q/mq). Here Q is an energy scale of the overall process, μ is the lepton mass and mq is a hadronic mass, the effective quark mass in a parton model. We identify those questions the answers to which do not depend on unknown hadron parameters like quark masses. The leading log radiative corrections to dδ/dy distributions and to suitably interpreted dδ/dx distributions are quark-mass independent. We improve upon the conventional leading log approximation and compute explicitly the largest terms that lie beyond the leading log level. In practice this means that our model-independent formulae, though approximate, are likely to be excellent estimates everywhere except at low energy or very large y. We point out that radiative corrections to measurements of deviations from the Callan-Gross relation and to measurements of the ""sea"" constituency of nucleons are gigantic. The QCD inspired study of deviations from scaling is of particular interest. We compute, beyond the leading log level, the radiative corrections to the QCD predictions. © 1979.",Scopus,2-s2.0-0000175869
English,Article,1979,"Dörfel B., Dorn H., Mann G.",On the Exponentiation of Infrared Logarithms in Massless (φ3)6‐theory,Annalen der Physik,491,6,,471,478,,1,10.1002/andp.19794910608,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981963901&doi=10.1002%2fandp.19794910608&partnerID=40&md5=2eaf6b6ed35b69bfec2dfe97ec05a4c4,"We investigate the relation between the coefficients of the perturbative series for the propagator in massless theories and the coefficients of the perturbative series of the renormalization group quantities. This enable us to prove, that the concept of exponentiation in the sense introduced by POGGIO is valid in massless theories to arbitrary high order at the leading log level, but breaks down if nonleading log's are included. Copyright © 1979 WILEY‐VCH Verlag GmbH & Co. KGaA, Weinheim",Scopus,2-s2.0-84981963901
English,Article,1978,"Amati D., Petronzio R., Veneziano G.",Relating hard QCD processes through universality of mass singularities (II),"Nuclear Physics, Section B",146,1,,29,49,,224,10.1016/0550-3213(78)90430-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001853548&doi=10.1016%2f0550-3213%2878%2990430-3&partnerID=40&md5=866c60c7f395b36385e63c674a4b301a,"Extending previous techniques we obtain at all orders the factorization of mass singularities for every hard QCD process. These appear in a universal factor that can be reabsorbed into the standard parton density. Thus suitable ratios of cross sections can be computed by a perturbative expansion in the running coupling constant. Moreover, at the leading log level we obtain, after explicit cancellation of infrared divergences, the scaling violation of the operator product expansion. © 1978.",Scopus,2-s2.0-0001853548
English,Article,2017,"Nguyen H., Cai C., Chen F.",Automatic classification of traffic incident's severity using machine learning approaches,IET Intelligent Transport Systems,11,10,,615,623,,16,10.1049/iet-its.2017.0051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032889763&doi=10.1049%2fiet-its.2017.0051&partnerID=40&md5=830ce2a31f9d22f3614e1f9d224f65c6,"During daily work at a Transport Management Centre (TMC), the operators have to record and process a large volume of traffic information especially incident records. Their tasks involve manual classification of the data and then decide appropriate operations to clear the incidents on time. A real-time automatic decision support system can minimise an operator's responded time and hence reduce congestion. Besides standard descriptions (e.g. incident location, date, time, lanes affected), severity is an important criteria that operators have to evaluate based on all available information before any control commands can be issued. The NSW TMC and the research organisation Data61 in Sydney have collaborated to discover and visualise frequent patterns in historical incident response records, leading to the automatic classification of severity levels among past incidents using advanced machine learning, active learning and outlier detection techniques. The experiments were executed using 4 years TMC's incident logs from 2011 to 2014 which includes >40,000 records. The classification model achieved nearly 90% accuracy in five-fold cross-validation and is expected to help the TMC to improve its procedures, response plans, and resource allocations. © The Institution of Engineering and Technology 2017.",Scopus,2-s2.0-85032889763
English,Conference Paper,2009,"Park Y.-H., Kim D.-H., Choi J.-H., Hong J.-S., Park C.-H., Lee S.-H., Yoo M.-H., Cho J.-D.",Score-Based Fixing Guidance Generation With Accurate Hot-Spot Detection Method,Proceedings of SPIE - The International Society for Optical Engineering,7275,,72750P,,,,2,10.1117/12.811840,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66749186585&doi=10.1117%2f12.811840&partnerID=40&md5=08ee67a6fec287e813514469a44aa8c3,"While predicting and removing of lithographic hot-spots are a matured practice in recent semiconductor industry, it is one of the most difficult challenges to achieve high quality detection coverage and to provide designer-friendly fixing guidance for effective physical design implementation. In this paper, we present an accurate hot-spot detection method through leveling and scoring algorithm using weighted combination of image quality parameters, i.e., normalized image log-slope (NILS), mask error enhancement factor (MEEF), and depth of focus (DOF) which can be obtained through lithography simulation. Hot-spot scoring function and severity level are calibrated with process window qualification results. Least-square regression method is used to calibrate weighting coefficients for each image quality parameter. Once scoring function is obtained with wafer results, it can be applied to various designs with the same process. Using this calibrated scoring function, we generate fixing guidance and rule for the detected hot-spot area by locating edge bias value which can lead to a hot-spot free score level. Fixing guidance is generated by considering dissections information of OPC recipe. Finally, we integrated hot-spot fixing guidance display into layout editor for the effective design implementation. Applying hot-spot scoring and fixing method to memory devices of the 50nm node and below, we could achieve a sufficient process window margin for high yield mass production. © 2009 SPIE.",Scopus,2-s2.0-66749186585
English,Article,2012,"Yang Q.X., Zhu Y.P.",Characterization of multiplier spaces by wavelets and logarithmic Morrey spaces,"Nonlinear Analysis, Theory, Methods and Applications",75,13,,4920,4935,,7,10.1016/j.na.2012.04.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862127503&doi=10.1016%2fj.na.2012.04.007&partnerID=40&md5=4cdacecf61c71bfae13d17c9d67ab0c6,"In this paper, we consider multipliers from Sobolev spaces to Lebesgue spaces. We establish some wavelet characterization of multiplier spaces without using capacity. Further, we give a sharp logarithmic Morrey space condition for multipliers which lessens Fefferman's Morrey space condition to the logarithm level and generalizes Lemarié's counter-example to non-integer cases and expresses his results in a more precise way. © 2012 Elsevier Ltd. All rights reserved.",Scopus,2-s2.0-84862127503
English,Article,2011,Kidonakis N.,Top quark rapidity distribution and forward-backward asymmetry,"Physical Review D - Particles, Fields, Gravitation and Cosmology",84,1,11504,,,,68,10.1103/PhysRevD.84.011504,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960986335&doi=10.1103%2fPhysRevD.84.011504&partnerID=40&md5=0e6e39f52c4dcfcd2d9f50bfe777e06a,"I present results for the top quark rapidity distribution at Large Hadron Collider and Tevatron energies, including higher-order corrections from threshold resummation. Approximate next-to-next-to-leading-order (NNLO) results are obtained by adding the NNLO soft-gluon corrections at next-to-next-to- leading-logarithm level to the exact next-to-leading-order calculation. Theoretical predictions are shown for the rapidity distribution, including the scale dependence of the distributions. The forward-backward asymmetry at the Tevatron is also calculated. © 2011 American Physical Society.",Scopus,2-s2.0-79960986335
English,Article,2016,"Cadorette-Breton Y., Hébert C., Ibarzabal J., Berthiaume R., Bauce É.",Vertical distribution of three longhorned beetle species (Coleoptera: Cerambycidae) in burned trees of the boreal forest,Canadian Journal of Forest Research,46,4,,564,571,,3,10.1139/cjfr-2015-0402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962722603&doi=10.1139%2fcjfr-2015-0402&partnerID=40&md5=505c452b9955adac6a9a9db700f479a8,"This study aimed to characterize the vertical distribution of longhorned beetle larvae in burned trees of the eastern Canadian boreal forest. Black spruce (Picea mariana (Mill.) Britton, Sterns & Poggenb.) and jack pine (Pinus banksiana Lamb.) trees burned at three severity levels were cut, and 30 cm boles were collected from the ground up to a height of 9.45 m. Boles were debarked and dissected to collect insect larvae. Results show that the three most abundant longhorned beetle species were vertically segregated among burned jack pine and black spruce trees, but the section having the highest timber value was heavily infested by woodborer larvae. Larval density distribution of Monochamus scutellatus scutellatus (Say) and of Acmaeops proteus proteus (Kirby) could be linked with bark thickness, which also depends on fire severity. Lightly burned stands of black spruce were the most heavily infested and should be salvaged only if they are easily accessible and can thus be rapidly harvested and processed at the mill. More severely burned stands should be salvaged later as they will be less affected by woodborers, as should jack pine, which is lightly infested compared with black spruce. The ecological role of stumps should be further investigated because they could still have an ecological value after salvage logging as Arhopalus foveicollis (Haldeman) uses them specifically. © 2016, National Research Council of Canada. All rights reserved.",Scopus,2-s2.0-84962722603
English,Article,2010,"Vega J.A., Fernández C., Pérez-Gorostiaga P., Fonturbel T.",Response of maritime pine (Pinus pinaster Ait.) recruitment to fire severity and post-fire management in a coastal burned area in Galicia (NW Spain),Plant Ecology,206,2,,297,308,,20,10.1007/s11258-009-9643-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75949125887&doi=10.1007%2fs11258-009-9643-y&partnerID=40&md5=106c28a0f64cb9539daa08fc09c4feec,"The short-term effects of fire severity and post-fire management on maritime pine recruitment were evaluated in a mature serotinous pine stand in a coastal area of Galicia (NW Spain) burned by a wildfire occurred in the summer of 2001. Two levels of fire severity estimated by the levels of tree crown damage-scorched and unaffected crown-were compared. Seed dispersal and first cohort pine (November 2001) seedling density, before salvage logging, were significantly and positively affected by fire severity. Between November and January, a fungal attack caused a noticeable decrease in seedling density in both levels of fire severity. The first cohort survival was significantly reduced by harvesting and slash treatments carried out in February 2002. However, slash chopping favoured a new pine cohort, particularly in the unaffected crown plots, in which seedling density was significantly higher than in the scorched crown plots between July 2002 and February 2003. First cohort seedling survival and height were positively related. Fire severity levels, combined with post fire management, did not appear to determine final pine seedling density and height. Finally, reduction in seedling density caused by post-fire management did not threaten pine establishment and may reduce the need for subsequent thinning operations. © Springer Science+Business Media B.V. 2009.",Scopus,2-s2.0-75949125887
English,Article,1979,"Thompson W.A., Holling C.S., Kira D., Huang C.C., Vertinskf I.",Evaluation of alternative forest system management policies the case of the spruce budworm in New Brunswick,Journal of Environmental Economics and Management,6,1,,51,68,,8,10.1016/0095-0696(79)90020-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018440769&doi=10.1016%2f0095-0696%2879%2990020-2&partnerID=40&md5=d808bb7ab06f7a055bb8fe85ea452fa0,"A major difficulty in choosing a forest management policy that regulates yearly spraying and logging levels is the fact that such a choice involves comparisons among risky alternatives. Practical and theoretical constraints often make the specification of a utility function for the system infeasible. This paper demonstrates that by making reasonable and easily verifiable assumptions about some properties of preference profiles of participants in the system, it is possible to produce an effective algorithm for forest policy evaluation. The method proposed and applied to the case of the New Brunswick forests consists of: (1) construction of a forest simulation to generate policy contingent distributions of outcomes, and (2) employment of stochastic dominance to identify a nondominated set of policies. © 1979.",Scopus,2-s2.0-0018440769
English,Letter,1992,"Cacciari M., Deandrea A., Montagna G., Nicrosini O.",Qed structure functions: A systematic approach,EPL,17,2,,123,128,,25,10.1209/0295-5075/17/2/007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956112606&doi=10.1209%2f0295-5075%2f17%2f2%2f007&partnerID=40&md5=077e28dd6a966a92723e8f6a16563928,"The problem of calculating higher-order contributions to QED structure functions is considered. A closed form for any given perturbative order is developed by means of a systematic approach. Analytical formulae for the structure function and its generating function are explicitly computed at the leading log level up to O(α3). An alternative approach to the calculation of the n-th order contribution to the structure function, particularly useful for numerical applications, is developed. A numerical study of the relevance of third-and fourth-order hard-photon contribution is performed. © 1992 IOP Publishing Ltd.",Scopus,2-s2.0-84956112606
English,Article,1993,Cornish P.M.,"The effects of logging and forest regeneration on water yields in a moist eucalypt forest in New South Wales, Australia",Journal of Hydrology,150,2-4,,301,322,,80,10.1016/0022-1694(93)90114-O,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027790013&doi=10.1016%2f0022-1694%2893%2990114-O&partnerID=40&md5=7d2f027dc8eb1247d8c1b7bbdb0da071,"Water yields increased after logging by 150-250 mm per year in small catchments of moist old-growth eucalypt at Karuah in central New South Wales. The magnitude of this initial increase was directly related to the percentage of the catchment logged (29-79%). Where substantial vegetation removal took place in less than 20% of one catchment no increased water yield was observed. Water yields began to decline in all catchments 2-3 years after logging as regrowth eucalypts became established, and the rate of this decline was related to the mean stocking rate of eucalypt regeneration during the next 4 years. This water yield decline exceeded 250 mm in the sixth year after logging in the catchment with the highest stocking of regeneration and the highest regrowth basal area. Water yields in this catchment had declined to levels significantly below pre-logging levels by this time, supporting the notion that regrowth evapotranspiration had begun to exceed that of the old-growth forest. Patterns of declining water yield in the other catchments suggest that yields in some may also decline below pre-logging levels as regrowth evapotranspiration increases in line with increases in the basal area of the regrowth forest. Further study is required to determine the magnitude and duration of water yield reductions in these regrowth catchments, and to quantify the eucalypt growth rates and stand conditions responsible for the reductions. Nevertheless, these early results are consistent with water yield changes observed in mountain ash forest in Victoria, and support the concept of greater water use by a rapidly regenerating forest. © 1993.",Scopus,2-s2.0-0027790013
English,Article,2015,"Chiou Y.-C., Fu C.",Modeling crash frequency and severity with spatiotemporal dependence,Analytic Methods in Accident Research,5-6,,,43,58,,37,10.1016/j.amar.2015.03.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929087929&doi=10.1016%2fj.amar.2015.03.002&partnerID=40&md5=1c0b818edc220cbfe4e318f84a4270b1,"This study proposes a novel multinomial generalized Poisson model with error components and spatiotemporal dependence (ST-EMGP) to analyze multi-period crash frequency and severity data. The proposed model not only simultaneously models crash frequency and severity, but also accommodates spatial and temporal dependence (spatiotemporal dependence) by specifying a spatiotemporal function. To demonstrate the applicability of the proposed model, a case study is conducted on five consecutive years' (2004-2008) crash data of Taiwan's Freeway No. 1. Estimation results show that ST-EMGP model performs better than the models without considering spatiotemporal dependence in terms of adjusted likelihood ratio index, consistent Akaike information criterion and log-likelihood test. Additionally, the estimated ST-EMGP model shows that spatial and temporal dependences exist and correlate mutually. Spatial dependence may overstate its impact magnitude, but underestimate its impact range when temporal dependence is ignored. According to the distribution and regression results of spatiotemporal effects, temporal effects are higher in crash frequency and are mainly affected by traffic characteristics; while spatial effects are higher in severe crash severity levels and are mainly affected by geometric configuration. Obviously, the proposed model can successfully elucidate the sources of spatiotemporal dependence as well as their effects on crash frequency and severity. © 2015 Elsevier Ltd. All rights reserved.",Scopus,2-s2.0-84929087929
Portuguese,Article,2010,"De Souza S.E., Sansigolo C.A., Furtado E.L., De Jesus Jr. W.C., Oliveira R.R.",Influence of the basal canker on Eucalyptus grandis wood properties and kraft pulping [Influência do cancro basal em Eucalyptus grandis nas propriedades da madeira e polpação kraft],Scientia Forestalis/Forest Sciences,,88,,547,557,,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952211368&partnerID=40&md5=4fb4bad842f473c4013ac693e20d0c5c,"The objective of this research is to assess the influences of basal canker on wood properties for the kraft pulp production. The material consisted of seeded E. grandis trees classified into 4 levels of basal canker severity (0, 1, 2 and 3) and installed in three soil types classified by texture (AQ1 and AQ2 - 10 to 15% clay, and LEm2 - 26 to 35 clay). The sampling consisted of randomly selecting five trees for each for each canker severity level and soil type, totaling 60 trees (4 canker levels x 3 soil types x 5 trees). These trees were fallen and cut into sections at 0, 25, 50, 75 and 100% of the commercial height for the collection of disk and logs of wood. The results showed that the soil texture influences in the Eucalyptus canker severity and this fact should be considered when assessing the wood properties and their final destination. The texture of the soil and the severe levels of basal canker influence the wood properties, and therefore the kraft pulping.",Scopus,2-s2.0-79952211368
English,Article,2011,"El-Basyouny K., Sayed T.",A full Bayes multivariate intervention model with random parameters among matched pairs for before-after safety evaluation,Accident Analysis and Prevention,43,1,,87,94,,64,10.1016/j.aap.2010.07.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650020622&doi=10.1016%2fj.aap.2010.07.015&partnerID=40&md5=1871be3a0cd37094b232dddce0aca50e,"The objective of this study is to evaluate the safety performance of a sample of intersections that have been improved with the implementation of certain safety countermeasures in the Greater Vancouver area. A full Bayes approach is utilized to determine the effectiveness of the improvements using a before-after design with matched (yoked) comparison groups. A multivariate Poisson-lognormal intervention model is used for the analysis of crash counts by severity levels. The model is extended to incorporate random parameters to account for the correlation between sites within comparison-treatment pairs. The full Bayes analysis revealed that incorporating such design features as matched comparison groups in the specification of safety performance functions can significantly improve the fit, while reducing the estimates of the extra-Poisson variation. As well, such extended models can be used to account for heterogeneity due to unobserved road geometrics, traffic characteristics, environmental factors and driver behavior. The results showed that the overall odds ratios for injuries and fatalities (I + F) and property damage only (PDO) imply significant reductions in predicted crash counts of 23% and 15%, respectively. The corresponding credible intervals were (12%, 33%) and (6%, 24%) at the 0.95 confidence level. The majority of the site-level odds ratio exhibited reductions in both I + F and PDO predicted crash counts. However, only some of these reductions were significant. As well, the effectiveness of the treatment seems to vary by severity level from one location to another. For I + F, the crash reduction factors were 29%, 15% and 21% for improving signal visibility, left turn phase improvement and left turn lane installation, respectively. The corresponding crash reduction factors for PDO were 21%, 4% and 20%, respectively. © 2010 Elsevier Ltd.",Scopus,2-s2.0-78650020622
English,Article,2007,"Augustin A., Sahel J.-A., Bandello F., Dardennes R., Maurel F., Negrini C., Hieke K., Berdeaux G., Chaine G., Weber M., Quentel G., Cohen S.Y., Mauget-Faysse M., Brasseur G., Korobelnik J.-F., Benchaboune M., Charlin J.-F., Weinhold K., Kaut S., Hyppa M., Jurgeit-Wippermann A., Incorvaia C., Polito A., Menchini U., Capobianco B., Boscia F., Malerba E., Setaccioli M., Varano M., Schiano-Lomoriello D.",Anxiety and depression prevalence rates in age-related macular degeneration,Investigative Ophthalmology and Visual Science,48,4,,1498,1503,,84,10.1167/iovs.06-0761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248355564&doi=10.1167%2fiovs.06-0761&partnerID=40&md5=7e4a318f44b798237fb961a3d3915338,"PURPOSE. To estimate the prevalence rates of depression and anxiety in patients with wet age-related macular degeneration (AMD) and the relationship with visual acuity and to develop a simple algorithm for depression screening. METHODS. This cross-sectional, prospective, observational, multicenter study was performed in France, Germany, and Italy. Retina specialists at 10 centers per country each enrolled 12 consecutive patients with wet ARMD. Patients were stratified into four severity groups by using best eye (BE) and worst eye (WE) visual acuity (VA) thresholds (BE:VA 20/40 and WE:VA 20/200). Patients rated themselves on the Hospital Anxiety and Depression Scale (HADS). Analysis of variance was performed to estimate the effect of VA severity levels on HADS scores adjusted on age, gender, and country. RESULTS. Patients (females 60%) were recruited, with a mean age of 77 years and 2.3 years' disease duration. Mean BE:VA at inclusion was 0.49 logMar (logarithm of the minimum angled of resolution) and WE:VA 1.0 logMar. The prevalence of severe depression increased from 0% (BE:VA ≥ 20/40+WE:VA ≥ 20/200) to 7.6% (BE:VA < 20/40+WE:VA < 20/200), whereas anxiety was unrelated to VA loss. Moreover, total depression scores were strongly associated with VA severity (P = 0.006), but not total anxiety scores (P = 0.840). Responses to two HADS items (""I still enjoy things I used to enjoy""; ""I can enjoy a good book or radio or television program"") identified 95% of severely to moderately depressed patients. CONCLUSIONS. Self-rated depression in patients with AMD was associated with VA severity level. It should, therefore, be relatively easy for ophthalmologists to implement the screening procedure and refer identified patients to psychiatrists for proper assessment and treatment. Copyright © Association for Research in Vision and Ophthalmology.",Scopus,2-s2.0-34248355564
English,Article,2021,"Fischer M., Pikalo J., Beer M., Blome S.",Stability of African swine fever virus on spiked spray-dried porcine plasma,Transboundary and Emerging Diseases,,,,,,,,10.1111/tbed.14192,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108805603&doi=10.1111%2ftbed.14192&partnerID=40&md5=322767416e8897ac484246b4e0cd889f,"African swine fever (ASF) is a viral disease that affects members of the Suidae family. The notifiable disease is considered a major threat to the pig industry, animal health, and food security worldwide. According to the European Food Safety Authority, ASF virus (ASFV) survival and transmission in feed and feed materials is a major research gap. Against this background, the objective of this study was to determine the survival of ASFV on spiked spray-dried porcine plasma (SDPP) when stored at two different temperatures. To this means, commercial SDPP granules were contaminated with high titers of ASFV in a worst-case external contamination scenario. Three samples per time point and temperature condition were subjected to blind passaging on macrophage cultures and subsequent haemadsorption test to determine residual infectivity. In addition, viral genome was detected by real-time PCR. The results indicate that heavily contaminated SDPP stored at 4°C remains infectious for at least 5 weeks. In contrast, spiked SDPP stored at room temperature displayed a distinct ASFV titer reduction after 1 week (>2.8 log levels) and complete inactivation after 2 weeks (>5.7 log levels). In conclusion, the residual risk of ASFV transmission through externally contaminated SDPP is low if SDPP is stored at room temperature (21 ± 2°C) for a period of at least 2 weeks before feeding. © 2021 The Authors. Transboundary and Emerging Diseases published by Wiley-VCH GmbH",Scopus,2-s2.0-85108805603
English,Article,2021,"Meurle T., Knaus J., Barbano A., Hoenes K., Spellerberg B., Hessling M.",Photoinactivation of staphylococci with 405 nm light in a trachea model with saliva substitute at 37◦ c,Healthcare (Switzerland),9,3,310,,,,,10.3390/healthcare9030310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104416100&doi=10.3390%2fhealthcare9030310&partnerID=40&md5=a0195a5dbd14864531366d903b545484,"The globally observed rise in bacterial resistance against antibiotics has increased the need for alternatives to antibiotic treatments. The most prominent and important pathogen bacteria are the ESKAPE pathogens, which include among others Staphylococcus aureus, Klebsiella pneumoniae and Acinetobacter baumannii. These species cause ventilator-associated pneumonia (VAP), which accounts for 24% of all nosocomial infections. In this study we tested the efficacy of photoinactivation with 405 nm violet light under conditions comparable to an intubated patient with artificial saliva for bacterial suspension at 37◦ C. A technical trachea model was developed to investigate the visible light photoinactivation of Staphylococcus carnosus as a non-pathogen surrogate of the ESKAPE pathogen S. aureus (MRSA). The violet light was coupled into the tube with a fiber optic setup. The performed tests proved, that photoinactivation at 37◦ C is more effective with a reduction of almost 3 log levels (99.8%) compared to 25◦ C with a reduction of 1.2 log levels. The substitution of phosphate buffered saline (PBS) by artificial saliva solution slightly increased the efficiency during the experimental course. The increased efficiency might be caused by a less favorable environment for bacteria due to for example the ionic composition. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Scopus,2-s2.0-85104416100
English,Conference Paper,2014,"Hönes K., Stangl F., Sift M., Hessling M.",Visible optical radiation generates bactericidal effect applicable for inactivation of health care associated germs demonstrated by inactivation of E. coli and B. subtilis using 405 nm and 460 nm light emitting diodes,Optics InfoBase Conference Papers,,,,,,9,,10.1117/12.2183903,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019505522&doi=10.1117%2f12.2183903&partnerID=40&md5=23356026357a6796dcb5537d69d602f0,"The Ulm University of Applied Sciences is investigating a technique using visible optical radiation (405 nm and 460 nm) to inactivate health-hazardous bacteria in water. A conceivable application could be point-of-use disinfection implementations in developing countries for safe drinking water supply. Another possible application field could be to provide sterile water in medical institutions like hospitals or dental surgeries where contaminated pipework or long-term disuse often results in higher germ concentrations. Optical radiation for disinfection is presently mostly used in UV wavelength ranges but the possibility of bacterial inactivation with visible light was so far generally disregarded. One of the advantages of visible light is, that instead of mercury arc lamps, light emitting diodes could be used, which are commercially available and therefore cost-efficient concerning the visible light spectrum. Furthermore they inherit a considerable longer life span than UV-C LEDs and are non-hazardous in contrast to mercury arc lamps. Above all there are specific germs, like Bacillus subtilis, which show an inactivation resistance to UV-C wavelengths. Due to the totally different deactivation mechanism even higher disinfection rates are reached, compared to Escherichia coli as a standard laboratory germ. At 460 nm a reduction of three log-levels appeared with Bacillus subtilis and a half log-level with Escherichia coli both at a dose of about 300 J/cm2. With the more efficient wavelength of 405 nm four and a half loglevels are reached with Bacillus subtilis and one and a half log-level with Escherichia coli also both at a dose of about 300 J/cm2. In addition the employed optical setup, which delivered a homogeneous illumination and skirts the need of a stirring technique to compensate irregularities, was an important improvement compared to previous published setups. Evaluated by optical simulation in ZEMAX® the designed optical element provided proven homogeneity distributions with maximum variation of ± 10%. © 2015 SPIE-OSA.",Scopus,2-s2.0-85019505522
English,Article,2017,"Echeverría L.E., Rojas L.Z., Calvo L.S., Roa Z.M., Rueda-Ochoa O.L., Morillo C.A., Muka T., Franco O.H.",Profiles of cardiovascular biomarkers according to severity stages of Chagas cardiomyopathy,International Journal of Cardiology,227,,,577,582,,12,10.1016/j.ijcard.2016.10.098,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003968600&doi=10.1016%2fj.ijcard.2016.10.098&partnerID=40&md5=2aa993fe60e047fdf6b086022a69bfd0,"Background/objectives Up 30 to 40% of Chagas patients exhibit cardiomyopathy with different degrees of cardiac involvement. Biomarkers may help in differentiation of the severity of Chagas cardiomyopathy (CCM). This study sought to examine the diagnostic value of a panel of biomarkers to distinguish the severity of (CCM). Methods 100 patients with CCM were included in this cross-sectional study. Based on electrocardiogram and echocardiogram, CCM patients were classified in three stages according to disease's severity. Levels of high-sensitivity cardiac troponin T (Hs-cTnT), N-terminal pro B-type natriuretic peptide (NT-proBNP), galectin-3 (Gal-3), neutrophil gelatinase-associated lipocalin (NGAL), soluble ST2 (sST2) and cystatin-c (Cys-c) were measured. Logistic regression models were used to assess the association between levels of natural log-transformed values of biomarkers and stages C/D versus B. We also calculated the area under curve (AUC) for each of the models. Results In models adjusted for age, sex, body mass index, kidney function and medication use, increased levels of NT-proBNP (per 1 unit natural log-transformed values, odds ratio (OR) = 5.55; 95CI%:1.65–18.72) and Hs-cTnT (per 1 unit natural log-transformed values, OR = 7.11; 95CI%:1.41–35.90) showed significant association with the severity of CCM per 1 unit increase of biomarkers. The accuracy of NT-proBNP and Hs-cTnT for diagnosis of the severity of CCM was high: AUC of 0.968 and 0.956 respectively. No significant difference was found in the AUC between NT-proBNP and Hs-cTnT. No association was found between Gal-3, NGAL, sST2 and Cys-C and severity of CCM. Conclusions NT-proBNP and Hs-cTnT have both same diagnostic value in distinguishing severity of CCM. © 2016",Scopus,2-s2.0-85003968600
English,Article,2005,Davradakis E.,Macroeconomic fundamentals and exchange rates: A non-parametric cointegration analysis,Applied Financial Economics,15,7,,439,446,,5,10.1080/09603100500056593,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17744390169&doi=10.1080%2f09603100500056593&partnerID=40&md5=81a6c315b3c23c5dce73f1ff5a893552,"This paper examines in a non-parametric setup whether a long-run relationship exists between monetary fundamentals and the dollar spot exchange rates for 19 countries. Although the Johansen's parametric approach failed to retrieve a long-relationship for any of the countries considered, the Bierens (1997a) non-parametric approach suggests that there is one cointegrating relationship for the majority of the countries considered. In addition, the [1, -1] cointegrating vector between the fundamentals and the log-level of the dollar exchange rate could not be rejected in the non-parametric formulation. © 2005 Taylor & Francis Group Ltd.",Scopus,2-s2.0-17744390169
English,Article,1992,"Wang P., Yip C.K.",Examining the long-run effect of money on economic growth,Journal of Macroeconomics,14,2,,359,369,,22,10.1016/0164-0704(92)90050-I,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38249009000&doi=10.1016%2f0164-0704%2892%2990050-I&partnerID=40&md5=d1ea02a0bd7c04023793db0aed6a216d,"Christiano and Ljungqvist (1988) find a statistically significant Granger-causal relation between money and output when data are measured in log levels, but not when they use data in log differences. To resolve this puzzle, we develop an endogenous growth model with money to study the long-run interactions between real and monetary sectors. Money is regarded as a Hicks-neutral technological factor that improves the efficiency of goods production. We examine the effects of anticipated inflation on the growth rates of real macroeconomic variables and find that money is ""superneutral"" in the growth rate sense. © 1992.",Scopus,2-s2.0-38249009000
English,Article,2012,"Amacher G.S., Ollikainen M., Koskela E.",Corruption and forest concessions,Journal of Environmental Economics and Management,63,1,,92,104,,35,10.1016/j.jeem.2011.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866557492&doi=10.1016%2fj.jeem.2011.05.007&partnerID=40&md5=3f7011f326777cd834790cdbbe9402b6,"We examine how corruption impacts a central government's application of concession policy instruments consisting of royalty rates, concession size, environmentally sensitive logging levels, and enforcement. Harvesters have incentives to illegally log by taking more volume than is allowed, high grading through removal of only the highest valued and best formed trees, and shirking environmentally sensitive logging requirements, all of which reduce public goods produced from native tropical forests. Corruption is introduced through logging inspectors who can be bribed by harvesters to avoid fines associated with illegal logging. Both the theory and a simulation are used to compare policy design under corruption and no corruption. © 2011.",Scopus,2-s2.0-84866557492
English,Article,2018,"Ye X., Wang K., Zou Y., Lord D.",A semi-nonparametric Poisson regression model for analyzing motor vehicle crash data,PLoS ONE,13,5,e0197338,,,,36,10.1371/journal.pone.0197338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047557337&doi=10.1371%2fjournal.pone.0197338&partnerID=40&md5=a70c3a80d9e716842be348bb67412ac2,"This paper develops a semi-nonparametric Poisson regression model to analyze motor vehicle crash frequency data collected from rural multilane highway segments in California, US. Motor vehicle crash frequency on rural highway is a topic of interest in the area of transportation safety due to higher driving speeds and the resultant severity level. Unlike the traditional Negative Binomial (NB) model, the semi-nonparametric Poisson regression model can accommodate an unobserved heterogeneity following a highly flexible semi-nonparametric (SNP) distribution. Simulation experiments are conducted to demonstrate that the SNP distribution can well mimic a large family of distributions, including normal distributions, log-gamma distributions, bimodal and trimodal distributions. Empirical estimation results show that such flexibility offered by the SNP distribution can greatly improve model precision and the overall goodness-of-fit. The semi-nonparametric distribution can provide a better understanding of crash data structure through its ability to capture potential multimodality in the distribution of unobserved heterogeneity. When estimated coefficients in empirical models are compared, SNP and NB models are found to have a substantially different coefficient for the dummy variable indicating the lane width. The SNP model with better statistical performance suggests that the NB model overestimates the effect of lane width on crash frequency reduction by 83.1%. © 2018 Ye et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",Scopus,2-s2.0-85047557337
English,Article,1995,"Chakraborty S., Smyth G.K.",A Stochastic Model Incorporating the Effect of Weather Conditions on Anthracnose Development in Stylosanthes scabra,Journal of Phytopathology,143,8,,495,499,,5,10.1111/j.1439-0434.1995.tb04561.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444300374&doi=10.1111%2fj.1439-0434.1995.tb04561.x&partnerID=40&md5=cbcdeaf273893c1f689964eb04c7ddaa,"Spatial and temporal progress of anthracnose caused by Colletotrichum gloeosporioides in quantitatively resistant accessions of the tropical pasture legume Stylosanthes scabra were studied in a field experiment at the Southedge Research Station, Queensland, Australia. In a previously published work a conditional ordinal logistic regression model was developed to explain the probability of a plant developing a given disease severity level, depending on its previous disease state and that of its neighbours. In the present study this model is augmented to incorporate the effects of three weather variables which were measured daily during a growing season. Two approaches were used: (a) threshold values for relative humidity (RH), rainfall and net evaporation were used to classify days as suitable or unsuitable for anthracnose growth; (b) days were assumed to vary continuously in their rate of anthracnose growth depending on the numerical values of the weather variables. High 9am RH, low net evaporation and low 9am temperatures are significantly associated with anthracnose growth. Net evaporation proved to be a better index than rainfall and heavy rainfall was not conducive to high levels of anthracnose; however, rainfall was useful once evaporation was taken into account. The effect of 9am RH can be described either by a threshold value around 70% or by a quadratic function. A two‐variable model with net evaporation and log(rain+1) explains 97.6% of the available deviance. Copyright © 1995, Wiley Blackwell. All rights reserved",Scopus,2-s2.0-4444300374
English,Article,1987,"Robinson Steven W., Bizanti M.S.",PC PROGRAM CALCULATES WELL BORE TRAJECTORY.,Oil and Gas Journal,85,28,,81,84,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023384666&partnerID=40&md5=cdb60037df8d2d6c18dd79d6e50018c0,"A basic program for the IBM personal computer (PC) calculates the trajectory of a wellbore. The program employs the three most widely used methods of solution: tangential, angle-averaging, and radius of curvature. When using the program, one can easily determine the relative accuracy of each method. The program calculates the following: true vertical depth (TVD), north or south departure, east or west departure, total departure, departure angle, and dog log severity. The input data include measured depth of survey, hole inclination angle (degrees), and the azimuth. The corresponding variables are DEPTH(I), IN(I), and A(I), respectively. The string variable is used for the azimuth to distinguish between north and south or east and west departures which is determined with LEFT and RIGHT functions and assigned to the variables NS and EW. The azimuth angle is then assigned to A(I). The survey interval is assigned to the variable MD(I).",Scopus,2-s2.0-0023384666
English,Article,2007,"Lindblad M., Lindmark H., Thisted Lambertz S., Lindqvist R.",Microbiological baseline study of swine carcasses at Swedish slaughterhouses,Journal of Food Protection,70,8,,1790,1797,,33,10.4315/0362-028X-70.8.1790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547753414&doi=10.4315%2f0362-028X-70.8.1790&partnerID=40&md5=323d5917d1b073a1c7c2fe6484d4db99,"This 13-month survey was conducted to estimate the prevalence and counts of foodborne pathogenic bacteria and indicator bacteria on swine carcasses in Sweden. A total of 541 swine carcasses were sampled by swabbing prechill at the 10 largest slaughterhouses in Sweden. Pathogenic Yersinia enterocolitica was detected by PCR in 16% of the samples. The probability of finding Y. enterocolitica increased with increasing counts of Escherichia coli. No samples were positive for Salmonella. The prevalences of Campylobacter, Listeria monocytogenes, and verocytotoxin-producing E. coli were low (1, 2, and 1%, respectively). None of the verocytotoxin-positive enrichments, as determined by a reverse passive latex agglutination assay, tested positive for the virulence genes eaeA or hlyA by PCR. Coagulase-positive staphylococci, E. coli, and Enterobacteriaceae were recovered from 30, 57, and 87% of the samples, respectively, usually at low levels (95th percentiles, 0.79, 1.09, and 1.30 log CPU/cm2, respectively). The mean log level of Enterobacteriaceae was 0.35 log CPU/cm2 higher than that of E. coli on carcasses positive for both bacteria. The mean log level of aerobic microorganisms was 3.48 log CPU/cm2, and the 95th percentile was 4.51 log CPU/cm2. These data may be useful for risk assessment purposes and can serve as a basis for risk management actions, such as the use of E. coli as an alternative indicator organism for process hygiene control. Copyright ©, International Association for Food Protection.",Scopus,2-s2.0-34547753414
English,Article,2019,"Zeng Y., Chen J., Shang W., Chen T.-H.P.",Studying the characteristics of logging practices in mobile apps: a case study on F-Droid,Empirical Software Engineering,24,6,,3394,3434,,12,10.1007/s10664-019-09687-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061227403&doi=10.1007%2fs10664-019-09687-9&partnerID=40&md5=e7e8e8f271b8fd2e1d0d11ba2ce4e082,"Logging is a common practice in software engineering. Prior research has investigated the characteristics of logging practices in system software (e.g., web servers or databases) as well as desktop applications. However, despite the popularity of mobile apps, little is known about their logging practices. In this paper, we sought to study logging practices in mobile apps. In particular, we conduct a case study on 1,444 open source Android apps in the F-Droid repository. Through a quantitative study, we find that although mobile app logging is less pervasive than server and desktop applications, logging is leveraged in almost all studied apps. However, we find that there exist considerable differences between the logging practices of mobile apps and the logging practices in server and desktop applications observed by prior studies. In order to further understand such differences, we conduct a firehouse email interview and a qualitative annotation on the rationale of using logs in mobile app development. By comparing the logging level of each logging statement with developers’ rationale of using the logs, we find that all too often (35.4%), the chosen logging level and the rationale are inconsistent. Such inconsistency may prevent the useful runtime information to be recorded or may generate unnecessary logs that may cause performance overhead. Finally, to understand the magnitude of such performance overhead, we conduct a performance evaluation between generating all the logs and not generating any logs in eight mobile apps. In general, we observe a statistically significant performance overhead based on various performance metrics (response time, CPU and battery consumption). In addition, we find that if the performance overhead of logging is significantly observed in an app, disabling the unnecessary logs indeed provides a statistically significant performance improvement. Our results show the need for a systematic guidance and automated tool support to assist in mobile logging practices. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Scopus,2-s2.0-85061227403
English,Article,2010,"Ciafaloni P., Urbano A.",Infrared weak corrections to strongly interacting gauge boson scattering,"Physical Review D - Particles, Fields, Gravitation and Cosmology",81,8,85033,,,,7,10.1103/PhysRevD.81.085033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952970420&doi=10.1103%2fPhysRevD.81.085033&partnerID=40&md5=29ae8e6078de7c621fa57de0c0a59caf,"We evaluate the impact of electroweak corrections of infrared origin on strongly interacting longitudinal gauge boson scattering, calculating all-order resummed expressions at the double log level. As a working example, we consider the standard model with a heavy Higgs. At energies typical of forthcoming experiments (LHC, International Linear Collider, Compact Linear Collider), the corrections are in the 10%-40% range, with the relative sign depending on the initial state considered and on whether or not additional gauge boson emission is included. We conclude that the effect of radiative electroweak corrections should be included in the analysis of longitudinal gauge boson scattering. © 2010 The American Physical Society.",Scopus,2-s2.0-77952970420
English,Article,2001,"Ericsson N.R., Irons J.S., Tryon R.W.",Output and inflation in the long run,Journal of Applied Econometrics,16,3,,241,253,,46,10.1002/jae.614,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035626701&doi=10.1002%2fjae.614&partnerID=40&md5=db37a857ffbd171f245fc77b6b189022,"Cross-country regressions explaining output growth often obtain a negative effect from inflation. However, that result is not robust, due to the selection of countries in sample, temporal aggregation, and omission of consequential variables in levels. This paper demonstrates some implications of these mis-specifications, both analytically and empirically. In particular, for most G-7 countries, annual time series of inflation and the log-level of output are cointegrated, thus rejecting the existence of a long-run relation between output growth and inflation. Typically, output and inflation are positively related in these cointegrating relationships: a price markup model helps to interpret this surprising feature. Copyright © 2001 John Wiley & Sons, Ltd.",Scopus,2-s2.0-0035626701
English,Article,1992,Lewbel A.,Aggregation with Log-Linear Models,Review of Economic Studies,59,3,,635,642,,40,10.2307/2297869,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963002149&doi=10.2307%2f2297869&partnerID=40&md5=210e36c1eeec084cf61b4bea101436bb,"When economic theory suggests a log-linear specification for individual agents, e.g., Cobb- Douglas production, it is common to estimate the same log-linear model with aggregate data, invoking a representative agent assumption and thereby assuming away aggregation errors. This paper gives necessary and sufficient restrictions on the distribution of agents in an economy for log-linear agent models to aggregate into log-linear macro models, and discusses the aggregation bias resulting from violation of these restrictions. Theorems, tests, economic rationales, and empirical results are given. Included are connections to random walks and to cointegration. Analogous results for log-level models are derived. © 1992 The Review of Economic Studies Limited.",Scopus,2-s2.0-84963002149
English,Article,1988,"Raj B., Siklos P.L.",SOME QUALMS ABOUT THE TEST OF THE INSTITUTIONALIST HYPOTHESIS OF THE LONG‐RUN BEHAVIOR OF VELOCITY,Economic Inquiry,26,3,,537,545,,7,10.1111/j.1465-7295.1988.tb01515.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977409186&doi=10.1111%2fj.1465-7295.1988.tb01515.x&partnerID=40&md5=9d3a91f5a1637dbf1c09b68e3b78f046,"This paper expresses econometric qualms about Bordo and Jonung's [1981] analysis of long‐run velocity. They did not recognize that, for U.S. and Canadian data, the log of velocity has a unit root. Hence, estimation of a log level regression may produce spurious regressions. When Bordo and Jonung's velocity equation is reestimated in rate of change form, permanent income is significant, contrary to their earlier conclusion. Moreover, using this approach gives a stronger result for one of the institutional variables in the velocity function, in the sense that the remaining variables become more significant. Copyright © 1988, Wiley Blackwell. All rights reserved",Scopus,2-s2.0-84977409186
English,Article,2020,"Liu Q., Fang X., Tokuno S., Chung U., Chen X., Dai X., Liu X., Xu F., Wang B., Peng P.",A web visualization tool using T cell subsets as the predictor to evaluate COVID-19 patient's severity,PLoS ONE,15,9 September,e0239695,,,,1,10.1371/journal.pone.0239695,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091619335&doi=10.1371%2fjournal.pone.0239695&partnerID=40&md5=66b53ec506b566af559ea793e0379c1e,"Wuhan, China was the epicenter of the 2019 coronavirus outbreak. As a designated hospital for COVID-19, Wuhan Pulmonary Hospital has received over 700 COVID-19 patients. With the COVID-19 becoming a pandemic all over the world, we aim to share our epidemiological and clinical findings with the global community. We studied 340 confirmed COVID-19 patients with clear clinical outcomes from Wuhan Pulmonary Hospital, including 310 discharged cases and 30 death cases. We analyzed their demographic, epidemiological, clinical and laboratory data and implemented our findings into an interactive, free access web application to evaluate COVID-19 patient's severity level. Our results show that baseline T cell subsets results differed significantly between the discharged cases and the death cases in Mann Whitney U test: Total T cells (p < 0.001), Helper T cells (p <0.001), Suppressor T cells (p <0.001), and TH/TSC (Helper/Suppressor ratio, p<0.001). Multivariate logistic regression model with death or discharge as the outcome resulted in the following significant predictors: Age (OR 1.05, 95% CI, 1.00 to 1.10), underlying disease status (OR 3.42, 95% CI, 1.30 to 9.95), Helper T cells on the log scale (OR 0.22, 95% CI, 0.12 to 0.40), and TH/TSC on the log scale (OR 4.80, 95% CI, 2.12 to 11.86). The AUC for the logistic regression model is 0.90 (95% CI, 0.84 to 0.95), suggesting the model has a very good predictive power. Our findings suggest that while age and underlying diseases are known risk factors for poor prognosis, patients with a less damaged immune system at the time of hospitalization had higher chance of recovery. Close monitoring of the T cell subsets might provide valuable information of the patient's condition change during the treatment process. Our web visualization application can be used as a supplementary tool for the evaluation. © 2020 Liu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",Scopus,2-s2.0-85091619335
English,Article,2019,"Caballer-Tarazona V., Guadalajara-Olmeda N., Vivas-Consuelo D.",Predicting healthcare expenditure by multimorbidity groups,Health Policy,123,4,,427,434,,4,10.1016/j.healthpol.2019.02.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061671225&doi=10.1016%2fj.healthpol.2019.02.002&partnerID=40&md5=390cca7774358b5089dcc5abffb77a1c,"Objectives: This article has two main purposes. Firstly, to model the integrated healthcare expenditure for the entire population of a health district in Spain, according to multimorbidity, using Clinical Risk Groups (CRG). Secondly, to show how the predictive model is applied to the allocation of health budgets. Methods: The database used contains the information of 156,811 inhabitants in a Valencian Community health district in 2013. The variables were: age, sex, CRG's main health statuses, severity level, and healthcare expenditure. The two-part models were used for predicting healthcare expenditure. From the coefficients of the selected model, the relative weights of each group were calculated to set a case-mix in each health district. Results: Models based on multimorbidity-related variables better explained integrated healthcare expenditure. In the first part of the two-part models, a logit model was used, while the positive costs were modelled with a log-linear OLS regression. An adjusted R 2 of 46–49% between actual and predicted values was obtained. With the weights obtained by CRG, the differences found with the case-mix of each health district proved most useful for budgetary purposes. Conclusions: The expenditure models allowed improved budget allocations between health districts by taking into account morbidity, as opposed to budgeting based solely on population size. © 2019 Elsevier B.V.",Scopus,2-s2.0-85061671225
English,Article,2021,"Berger W.J., III, Metz Z.I., Ul-Hadi S., Thomson J., Keenan J., Wedding D., Nguyen T.",A system for monitoring a marine well for shallow water flow: Development of early detection,Interpretation,9,2,,T395,T405,,,10.1190/INT-2019-0092.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106884963&doi=10.1190%2fINT-2019-0092.1&partnerID=40&md5=66afcf27041b58ce07a33b3b2c623287,"Deepwater basins around the world contain shallow sequences of overpressured, sand-prone sediments that can result in shallow water flow (SWF) events. These events have frequently resulted in wellbore instability and increased man-hour exposure to potential health, safety, security, and environment risks, as well as nonproductive time, and they have sometimes been the cause of the loss of a well while drilling the shallow (riserless) section for oil and gas exploration or development projects. Methods previously established to classify the magnitude of an SWF event have been used with partial success to identify the onset of an SWF event. The need existed to develop a system enabling early prediction, detection, and mitigation of SWF events while drilling. Real-time monitoring of the riserless section of a marine well for SWF requires a system using a plurality of data feeds that we defined as the SYSTEM. The data feeds include seismic data, remotely operated vehicle video, and surface and downhole logging measurements. An SWF surveillance methodology, which we defined as a discharge category model (DCM), has been developed for early detection of an SWF event, prior to the onset of wellbore instability. DCM focuses on baseline discharge categories (ranging from no flow to minor flow) prior to wellbore instability and taking into account the U-tube effects. Real-time monitoring of data feeds coupled with DCM in the context of SYSTEM has helped to mitigate SWF events. There have been no wells lost due to SWF events that have used DCM in the context of SYSTEM in various basins throughout the world. In total, 154 wells have been monitored globally using DCM with 46 SWF events detected and mitigated before reaching a severity level that might compromise well integrity from 2012 to 2019. © 2021 Society of Exploration Geophysicists and American Association of Petroleum Geologists",Scopus,2-s2.0-85106884963
English,Article,2021,"Vatter P., Hoenes K., Hessling M.",Photoinactivation of the Coronavirus Surrogate phi6 by Visible Light,Photochemistry and Photobiology,97,1,,122,125,,1,10.1111/php.13352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096722163&doi=10.1111%2fphp.13352&partnerID=40&md5=2eda047fee4fed20b29957817a2719ae,"To stop the coronavirus spread, new inactivation approaches are being sought that can also be applied in the presence of humans or even on humans. Here, we investigate the effect of visible violet light with a wavelength of 405 nm on the coronavirus surrogate phi6 in two aqueous solutions that are free of photosensitizers. A dose of 1300 J cm−2 of 405 nm irradiation reduces the phi6 plaque-forming unit concentration by three log-levels. The next step should be similar visible light photoinactivation investigations on coronaviruses, which cannot be performed in our lab. © 2020 The Authors. Photochemistry and Photobiology published by Wiley Periodicals LLC on behalf of American Society for Photobiology.",Scopus,2-s2.0-85096722163
English,Conference Paper,2018,"Kardos A., Kluth S., Somogyi G., Tulipánt Z., Verbytskyi A.",Precise determination of αS(mZ0) from a global fit of energy-energy correlations to NNLO+NNLL predictions,"Proceedings - 48th International Symposium on Multiparticle Dynamics, ISMD 2018",,,5002,,,,,10.1051/epjconf/201920605002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069785294&doi=10.1051%2fepjconf%2f201920605002&partnerID=40&md5=d942343b56491dedaa9d977586666920,"We present a determination of the strong coupling constant αS(mZ0) using a global fit of theory predictions in next-to-next-next-leading-order (NNLO) combined with resummed predictions at the next-to-next-leading-log level (NNLL) [1]. The predictions are compared to distributions of energy-energy correlations measured in e+e− annihilation to hadronic final states by experiments at the e+e− colliders LEP, PETRA, TRISTAN and PEP. The predictions are corrected for hadronisation effects using the modern generator programs Sherpa 2.2.4 and Herwig 7.1.1. © The Authors, published by EDP Sciences.",Scopus,2-s2.0-85069785294
English,Article,2011,"Zimmermann J.L., Dumler K., Shimizu T., Morfill G.E., Wolf A., Boxhammer V., Schlegel J., Gansbacher B., Anton M.",Effects of cold atmospheric plasmas on adenoviruses in solution,Journal of Physics D: Applied Physics,44,50,505201,,,,102,10.1088/0022-3727/44/50/505201,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455217439&doi=10.1088%2f0022-3727%2f44%2f50%2f505201&partnerID=40&md5=661c458a3a572fbd04173fc012a0376f,"Experiments were performed with cold atmospheric plasma (CAP) to inactivate adenovirus, a non-enveloped double stranded DNA virus, in solution. The plasma source used was a surface micro-discharge technology operating in air. Various plasma diagnostic measurements and tests were performed in order to determine the efficacy of CAPs and to understand the inactivation mechanism(s). Different stages of the adenovirus life cycle were investigatedinfectivity and gene expression as well as viral replication and spread. Within 240s of CAP treatment, inactivation of up to 6 decimal log levels can be achieved. © 2011 IOP Publishing Ltd.",Scopus,2-s2.0-83455217439
English,Article,1993,"Baer H., Reno M.H.",QCD corrections to leptonic and hadronic observables from pp̄→W+X→τ̄ντX,Physical Review D,47,9,,3906,3912,,1,10.1103/PhysRevD.47.3906,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4243434419&doi=10.1103%2fPhysRevD.47.3906&partnerID=40&md5=34c5016cdd8f97d0ede7eae3de906371,"We set up a formalism for calculating the O(αs) corrections to the process pp̄→W+X→τ̄ντX with spin-correlated τ decays to leptons and mesons. Our results are applicable to Monte Carlo integration, which allows easy construction of any desired observable at next-to-leading-log level, and the possibility to include experimental cuts. Our results are applied explicitly to the decay modes τ→ν̄τēνe, τ̄→ν̄τπ+, and τ̄→ν̄τπ+π0; other decay modes may be included in a straightforward fashion. We show results for transverse momentum and rapidity variables in leading-log and next-to-leading-log approximations; the leptonic observables are compared to similar observables from direct W→ēνe. © 1993 The American Physical Society.",Scopus,2-s2.0-4243434419
English,Article,1993,"Flaig G., Steiner V.","Searching for the ""productivity slowdown': some surprising findings from West German manufacturing",Review of Economics & Statistics,75,1,,57,65,,11,10.2307/2109626,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027739105&doi=10.2307%2f2109626&partnerID=40&md5=8fb60fd9af5366ea2625584a50f694f6,"We test the hypothesis of a negative long-term trend and/or a structural break in total factor productivity (TFP) after the first oil price shock for West German manufacturing industries within an econometric model based on a flexible cost function with capital as a quasi-fixed factor. After adjusting TFP growth for scale economies and varying capacity utilization this hypothesis is not supported by our empirical findings for the great majority of industries studied, whereas the hypothesis that the (log-)level of TFP follows a random walk with drift is not rejected by various statistical tests. -Authors",Scopus,2-s2.0-0027739105
English,Article,1988,"Christiano L.J., Ljungqvist L.",Money does Granger-cause output in the bivariate money-output relation,Journal of Monetary Economics,22,2,,217,235,,87,10.1016/0304-3932(88)90020-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000467517&doi=10.1016%2f0304-3932%2888%2990020-7&partnerID=40&md5=ee743d3bdf1c7edeee4dec2e9a941785,"A bivariate Granger-causality test on money and output finds statistically significant causality when data are measured in log levels, but not when they are measured in first differences of the logs. Bootstrap simulation experiments indicate that, most probably, the first difference results reflect lack of power, whereas the level results reflect Granger-causality that is actually in the data The reason for the lack of power in the first difference F-statistic is that first differencing the data appears to entail a specification error. By showing that money does Granger-cause output in the bivariate relation, we remove a potential embarrassment for models that assign an important role to money in business fluctuations. © 1988.",Scopus,2-s2.0-0000467517
English,Article,1983,"Bigi I.I., Frère J.-M.",Strong radiative corrections to strangeness-changing processes in the presence of right-handed currents,Physics Letters B,129,6,,469,472,,18,10.1016/0370-2693(83)90141-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48749149060&doi=10.1016%2f0370-2693%2883%2990141-7&partnerID=40&md5=8827b7cf95aefa3b4a2cbea6b237477f,"In the presence of weak right-handed currents the GIM suppression becomes less efficient in the effective ΔS=2 operator. Therefore new features arise and have to be taken into account when applying strong radiative corrections. We determine these QCD corrections to the ΔS=2 operator on the leading log level and (in contrast to the standard model) find a sizeable enhancement. If one ascribes CP violation to the presence of right-handed currents, then the parameter ε{lunate} will enjoy the same enhancement. We show however that ε{lunate}′ is increased in a similar way; the ratio ε{lunate}′ ε{lunate} is therefore not significantly affected. © 1983.",Scopus,2-s2.0-48749149060
English,Article,2020,"Lozada Dávila J.R., Soriano P., Costa M., García-Quintero A.M., Sánchez D., Villarreal A., Arends E.",Long-term carbon stock recovery in a neotropical-logged forest,Plant Biosystems,154,2,,241,247,,,10.1080/11263504.2019.1591537,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064738728&doi=10.1080%2f11263504.2019.1591537&partnerID=40&md5=b545976aa9ea96e77e695b1dcb2a9d08,"This article assesses the effect of different logging levels on loss of above-ground biomass and the contribution of different ecological groups of species in the long-term recovery of C stocks. A randomized complete blocks design was established in 1987 with three felling treatments: diameter above 20 cm, 40 cm and 60 cm. All the trees and palms bigger than 10 cm dbh were measured. The average stock in old growth forests was 84.3 t C/ha. Low- and medium-impact treatments led to C reductions in 10% and 44%, respectively; for low impact, a significant increase was detected after the 25-year measurements, which could be related to a possible CO2 fertilization effect. For high impact, 79% of C was lost, but the original level recovered after 20 years. The ecological succession process, subsequent to logging, is an efficient mechanism to restore C stocks. © 2019, © 2019 Societá Botanica Italiana.",Scopus,2-s2.0-85064738728
English,Conference Paper,2014,"Gomathy M., Devi V.K., Meenakshi D.",Developing an error logging framework for ruby on rails application using AOP,"Proceedings - 2014 World Congress on Computing and Communication Technologies, WCCCT 2014",,,6755103,44,49,,,10.1109/WCCCT.2014.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899461489&doi=10.1109%2fWCCCT.2014.19&partnerID=40&md5=9d9f81ca67310b173ac42ae33d4d9860,"A framework for detecting and recording the flaws that happen during the usage of web applications is designed and a library functionality to perform this is discussed in this paper. The recorded information can be stored at different levels of detail, commonly called the logging levels. For some modules more than others, it may be required to store more detailed information about any error that arises during its usage according to its importance. A Web Application also needs to print the stack trace containing the error information on the web page when an error occurs for the user to understand the nature of the error. When dealing with legacy web applications, it is difficult to insert code. The proposed and designed framework is tested with a web application called Kic Kart. © 2014 IEEE.",Scopus,2-s2.0-84899461489
Portuguese,Article,2020,"Pio F.P.B., Vieira E.M.","Determination of areas affected by burns in watersheds by the queimada index (Nbr), case study of the piracicaba-mg river basin [Determinação das áreas atingidas por queimadas em bacias hidrográficas por meio do índice de queimada (Nbr), estudo de caso da bacia do rio piracicaba-mg]",Revista Brasileira de Geografia Fisica,13,1,,87,101,,1,10.26848/rbgf.v13.1.p087-101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100152822&doi=10.26848%2frbgf.v13.1.p087-101&partnerID=40&md5=ba8c52d0cb5956770047bf5d9e12079e,"The development of mankind and consequent alteration in land use has made areas increasingly susceptible to forest fires, a fact aggravated by the practice of burning. Burning events are considered worrisome due to the extent of the impacts that influence, including climate change. Thus, remote sensing techniques can be used to identify and spatialize burned areas. The severity of the impacts generated makes visible the importance of studies capable of detecting burn scars in order to contribute to the development of monitoring techniques, awareness, prevention and recovery of affected areas. Thus, the present study aimed to generate the burn rate (NBR) as well as its variation (ΔNBR) and the Normalized Water Difference Index (NDWI) to distinguish between burned areas and wetlands for the Piracicaba river basin region, Minas Gerais, using Landsat 8 scenes orbit / point 217/074 and 218/074 from the months of August and September 2016. The efficiency of the method for the detection of burned areas was analyzed from the comparison with burned polygons of the INPE and fire occurrence points provided by the 4th Itabira Military Fire Squad. The results of ΔNBR were classified in severity levels and were efficient for the detection of burned areas when compared to INPE burn logs. With the calculation of the index for the whole basin it was possible to perceive regions with higher occurrence of the moderate and high severity classes in the northwest and northeast portions of the basin. Keywords: burned, sevirity of fire, remote sensing, burn rate. © 2020, Universidade Federal de Pernambuco. All rights reserved.",Scopus,2-s2.0-85100152822
English,Article,2017,"Yokota R.T.C., Van Oyen H., Looman C.W.N., Nusselder W.J., Otava M., Kifle Y.W., Molenberghs G.",Multinomial additive hazard model to assess the disability burden using cross-sectional data,Biometrical Journal,59,5,,901,917,,6,10.1002/bimj.201600157,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016022903&doi=10.1002%2fbimj.201600157&partnerID=40&md5=7cfab0fbb022aa9efa10724d893687c3,"Population aging is accompanied by the burden of chronic diseases and disability. Chronic diseases are among the main causes of disability, which is associated with poor quality of life and high health care costs in the elderly. The identification of which chronic diseases contribute most to the disability prevalence is important to reduce the burden. Although longitudinal studies can be considered the gold standard to assess the causes of disability, they are costly and often with restricted sample size. Thus, the use of cross-sectional data under certain assumptions has become a popular alternative. Among the existing methods based on cross-sectional data, the attribution method, which was originally developed for binary disability outcomes, is an attractive option, as it enables the partition of disability into the additive contribution of chronic diseases, taking into account multimorbidity and that disability can be present even in the absence of disease. In this paper, we propose an extension of the attribution method to multinomial responses, since disability is often measured as a multicategory variable in most surveys, representing different severity levels. The R function constrOptim is used to maximize the multinomial log-likelihood function subject to a linear inequality constraint. Our simulation study indicates overall good performance of the model, without convergence problems. However, the model must be used with care for populations with low marginal disability probabilities and with high sum of conditional probabilities, especially with small sample size. For illustration, we apply the model to the data of the Belgian Health Interview Surveys. © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim",Scopus,2-s2.0-85016022903
English,Article,2017,"DeLuca S.C., Trucks M.R., Wallace D.A., Ramey S.L.",Practice-based evidence from a clinical cohort that received pediatric constraint- induced movement therapy,Journal of pediatric rehabilitation medicine,10,1,,37,46,,6,10.3233/PRM-170409,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037599810&doi=10.3233%2fPRM-170409&partnerID=40&md5=a70945797eacbfabc79bda909a4d8a43,"PURPOSE: Constraint-Induced Movement Therapy (CIMT) is now designated a highly efficacious treatment for children with cerebral palsy, based on rigorous clinical trials. Yet virtually no evidence confirms that these moderate to large size effects can be replicated in clinical practice for a more heterogeneous clinical population. Thus there is a need to collect and report treatment outcome data based on actual clinical practice as a critical next step for implementation.METHODS: This study presents results from a prospective study conducted on a clinical cohort of 88 children, 18 months to 12 years old (M = 55 months, SD = 5 months), who received high-intensity CIMT known as ACQUIREc. The children varied in severity and etiology of their hemiparesis and a subset was diagnosed with asymmetric quadriparesis.RESULTS: Pre- to post-CIMT assessments confirmed highly significant and clinically meaningful changes based on both parental report (Pediatric Motor Activity Log, p< 0.0001) and standardized measures (The Assisting Hand Assessment, p= 0.04).CONCLUSIONS: Clinical practice of high-intensity CIMT (120 hours in 4 weeks) with full-time casting of the less-impaired upper extremity produced benefits of comparable magnitude to those from rigorous randomized controlled trials (RCTs). Therapists were highly trained and actively monitored. Children across a wide range of etiologies and severity levels realized positive outcomes.",Scopus,2-s2.0-85037599810
English,Article,2005,"Meaden P.M., Hartlage S.A., Cook-Karr J.",Timing and severity of symptoms associated with the menstrual cycle in a community-based sample in the Midwestern United States,Psychiatry Research,134,1,,27,36,,40,10.1016/j.psychres.2005.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244403006&doi=10.1016%2fj.psychres.2005.01.003&partnerID=40&md5=621ad2744c4b77c046009c70e4438e4b,"This study describes the experience of menstruation among normal women, establishing a baseline for comparison with women reporting symptoms of a menstrual disorder. A community-based sample of 900 women kept a daily log of 50 physical, social, and psychological symptoms for a period of time that included two menstrual cycles. Twenty-five items were derived from the DSM-IV criteria for premenstrual dysphoric disorder (PMDD) and 13 from the premenstrual syndrome (PMS) literature. An additional 12 items were positively worded versions of some of the PMDD items. Women were told that the study was about women's health, with no specific reference to menstruation. Time sequence charts revealed that all symptoms peaked on the first day of menses, with severity levels more than 2 S.D. above the mean for each individual symptom. Women were more likely to endorse distress when symptoms were positively worded than when they were negatively worded. This study shows the importance of reducing bias in self-reports of menstrual symptoms, and illustrates the lag between hormonal changes in the luteal phase and the peak of symptom severity at onset of menses. Further research is needed to determine the nature and extent to which women with a presumed disorder vary from this baseline pattern. © 2005 Elsevier Ireland Ltd. All rights reserved.",Scopus,2-s2.0-16244403006
English,Article,2017,"De Antonio Liedo D., Muñoz E.F.",Hyper-parameterised dynamic regressions for nowcasting Spanish GDP growth in real time,International Journal of Computational Economics and Econometrics,7,1-2,,5,42,,,10.1504/IJCEE.2017.080667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044314753&doi=10.1504%2fIJCEE.2017.080667&partnerID=40&md5=c3f7cd660b9f67c3d79a6139a3b0589c,"This paper analyses the nowcasting performance of hyperparameterised dynamic regression models with a large number of variables in log levels, and compares it with state-of-the-art methods for nowcasting.We deal with the 'curse of dimensionality' by exploiting prior information originating in the Bayesian VAR literature. The real-time forecast simulation conducted over the most severe phase of the Great Recession shows that our method yields reliable GDP predictions almost one and a half months before the official figures are published. The usefulness of our approach is confirmed in a genuine out-of-sample evaluation over the European sovereign debt crisis and subsequent recovery. Copyright © 2017 Inderscience Enterprises Ltd.",Scopus,2-s2.0-85044314753
Chinese,Article,2014,"Gong Y.-Y., Liu Q.-R., Shao X.-Y., Zhu S.-P., Xing C.-Q., Peng Z.-B., He Y.-L.",A regular expression matching algorithm based on multi-dimensional cube,Tien Tzu Hsueh Pao/Acta Electronica Sinica,42,9,,1818,1822,,1,10.3969/j.issn.0372-2112.2014.09.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919819924&doi=10.3969%2fj.issn.0372-2112.2014.09.024&partnerID=40&md5=8107e8932bf4dae9685cea513e3a67e7,"A deterministic finite automaton(DFA)structure based on multi-dimensional cube is proposed aiming at the state explosion problem generated by the interaction among regular expression rules containing "". *"" under certain conditions. It divides and compresses redundant states by dimension. Then the algorithm M-D-Cube-DFA is proposed. It achieves equivalent state transition by constructing dynamic intersections. Theory and simulation results show that, compared with the conventional DFA algorithm, M-D-Cube-DFA achieves a logarithm-level compression of the number of states and the storage space without changing the time complexity. ©, 2014, Chinese Institute of Electronics. All right reserved.",Scopus,2-s2.0-84919819924
English,Conference Paper,2013,Wang K.P.,A negative selection algorithm base on the self R-tree,Applied Mechanics and Materials,411-414,,,2007,2012,,,10.4028/www.scientific.net/AMM.411-414.2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886299622&doi=10.4028%2fwww.scientific.net%2fAMM.411-414.2007&partnerID=40&md5=16fe2968dc4a604fe64c25c94a0a4794,"In this article, we present a new negative selection algorithm which the self data is organized as a R-Tree structure. And the negative selection process could be transformed into the data query process in the self R-Tree, if a new detector is indexed in any leaf node it will be dropped. As the time complexity of data query process in the tree is in the log level, the negative selection process of our algorithm is superior to the linearly comparation procedure in the traditional negative selection algorithms. © (2013) Trans Tech Publications, Switzerland.",Scopus,2-s2.0-84886299622
English,Conference Paper,2011,"Joshila Grace L.K., Maheswari V., Nagamalai D.",Web log data analysis and mining,Communications in Computer and Information Science,133 CCIS,PART 3,,459,469,,20,10.1007/978-3-642-17881-8_44,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960311961&doi=10.1007%2f978-3-642-17881-8_44&partnerID=40&md5=e5c3a3c0ed3e91017a7755b2270ce7c3,"Log files contain information about User Name, IP Address, Time Stamp, Access Request, number of Bytes Transferred, Result Status, URL that Referred and User Agent. The log files are maintained by the web servers. By analysing these log files gives a neat idea about the user. This paper gives a detailed discussion about these log files, their formats, their creation, access procedures, their uses, various algorithms used and the additional parameters that can be used in the log files which in turn gives way to an effective mining. It also provides the idea of creating an extended log file. © Springer-Verlag Berlin Heidelberg 2011.",Scopus,2-s2.0-79960311961
English,Review,2003,"Forte S., Ridolfi G.",Renormalization group approach to soft gluon resummation,Nuclear Physics B,650,1-2,,229,270,,61,10.1016/S0550-3213(02)01034-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037415304&doi=10.1016%2fS0550-3213%2802%2901034-9&partnerID=40&md5=4f692d7b1b0025914ca9488cc167b68c,"We present a simple proof of the all-order exponentiation of soft logarithmic corrections to hard processes in perturbative QCD. Our argument is based on proving that all large logs in the soft limit can be expressed in terms of a single dimensionful variable, and then using the renormalization group to resum them. Beyond the next-to-leading log level, our result is somewhat less predictive than previous all-order resummation formulae, but it does not rely on non-standard factorization, and it is thus possibly more general. We use our result to settle issues of convergence of the resummed series, we discuss scheme dependence at the resummed level, and we provide explicit resummed expressions in various factorization schemes. © 2002 Elsevier Science B.V. All rights reserved.",Scopus,2-s2.0-0037415304
Chinese,Article,2003,"Fan J.-X., He L.-Q.",BC interconnection networks and their properties,Jisuanji Xuebao/Chinese Journal of Computers,26,1,,84,90,,74,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037828832&partnerID=40&md5=4ca9603a09d8d8bc288e2b2018c950d8,"This paper proposes a family of interconnection networks called bijection-connected (BC) graphs that properly contain the crossed cubes and the Mobius cubes and possess the same logarithm-level diameters and node degrees, the highest connectivity (fault tolerance), and diagnosability as the hypercubes. Thus, this merges the study of some properties of the hypercube and a great many interconnection networks similar to it in structure. In addition, this paper proves that the BC interconnection network family contains a kind of Hamilton-connected graphs and gives a guess on the diameters of the graphs in it.",Scopus,2-s2.0-0037828832
English,Article,2021,"AlRukaibi F., AlKheder S., Sayed T., Alburait A.",Injury severity influence factors and collision prediction - A case study on Kuwait highways,Journal of Transport and Health,20,,101025,,,,,10.1016/j.jth.2021.101025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100960935&doi=10.1016%2fj.jth.2021.101025&partnerID=40&md5=ab847e933f85cc46554ca157a832663d,"Traffic collisions are considered a serious problem worldwide that cause severe losses. Gulf Cooperation Council (GCC) Countries have high rates of collisions, which requires urgent proactive strategies and attention to solve such problem. This study aimed to enhance traffic safety conditions and reduce collisions' severity levels at several locations in Kuwait City, Kuwait. Consequently, the study importance raised in four folds; reducing the total number of collisions on Kuwait highways, predicting the future numbers of collisions, identifying and managing risk factors contributing to collision's severity, and developing new strategies to enhance traffic safety condition in Kuwait. Three-year crash dataset from 2016 to 2018 including 4028 road collisions in Kuwait City were used to analyse the driver injury severity influence factors and to predict the future collision counts. Statistical indices were used to evaluate the mixed logit model performance, which are the MCFadden Pseudo R-Squared statistics and the two-log likelihood. Eight covariates were tested for significance in the mixed logit model estimation. Results showed that female drivers, driving during night-time, driving outside the city, and not using seatbelt produced the highest possibility of having incapacitating injuries and fatalities by 46.6% and 31.6%, respectively. Contrastingly, male drivers, driving during daytime, driving inside the city, and using seatbelt had resulted in the lowest probability of getting incapacitating injuries and fatalities by 4.7% and 0.2%, respectively. Furthermore, Bayesian hierarchical model was used to investigate the future road collision counts and to identify the collision blackspots (CBS) on Kuwait City highways. Six covariates were considered in the model. Log-linear regression model (CPM) was used to estimate the vector of means μj(t). By applying regression to mean (RTM) and predicting the trend in the mean collision rate λj (t), results showed that the average model accuracy for year 2018 was 45.76% (for a 95% confidence interval estimation using the collision dataset of previous years). It was also found that the highest number of collisions had occurred on the second ring road. Basing the results on year 2018 helped in predicting more accurate future collision counts, and in justifying collision rates. Additionally, further covariates can be added to the models for any additional recent crash data to increase the model accuracy. This work is anticipated to help Kuwaiti decision makers in designing accurate countermeasures to improve traffic safety condition. © 2021 Elsevier Ltd",Scopus,2-s2.0-85100960935
English,Conference Paper,2013,"Krishnamurthy V., Babu C., Krishnan. P.M., Aravindan. C., Balamurugan. S.",ReCon - Aspect oriented remotely reconfigurable error logging framework for web applications,"2013 International Conference on Information Communication and Embedded Systems, ICICES 2013",,,6508347,410,415,,,10.1109/ICICES.2013.6508347,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879853910&doi=10.1109%2fICICES.2013.6508347&partnerID=40&md5=704400fc5db17374ba9cfbe7178a5b5d,"Web applications need an error logging framework as they can adequately record and store the errors that are encountered either in the web applications or in the usage of them. In the existing MVC(Model-View-Controller) based frameworks, the log settings such as log location, mechanism and logging level are specified for all the modules in a single file which cannot be changed dynamically. Further, for individual modules, the log settings have to be hard coded into application code, confusing the role of the application developers and log administrators. A remotely configurable error logging framework has been proposed in this paper. This paper proposes to use the aspect oriented programming paradigm to weave the code for error logging into the legacy web application. The proposed configurable error logging framework has been tested on an e-Shopping web application built based on the 'Ruby on Rails' framework. © 2013 IEEE.",Scopus,2-s2.0-84879853910
English,Article,1997,"Suarez A.V., Pfennig K.S., Robinson S.K.",Nesting success of a disturbance-dependent songbird on different kinds of edges,Conservation Biology,11,4,,928,935,,102,10.1046/j.1523-1739.1997.96063.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030877629&doi=10.1046%2fj.1523-1739.1997.96063.x&partnerID=40&md5=d1cb60a76ecf092ee32d8816ab90da74,"We compared the nesting success of a disturbance-dependent species, the Indigo Bunting (Passerina cyanea), on different kinds of habitat edges in five sites (225 total nests) in southern Illinois from 1989 to 1993). Nest predation rates along agricultural and abrupt, permanent edges (e.g., wildlife openings, campgrounds) were nearly twice as high as rates along more gradual edges where plant succession was allowed to occur (e.g., treefalls, streamsides, gaps created by selective logging). Levels of brood parasitism by Brownheaded Cowbirds (Molothrus ater) varied significantly among sites and years, but not among edge types. Clutch sizes, however, were significantly smaller at agricultural edges where nest predation rates were also high, which suggests either decreased food availability or a population dominated by younger and/or lower-quality (poor condition) birds. The results of this study illustrate the need to reevaluate management practices (e.g., wildlife openings) that are designed to promote populations of disturbance-dependent wildlife.",Scopus,2-s2.0-0030877629
English,Article,2020,"Duman Y., Kuzucu C., Ersoy Y., Otlu B.",The effect of sodium dichloroisocyanurate dihydrate to prevent the environmental transmission of multidrug-resistant acinetobacter Baumanniiin hospital settings,Fresenius Environmental Bulletin,29,9,,7191,7197,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099438456&partnerID=40&md5=a4d41236681408092592d7edab4dfd21,"Nosocomial infections are a substantial concern as the major cause of morbidity and mortality of hospitalized patients' in the world. Disinfection of inanimate environment, equipment and hospital setting is important to prevent nosocomial infections. Sodium dichloroisocyanurate dihydrate (NaDCC) can be used for disinfection of environment and medical devices. The aims of this study were to determine the efficacy of NaDCC at various concentrations and times against multi-drug resistant Acinetobacter strains. In the first phase of the study, the bactericidal activity of NaDCC to A. baumannii was investigated by quantitative suspension test. In the second phase, the surface activity of NaDCC was tested by surface disinfection application test. In the third phase, before the cleaning of randomly selected patient's room A. baumannii contamination on the inanimate environment objects and equipment was investigated. After the cleaning of the room the effect of NaDCC was tested. As a result of the quantitative suspension test; NaDCC was inhibited the all A. baumannii and ATCC strains. In the surface disinfection application test, it was determined that at the concentration of 1000 ppm and 500 ppm, the activity of NaDCC; at 5th, 30th and 60th minutes was effective to microorganisms at 5 log level, respectively. But at 100 ppm concentration it was effective to at 5th minutes three, at 30th and 60th minutes seven A. baumannii strains at 5 log levels, while it was effective at log 1 level to other A. baumannii strains and S. aureus, E. coli and P. aeruginosa ATCC. As a result of investigation the A. baumannii contamination in patient's room; before the cleaning, we determined A. baumannii contamination on the inanimate objects of room (such as bed surface, bed edges, control device, nightstand, chair) and on equipment (such as stethoscope, steam appliance, blood pressure device, aspirator heads, ventilator surfaces). After the cleaning it was determined that at 1000 ppm concentration at 5th, 30th and 60thminutes NaDCC was effective to A. baumannii at 5 log levels. However, at 500 ppm concentration at 5th minute it was effective at log 5 level except control device. At 30th and 60th minutes of 500 ppm concentration of NaDCC was effective at log 5 level to A. baumannii. At 100 ppm concentration at 5th, 30th and 60th minutes it was effective to A. baumannii strains at log 1 level on inanimate objects and equipment. In low concentration, NaDCC efficacy was reduced against A. baumannii. The application concentration and time of the disinfectant to clean up the equipment and the environment is very important for preventing nosocomial infections and the spread of A. baumannii. Thus, it is necessary to check and follow up the staff and to create clean and disinfection training programs for educating staff. © by PSP",Scopus,2-s2.0-85099438456
English,Article,2002,"Dasgupta M., Salam G.P.",Accounting for coherence in interjet et flow: A case study,Journal of High Energy Physics,6,3,,271,285,,81,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044496645&partnerID=40&md5=b9e6dc69216f8cd43fe971a801f630e1,"Recently, interest has developed in the distribution of interjet energy flows, with for example the leading-log calculation of the highly non-trivial colour structure of primary emissions in 4-jet systems. Here we point out however that at leading-log level it is insufficient to consider only multiple primary emission from the underlying hard antenna additionally, one must take into account the coherent structure of emission from arbitrarily complicated ensembles of large-angle soft gluons. Similar considerations apply to certain definitions of rapidity gaps based on energy flow. We examine this new class of terms in the simpler context of 2-jet events, and discover features that point at novel aspects of the QCD dynamics. © SISSA/ISAS 2002.",Scopus,2-s2.0-23044496645
English,Article,2000,Kavanagh R.P.,"Effects of variable-intensity logging and the influence of habitat variables on the distribution of the Greater Glider Petauroides volans in montane forest, southeastern New South Wales",Pacific Conservation Biology,6,1,,18,30,,16,10.1071/pc000018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034532755&doi=10.1071%2fpc000018&partnerID=40&md5=a7525ab25adc8e18eecddb75a5ad5d1a,"Populations of arboreal marsupials were assessed in forests before and after logging at different levels of logging intensity to determine the sensitivity of these species to habitat disturbance. The logging treatments imposed were unlogged controls and two intensities of integrated logging for sawlogs and woodchips. The mean basal area of trees retained in logged blocks ranged from 83% to 35% of pre-logging levels. The objective of the study was to develop better methods for managing arboreal marsupials within the wood production forests of southeastern New South Wales. Seven species of arboreal marsupials were recorded during the study, but the Greater Glider Petauroides volans was the only species recorded in sufficient numbers for analysis (86% of all records). No significant differences were observed between the treatments in counts of the Greater Glider before and after logging. However, given the observed trend and an a priori expectation of a decline in numbers of this species following intensive logging, a one-tailed statistical test was applied which resulted in a significant difference at P = 0.08 for the contrast between the unlogged controls and the most intensively logged treatment. The existence of a threshold in logging intensity within the range of 21% to 39% retention of tree basal area, below which numbers of the Greater Glider suffer a marked decline, was inferred on the basis of comparisons with the results of other studies. Factors other than logging were important in determining the distribution of the Greater Glider. Elevation, in particular, was a significant environmental variable, with Greater Gliders more likely to occur in forests above 845 m a.s.l. The presence and absence of particular tree species also influenced the distribution of the Greater Glider. Forests containing Manna Gum E. viminalis and Mountain Gum E. dalrympleana were highly preferred compared to forests with a high proportion of E. obliqua. The presence of E. cypellocarpa appeared to improve the quality of habitat for the Greater Glider in forests dominated by E. obliqua. This study has shown that Greater Glider populations can be maintained at or near pre-logging levels when at least 40% of the original tree basal area is retained thoughout logged areas and when the usual practice of retaining unlogged forest in riparian strips is applied.",Scopus,2-s2.0-0034532755
English,Article,2021,Iscan A.G.,Water saturation calculation using fractional flow and production logging data in a Caspian region sandstone petroleum reservoir,Journal of Petroleum Science and Engineering,200,,108355,,,,,10.1016/j.petrol.2021.108355,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099552924&doi=10.1016%2fj.petrol.2021.108355&partnerID=40&md5=4dbda1d8fcb20c6574193086ce286a8b,"Water saturation in porous media plays a significant role in static, dynamic reservoir modeling and petroleum reserves calculations. In this paper, a Pliocene conglomerate type of sandstone Caspian reservoir was studied. The objective of this paper is to apply water saturation calculation method using fractional flow and PLT (Production Logging Tool) and to compare the results with other methods. In this study, 6 petrophysical rock typing were defined by two methods at log level using the porosity, water saturation logs and RQI(Reservoir Quality Index). Core porosity and permeability correlations were calculated for each rock type where k-phi transforms were generated, respectively. Relative Permeability data of the reservoir was classified based on the rock types. Oil-Water relative permeability ratios versus water saturation data were correlated exponentially. A solution for water saturation was derived at log level using fractional flow, relative permeability, core porosity, permeability and production data. Leverett-J function and Archie saturations were calculated to compare to the proposed fractional flow approach. The comparison of the saturation logs with J-function and Archie saturation showed good match. This approach can be considered as a tool to validate the saturation-J function. This methodology helps to estimate pre-production water cut performance of the wells in reservoir models considering that not all the wells will start flowing at 100% dry oil. The employed methods regarding the petrophysical rock typing and water saturation calculations will facilitate reservoir characterization for future field development activities. © 2021 Elsevier B.V.",Scopus,2-s2.0-85099552924
English,Article,2015,"Táborská M., Prívetivý T., Vrška T., Ódor P.",Bryophytes associated with two tree species and different stages of decay in a natural fir-beech mixed forest in the Czech Republic,Preslia,87,4,,387,401,,18,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958164964&partnerID=40&md5=f448ff09324731a90f5d5f6fd1b60643,"Species richness and composition of bryophyte communities on two species of trees at different stages of decay were studied on 57 logs of Abies alba and Fagus sylvatica in the natural montane beech-fir forest reserve Salajka (Czech Republic). There were 68 species of bryophytes. At the stand level, the species richness recorded on Fagus was higher than that on Abies. This is due to a higher diversity of epiphytic species on Fagus in the early stages of decay, when the conditions of logs aremore heterogeneous and there aremoremicrohabitats than on Abies. The log-level species richness was higher on Abies in later stages of decay because it is more favourable for epixylic species occurring on very acid and constantly moist substrates. Both at the stand- and log level, the highest species richness was recorded at intermediate stages of decay, which constitute a transitional phase in the decay succession in which species associated with all stages of decay overlap and therefore the overall number is relatively high. Species composition differed significantly on the two trees, with two clearly defined groups of indicator species. In contrast, the different stages of decay were not so sharply distinguished in terms of indicator species. We also found significant differences in pH both between the two trees and stages of decay, which may also affect compositional patterns on the logs studied. In conclusion, the species richness and composition of bryophytes on dead wood is associated with both stage of decay and species of tree and their various combinations, which further increase the total diversity. Therefore, successful bryophyte conservation should be focused on the preservation of mixed stands and the continuity of dead wood in the montane beech-fir zone of Europe.",Scopus,2-s2.0-84958164964
English,Article,1999,Spångberg K.,Classification of Picea abies pulpwood according to wood and stand properties,Scandinavian Journal of Forest Research,14,3,,276,281,,4,10.1080/02827589950152809,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344849467&doi=10.1080%2f02827589950152809&partnerID=40&md5=63bfe361aa24d71f1b87145f579aeca0,"Different classification systems for Norway spruce (Picea abies (L.) Karst.) pulpwood were compared. The classification systems were applied on truckloads or single logs in southern Sweden. Truckload classification according to mean annual growth ring width gave better separation of the wood properties basic density, juvenile wood and dry matter content, than classification according to harvest type (first thinning, later thinning or final felling). The assortments did not have significantly different wood brightness. Sorting at log level according to diameter, mean annual growth ring width or number of annual growth rings, which could be done at harvesting, did not drastically improve differentiation of the mean values of the wood properties or reduce variance compared to truckload classification. The variation in wood properties within assortments remained large owing to the large variation in wood properties between and within logs. Substantial reduction in dry matter variation could be achieved by truckload classification during the summer.Different classification systems for Norway spruce (Picea abies (L.) Karst.) pulpwood were compared. The classification systems were applied on truckloads or single logs in southern Sweden. Truckload classification according to mean annual growth ring width gave better separation of the wood properties basic density, juvenile wood and dry matter content, than classification according to harvest type (first thinning, later thinning or final felling). The assortments did not have significantly different wood brightness. Sorting at log level according to diameter, mean annual growth ring width or number of annual growth rings, which could be done at harvesting, did not drastically improve differentiation of the mean values of the wood properties or reduce variance compared to truckload classification. The variation in wood properties within assortments remained large owing to the large variation in wood properties between and within logs. Substantial reduction in dry matter variation could be achieved by truckload classification during the summer.",Scopus,2-s2.0-0344849467
English,Conference Paper,2018,"Tibeme B., Shahriar H., Zhang C.",Process Mining Algorithms for Clinical Workflow Analysis,Conference Proceedings - IEEE SOUTHEASTCON,2018-April,,8479176,,,,2,10.1109/SECON.2018.8479176,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056092271&doi=10.1109%2fSECON.2018.8479176&partnerID=40&md5=c00a9c2d64a04f739bd37c8c82566a3f,"Process Mining focuses on the extraction of knowledge from data generated and stored by information systems. Log level data contains the signatures of executed processes. Many case studies have used process mining to discover the processes for compliance or identifying anomalies in business workflow. In this paper, we apply workflow analysis for a possible clinical setting by leveraging an open source data mining tool named ProM. We apply four available mining algorithms (Alpha, Heuristic, Inductive and Fuzzy miners) and evaluate the outputs that describe a workflow. The work provides a genesis for clinical practitioners on the advantages and disadvantages of applying various algorithms towards a dataset based on event logs. © 2018 IEEE.",Scopus,2-s2.0-85056092271
English,Conference Paper,2010,Muskee W.,Where is the user? Filtering bots from the edurep query logs,CEUR Workshop Proceedings,681,,,23,29,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890069101&partnerID=40&md5=b171cf811739dc1550f8c471a7b46f4b,"Edurep indexes learning object metadata from several repositories, offering a webservice interface on which portals can build their own search implementation. At Edurep query log level, no obvious distinction can be made between human users and webcrawlers visiting these portal sites. This makes it impossible to gather any meaningful data on user search behaviour. Four query types, distinguished from the six largest portals' websites were related to one month of query logs. For two query types a distinction between human and automatic generated traffic could be found. However, these results can only be used to advise connected portals on their interface implementations. More research is needed to actually perform any reliable filtering.",Scopus,2-s2.0-84890069101
English,Article,2009,"Godoy-Matos A.F., Moreira R.O., MacDowell R., Bendet I., Mory P.B., Moises R.S.",Serum retinol binding protein 4 in patients with familial partial lipodystrophy,Clinical Biochemistry,42,10-11,,1183,1186,,4,10.1016/j.clinbiochem.2009.03.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349120639&doi=10.1016%2fj.clinbiochem.2009.03.006&partnerID=40&md5=c114dbb9c22e846c397576c53783c95b,"Objective: To determine Retinol Binding Protein 4 (RBP4) levels in patients with Familial Partial Lipodystrophy (FPLD). Methods: Ten patients with FPLD and a control group (9 patients) were selected to participate in the study. Results: RBP4-log levels were lower in patients with FPLD in comparison to control group (1.52 ± 0.32 vs 1.84 ± 0.25, p = 0.029). A statistical trend was observed between Waist-to-Hip Ratio and RBP4-log (r = - 0.44, p = 0.054). Conclusion: RBP4 levels are decreased in FPLD. © 2009 The Canadian Society of Clinical Chemists.",Scopus,2-s2.0-67349120639
English,Article,2009,Bhattacharyya S.,"Unbundled institutions, human capital and growth",Journal of Comparative Economics,37,1,,106,120,,39,10.1016/j.jce.2008.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63249132599&doi=10.1016%2fj.jce.2008.08.001&partnerID=40&md5=ed6f378b073fe712337015f127c51a30,"We investigate the partial effects of institutions and human capital on growth. We find that cross-country regressions of the log-level of per capita GDP on instrumented measures of institutions and schooling are uninformative about the relative importance of institutions and human capital in the long run because of multicollinearity problems. Using dynamic panel regressions we show that both institutions and human capital have significant effects on growth. Using Rodrik's [Rodrik, D., 2005. Growth strategies. Handbook of Economic Growth 1 (1), 967-1014] four-way partition of institutions, we also unbundle institutions. We show that strong market creating institutions and market stabilising institutions are growth enhancing. Market regulating institutions matter up to a certain extent and market legitimising institutions does not seem to matter. Journal of Comparative Economics 37 (1) (2009) 106-120. © 2008 Association for Comparative Economic Studies.",Scopus,2-s2.0-63249132599
English,Article,2007,[No author name available],Log level raised,Converting Today,21,1,,28,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857633503&partnerID=40&md5=81aaf2ac12ea23192a7b8ebf224817ae,"Italy's FEBA Converting Machinery produces the world's first fully automatic non intervention slitting operation on a lathe slitter with the help of new Level 5 specifications. The machine users are focusing toward Lean Manufacturing Techniques that allow one machine to be extremely flexible, handling al the blade size, log sizes, and slit width changes quickly and efficiently. The company's F400, F500, and F600 CNC controlled lathe slitter can all be equipped with Level 5 specification. The new technique removes the requirement for the operator to manually set the movements and distances which can quadruple cycle time. The new specification is automatically able to position the blade in relation to the external diameter of the log being slit and automatically self center the blade, irrespective of its diameter. The Level 5 machines can incorporate fully programmable blade axis angles and automatic roll edge acquisition.",Scopus,2-s2.0-84857633503
English,Article,2003,Anselmi D.,"""Integrability"" of RG flows and duality in three dimensions in the 1/N expansion",Nuclear Physics B,658,3,,440,460,,4,10.1016/S0550-3213(03)00174-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037454460&doi=10.1016%2fS0550-3213%2803%2900174-3&partnerID=40&md5=39b6e00b2fe7dc04c3dbd678cc48ce2b,"I study some classes of RG flows in three dimensions that are classically conformal and have manifest g→1/g dualities. The RG flow interpolates between known (four-fermion, Wilson-Fischer, 36) and new interacting fixed points. These models have two remarkable properties: (i) the RG flow can be integrated for arbitrarily large values of the couplings g at each order of the 1/N expansion; (ii) the duality symmetries are exact at each order of the 1/N expansion. I integrate the RG flow explicitly to the order O (1/N), write correlators at the leading-log level and study the interpolation between the fixed points. I examine how duality is implemented in the regularized theory and verified in the results of this paper. © 2003 Elsevier Science B.V. All rights reserved.",Scopus,2-s2.0-0037454460
English,Article,1997,"Grundberg S., Grönlund A.",Simulated grading of logs with an x–ray log scanner–grading accuracy compared with manual grading,Scandinavian Journal of Forest Research,12,1,,70,76,,37,10.1080/02827589709355386,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842411964&doi=10.1080%2f02827589709355386&partnerID=40&md5=1e69148228d0e42348b037f445e6c7f4,"Today sawmills have started to use automatic methods for log grading. The methods used are either optical or gamma–ray scanners. However, the signals from these scanners are too coarse for accurate log grading and for good control of the sawing process at the single log level. The objective of the present study was to determine the grading accuracy of a log–scanner with two industrial X–ray sources. The grading accuracy was compared with the accuracy of manual grading. The results showed that the manual grading of logs and boards is difficult. The accuracy of manual grading was low and the automatic grading systems were more reliable than manual ones. Possibilities for improving the automatic grading systems are discussed. © 1997 Scandinavian University Press.",Scopus,2-s2.0-1842411964
English,Article,2005,"Smith R.G.B., Nichols J.D., Vanclay J.K.",Dynamics of tree diversity in undisturbed and logged subtropical rainforest in Australia,Biodiversity and Conservation,14,10,,2447,2463,,14,10.1007/s10531-004-0215-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26844482990&doi=10.1007%2fs10531-004-0215-0&partnerID=40&md5=9e87e290a5b184e89ec73ab0cf7c157a,"In subtropical rainforest in eastern Australia, changes in the diversity of trees were compared under natural conditions and eight silvicultural regimes over 35 years. In the treated plots basal area remaining after logging ranged from 12 to 58 m2/ha. In three control plots richness differed little over this period. In the eight treated plots richness per plot generally declined after intervention and then gradually increased to greater than original diversity. After logging there was a reduction in richness per plot and an increase in species richness per stem in all but the lightest selective treatments. The change in species diversity was related to the intensity of the logging, however the time taken for species richness to return to pre-logging levels was similar in all silvicultural treatments and was not effected by the intensity of treatment. These results suggest that light selective logging in these forests mainly affects dominant species. The return to high diversity after only a short time under all silvicultural regimes suggests that sustainability and the manipulation of species composition for desired management outcomes is possible. © Springer 2005.",Scopus,2-s2.0-26844482990
English,Review,2021,"Van Der Vlegel M., Haagsma J.A., Havermans R.J.M., De Munter L., De Jongh M.A.C., Polinder S.",Long-term medical and productivity costs of severe trauma: Results from a prospective cohort study,PLoS ONE,16,6 June,e0252673,,,,,10.1371/journal.pone.0252673,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107354119&doi=10.1371%2fjournal.pone.0252673&partnerID=40&md5=7e1703e47546a462ed794d932e973890,"Background Through improvements in trauma care there has been a decline in injury mortality, as more people survive severe trauma. Patients who survive severe trauma are at risk of long-term disabilities which may place a high economic burden on society. The purpose of this study was to estimate the health care and productivity costs of severe trauma patients up to 24 months after sustaining the injury. Furthermore, we investigated the impact of injury severity level on health care utilization and costs and determined predictors for health care and productivity costs. Methods This prospective cohort study included adult trauma patients with severe injury (ISS≥16). Data on in-hospital health care use, 24-month post-hospital health care use and productivity loss were obtained from hospital registry data and collected with the iMTA Medical Consumption and Productivity Cost Questionnaire. The questionnaires were completed 1 week and 1, 3, 6, 12 and 24 months after injury. Log-linked gamma generalized linear models were used to investigate the drivers of health care and productivity costs. Results In total, 174 severe injury patients were included in this study. The median age of participants was 55 years and the majority were male (66.1%). The mean hospital stay was 14.2 (SD = 13.5) days. Patients with paid employment returned to work 21 weeks after injury. In total, the mean costs per patient were €24, 760 with in-hospital costs of €11, 930, post-hospital costs of €7, 770 and productivity costs of €8, 800. Having an ISS ≥25 and lower health status were predictors of high health care costs and male sex was associated with higher productivity costs. Conclusions Both health care and productivity costs increased with injury severity, although large differences were observed between patients. It is important for decision-makers to consider not only in-hospital health care utilization but also the long-term consequences and associated costs related to rehabilitation and productivity loss. © 2021 van der Vlegel et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",Scopus,2-s2.0-85107354119
English,Article,2020,"Jiménez-Murcia S., Granero R., Giménez M., del Pino-Gutiérrez A., Mestre-Bach G., Mena-Moreno T., Moragas L., Baño M., Sánchez-González J., de Gracia M., Baenas-Soto I., Contaldo S.F., Valenciano-Mendoza E., Mora-Maltas B., López-González H., Menchón J.M., Fernández-Aranda F.",Moderator effect of sex in the clustering of treatment-seeking patients with gambling problems [Moderierender Effekt des Geschlechts bei der Clusterbildung von behandlungssuchenden Patienten mit Glücksspielproblemen],Neuropsychiatrie,34,3,,116,129,,2,10.1007/s40211-020-00341-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081580425&doi=10.1007%2fs40211-020-00341-1&partnerID=40&md5=2c3c7887569d8abda590784fa817b17c,"Background: There are no studies based on a person-centered approach addressing sex-related differences in the characteristics of treatment-seeking patients with gambling disorder (GD). The main objective of the current study is to identify empirical clusters of GD based on several measures of the severity of gambling behavior, and considering the potential role of patient sex as a moderator. Methods: An agglomerative hierarchical clustering method was applied to an adult sample of 512 treatment-seeking patients (473 men and 39 women) by using a combination of the Schwarz Bayesian Information Criterion and log-likelihood function. Results: Three clusters were identified in the subsample of men: cluster M1 (low-mild gambling severity level, 9.1%), cluster M2 (moderate level, 60.9%), and cluster M3 (severe level, 30.0%). In the women subsample, two clusters emerged: cluster W1 (mild-moderate level, 64.1%), and cluster W2 (severe level, 35.9%). The most severe GD profiles were related to being single, multiple gambling preference for nonstrategic plus strategic games, early onset of the gambling activity, higher impulsivity levels, higher dysfunctional scores in the personality traits of harm avoidance, and self-directedness, and higher number of lifespan stressful life events (SLE). Differences between the empirical men and women clusters were found in different sociodemographic and clinical measurements. Conclusions: Men and women have distinct profiles regarding gambling severity that can be identified by a clustering approach. The sociodemographic and clinical characterization of each cluster by sex may help to establish specific preventive and treatment interventions. © 2020, Springer-Verlag GmbH Austria, ein Teil von Springer Nature.",Scopus,2-s2.0-85081580425
English,Article,2012,"Wu C.X., Tan W.S., Toh M.P.H.S., Heng B.H.",Stratifying healthcare costs using the Diabetes Complication Severity Index,Journal of Diabetes and its Complications,26,2,,107,112,,19,10.1016/j.jdiacomp.2012.02.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860673448&doi=10.1016%2fj.jdiacomp.2012.02.004&partnerID=40&md5=d70d13949557e4e9876bd7506dd5a8c0,"Objective: We aim to determine whether healthcare costs for patients diagnosed with Type 2 Diabetes Mellitus (T2DM) are associated with the severity of diabetes complications as measured by the Diabetes Complication Severity Index (DCSI). Methods: Retrospective cohort analysis was performed on a 2007 primary care cohort of T2DM patients. The DCSI is a 13-point scale, which comprises 7 categories of complications and their severity levels. Healthcare cost data from 2008 and 2009 were used as primary outcome. Inpatient and outpatient costs incurred for services consumed by patients within the provider network were included. Generalized linear model with log-link and gamma distribution was used to predict healthcare costs. Results: Of the 59,767 T2DM patients, 2977 (5.0%) deaths occurred and 1336 (2.2%) were lost to follow up. Healthcare cost was strongly associated with increase in DCSI score. Compared to patients without complications, those with more complications (higher DCSI score) had an increased risk of higher healthcare costs. Risk ratio (RR) increased from 1.25 (95%CI: 1.19-1.32) for DCSI = 1 to 1.61 (1.51-1.72) for DCSI = 2; 2.10 (1.91-2.31) for DCSI = 3; 2.52 (2.21-2.87) for DCSI = 4 and 3.62 (3.09-4.25) for DCSI ≥ 5. As a continuous score, a one-point increase in the DCSI was associated with a cost increase of 27% (95%CI: 1.25-1.29). Conclusion: The DCSI score is a useful tool for predicting direct healthcare costs. The DCSI can be used to triage high-risk patients for more focused secondary prevention interventions at primary care level, in a bid to lower overall healthcare costs. © 2012 Elsevier Inc. All rights reserved.",Scopus,2-s2.0-84860673448
English,Article,2016,Moosa I.A.,Identification of a basket peg: the model specification controversy,Applied Economics Letters,23,11,,795,800,,1,10.1080/13504851.2015.1109033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946865999&doi=10.1080%2f13504851.2015.1109033&partnerID=40&md5=0fbd2a80be162c5d4c7290c2a688772b,"A controversy has been triggered by the Chinese exchange rate regime shift from a single currency peg to an alleged basket peg. The controversy is about the specification of the model used to represent the basket as three models have been used: levels, log levels and first log differences. It is suggested that one way to confirm the validity or otherwise of a model is to use data on the special drawing rights exchange rate since the currency weights are known. The results show that the estimated weights are almost identical no matter which model and which numeraire is used. However, nonnested model selection criteria show that the best model is that written in levels, simply because this is what is used in practice by central banks adopting basket pegs. © 2015 Taylor & Francis.",Scopus,2-s2.0-84946865999
English,Article,2006,"Corradi V., Swanson N.R.","The effect of data transformation on common cycle, cointegration, and unit root tests: Monte Carlo results and a simple test",Journal of Econometrics,132,1,,195,229,,15,10.1016/j.jeconom.2005.01.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646135797&doi=10.1016%2fj.jeconom.2005.01.028&partnerID=40&md5=217e9a28cb7a5f165b6df2e611e0d492,"Cointegration, common cycle, and related tests statistics are often constructed using logged data, even without clear reason why logs should be used rather than levels. Unfortunately, it is also the case that standard data transformation tests, such as those based on Box-Cox transformations, cannot be shown to be consistent unless assumptions concerning whether variables I ( 0 ) or I ( 1 ) are made. In this paper, we propose a simple randomized procedure for choosing between levels and log-levels specifications in the (possible) presence of deterministic and/or stochastic trends, and discuss the impact of incorrect data transformation on common cycle, cointegration and unit root tests. © 2005 Elsevier B.V. All rights reserved.",Scopus,2-s2.0-33646135797
English,Article,2003,"Dollar D., Kraay A.","Institutions, trade, and growth",Journal of Monetary Economics,50,1,,133,162,,509,10.1016/S0304-3932(02)00206-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037273711&doi=10.1016%2fS0304-3932%2802%2900206-4&partnerID=40&md5=70d39926e621b85933b87140aeada99f,"Countries withbetter institutions and countries that trade more grow faster. Countries with better institutions also tend to trade more. These three stylized facts have been documented extensively. Here we investigate the partial effects of institutions and trade on growth. We argue that cross-country regressions of the log-level of per capita GDP on instrumented measures of trade and institutional quality are uninformative about the relative importance of trade and institutions in the long run, because of the very high correlation between the latter two variables. In contrast, regressions of changes in decadal growth rates on instrumented changes in trade and changes in institutional quality provide evidence of a significant effect of trade on growth, with a smaller role for improvements in institutions. These results are suggestive of an important joint role for bothtrade and institutions in the very long run, but a relatively larger role for trade over shorter horizons. © 2002 Elsevier Science B.V. All rights reserved.",Scopus,2-s2.0-0037273711
English,Article,2001,"Espinosa J.R., Navarro I.",Radiative corrections to the Higgs boson mass for a hierarchical stop spectrum,Nuclear Physics B,615,1-3,,82,116,,80,10.1016/S0550-3213(01)00429-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002959008&doi=10.1016%2fS0550-3213%2801%2900429-1&partnerID=40&md5=54d62407e2341513d1db4c75ec35239f,"An effective theory approach is used to compute analytically the radiative corrections to the mass of the light Higgs boson of the Minimal Supersymmetric Standard Model when there is a hierarchy in the masses of the stops (M~t1≫M~t2≫Mtop, with moderate stop mixing). The calculation includes up to two-loop leading and next-to-leading logarithmic corrections dependent on the QCD and top-Yukawa couplings, and is further completed by two-loop non-logarithmic corrections extracted from the effective potential. The results presented disagree already at two-loop-leading-log level with widely used findings of previous literature. Our formulas can be used as the starting point for a full numerical resummation of logarithmic corrections to all loops, which would be mandatory if the hierarchy between the stop masses is large. © 2001 Elsevier Science B.V.",Scopus,2-s2.0-0002959008
English,Article,1999,"Choi J.J., Hauser S., Kopecky K.J.",Does the stock market predict real activity? Time series evidence from the G-7 countries,Journal of Banking and Finance,23,12,,1771,1792,,75,10.1016/S0378-4266(99)00020-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033474607&doi=10.1016%2fS0378-4266%2899%2900020-5&partnerID=40&md5=6ceb7692a2b7fd6bdce33293dce38fe0,"This paper extends one aspect of the US stock market study of Fama (1990) and Schwert (1990). We examine the relationship between industrial production (IP) growth rates and lagged real stock returns for the G-7 countries using both in-sample cointegration and error-correction models and the out-of-sample forecast-evaluation procedure of Ashley et al. (1980). The cointegration tests show a long-run equilibrium relationship between the log levels of IP and real stock prices, while the error-correction models indicate a correlation between IP growth and lagged real stock returns for all countries except Italy. The out-of-sample tests show that in several sub-periods the US, UK, Japanese, and Canadian stock markets enhance predictions of future IP. © 1999 Elsevier Science B.V. All rights reserved.",Scopus,2-s2.0-0033474607
English,Article,1996,Raj B.,The trend behaviour of public capital expenditure on highways in the United States. Did the growth rate of public expenditure experience a decline?,Logistics and Transportation Review,32,1,,77,92,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029962574&partnerID=40&md5=8f4400c943906cc62cda830d9b0bd2b2,"This paper tests the unit root hypothesis in the log levels of the capital outlays on highways in the US during the postwar period using some newly developed time series methods of analysis. The evidence for a structural break in the trend function in the total capital outlays is found, rejecting the unit root hypothesis. The analysis uses annual data from 1956 to 1989. The timing of the break is treated as an unknown parameter. The results of this paper call into question a generally held belief that the total capital expenditure on new highways has experienced a decline in its growth rate. The evidence of trend stationarity of the capital outlays should be of interest since the validity of many econometric tests and procedures critically depends upon this property.",Scopus,2-s2.0-0029962574
English,Article,2018,"Wilkinson C.L., Yeo D.C.J., Tan H.H., Fikri A.H., Ewers R.M.","Land-use change is associated with a significant loss of freshwater fish species and functional richness in Sabah, Malaysia",Biological Conservation,222,,,164,171,,22,10.1016/j.biocon.2018.04.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045538974&doi=10.1016%2fj.biocon.2018.04.004&partnerID=40&md5=e378c5f2704c4cb473ac8f9e751aff38,"Global biodiversity is being lost due to extensive anthropogenic land cover change. In Southeast Asia, biodiversity-rich forests are being extensively logged and converted to oil-palm monocultures. The impacts of this land-use change on freshwater ecosystems, and particularly on freshwater biodiversity, remain largely understudied and poorly understood. We assessed the differences between fish communities in headwater stream catchments across an established land-use gradient in Sabah, Malaysia (protected forest areas, twice-logged forest, salvage-logged forest, oil-palm plantations with riparian reserves, and oil-palm plantations without riparian reserves). Stream fishes were sampled using an electrofisher, a cast net and a tray net in 100 m long transects in 23 streams in 2017. Local species richness and functional richness were both significantly reduced with any land-use change from protected forest areas, but further increases in land-use intensity had no subsequent impacts on fish biomass, functional evenness, and functional divergence. Any form of logging or land-use change had a clear and negative impact on fish communities, but the magnitude of that effect was not influenced by logging severity or time since logging on any fish community metric, suggesting that just two rounds of selective impact (i.e., logging) appeared sufficient to cause negative effects on freshwater ecosystems. It is therefore essential to continue protecting primary forested areas to maintain freshwater diversity, as well as to explore strategies to protect freshwater ecosystems during logging, deforestation, and conversion to plantation monocultures that are expected to continue across Southeast Asia. © 2018 Elsevier Ltd",Scopus,2-s2.0-85045538974
English,Article,2014,"Munhoz C.A., da Silva J.V., Marques M.C.M.",Demography of the endangered tree species Ocotea porosa (Lauraceae) along a gradient of forest disturbance in Southern Brazil,Acta Botanica Brasilica,28,4,,617,623,,,10.1590/0102-33062014abb3516,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928160359&doi=10.1590%2f0102-33062014abb3516&partnerID=40&md5=2dde9a367dd432fa9953d27099025a1b,"Ocotea porosa (Ness) Barroso (Lauraceae), a typical tree of the southern Atlantic Forest in Brazil, was heavily exploited for timber in the last century. With the aim of examining the status of the remaining populations, we surveyed five forest fragments in the state of Paraná, in southern Brazil, and evaluated whether disturbances caused by selective logging and fragmentation were related to population structure of O. porosa. We assessed demographic aspects related to tree density, size hierarchy and individual allometry, correlating those parameters with fragment structure variables (fragment size, isolation and logging level). We found that, although all populations occurred in low densities (60-440 individuals ha-1), the number of adults was significantly lower in the smaller and most disturbed fragments (13 and 35 individuals ha-1, respectively). We did not detect changes in allometric relationships among individuals in the five populations studied. However, we found that populations in more heavily disturbed areas presented lower size hierarchy (i.e., less dominance of larger trees) than did those in undisturbed areas, suggesting that selective logging affects the population structure of O. porosa, possibly affecting the rates of reproduction and fecundity, which may ultimately increase the probability of local extinction. © 2014, Sociedade Botanica do Brasil. All rights reserved.",Scopus,2-s2.0-84928160359
English,Article,2004,"Knop E., Ward P.I., Wich S.A.",A comparison of orang-utan density in a logged and unlogged forest on Sumatra,Biological Conservation,120,2,,183,188,,34,10.1016/j.biocon.2004.02.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3543060111&doi=10.1016%2fj.biocon.2004.02.010&partnerID=40&md5=a97ebd74ca70ec43f3d121d5ce4b738c,"Several studies have shown that there is a strong decline in orang-utan densities shortly after logging. Nevertheless, there is little information on whether orang-utan densities return to their pre-logging values when logged forest is left to recover. This study investigates the orang-utan density in a 22-year-old selectively logged forest and compares it with the orang-utan density in a nearby ecologically similar primary forest. The results show that the orang-utan density did not differ significantly between primary forest and the selectively logged forest. Since we found no difference in fruit availability between the selectively logged and primary forest, we suggest that the selectively logged forest regenerated sufficiently well to sustain pre-logging levels of orang-utans. This study confirms previous studies that suggest fruit availability is the best ecological predictor of orang-utan densities and found a positive overall correlation between orang-utan density and fruit availability. The food attraction hypothesis, which explains local fluctuations in orang-utan density as a result of variation in fruit availability, was not supported. © 2004 Elsevier Ltd. All rights reserved.",Scopus,2-s2.0-3543060111
English,Conference Paper,2020,"Friedel Z.P., Leishman R.C.",Smart Features for Dynamic Vision Sensors,"2020 IEEE/ION Position, Location and Navigation Symposium, PLANS 2020",,,9109905,973,978,,,10.1109/PLANS46316.2020.9109905,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087060867&doi=10.1109%2fPLANS46316.2020.9109905&partnerID=40&md5=42d1b27fd32381122462aa9ef2d5ad9c,"This paper presents a semi-supervised procedure for training a fully convolutional neural network for interest point detection and description. Contrary to previously trained networks utilized for the same purpose, our network was tailored to work with event-based imagery from a downward facing camera aboard a fixed-wing unmanned aerial vehicle. Event-based cameras are a novel type of visual sensor that operate under a unique paradigm, providing asynchronous data on the log-level changes in light intensity for individual pixels. This hardware-level approach to change detection allows these cameras to achieve ultra-wide dynamic range and high temporal resolution. The final system produces state-of-the-art repeatability and homography estimation results on an aerial event-based image dataset when compared to traditional interest point detector and descriptor algorithms. © 2020 IEEE.",Scopus,2-s2.0-85087060867
English,Conference Paper,2015,"Li Y., Luo X., Shao X., Wei D.",MDC-DFA: A multi-dimensional cube deterministic finite automata-based feature matching algorithm,"International Conference on ICT Convergence 2015: Innovations Toward the IoT, 5G, and Smart Media Era, ICTC 2015",,,7354753,1119,1124,,1,10.1109/ICTC.2015.7354753,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964832902&doi=10.1109%2fICTC.2015.7354753&partnerID=40&md5=ae4c9f7ba70b26502fd076426251c2de,"The regular expression matching algorithm (REM) is widely applied in the deep packet inspection(DPI), which is more flexible and efficient compared with conventional exact matching algorithm. The REM based on the deterministic finite automaton (DFA) is applicable in the high speed networks due to its linear matching speed. However, it faces the problem of the state explosion simultaneously. Therefore, we propose a multi-dimensional cube deterministic finite automata algorithm (MDC-DFA) for anomaly feature matching. The algorithm divides and compresses redundant states by each dimension, and achieves equivalent state transition by constructing dynamic nodes. Theory and simulation results show that, compared with conventional deterministic finite automata algorithm, the algorithm achieves a logarithm-level compression to the number of states and the storage space while maintaining the time complexity. © 2015 IEEE.",Scopus,2-s2.0-84964832902
English,Article,2010,"Bhattacharya M., Narayan P.K.",Labour productivity trends in Australian manufacturing: Some time series properties,Applied Economics,42,25,,3221,3230,,3,10.1080/00036840801982692,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957034684&doi=10.1080%2f00036840801982692&partnerID=40&md5=f74c7167ce09559673e643b3a1e095fb,"Labour productivity plays a significant role in economic growth, labour demand and employment situation of a particular economy. In this light, the presence of a structural break in productivity, and its unit root property, has important consequences for the overall economy and in major sectors such as manufacturing. In this article, using some recently developed unit root tests, we examine: (i) the null hypothesis of a unit root in the log-level of labour productivity for 38 manufacturing subdivisions against the alternative of trend stationarity over a three-decade period; and (ii) the presence of a structural break in the series, and whether the break has had a permanent or a transitory effect on manufacturing labour productivity. Our main finding is that shocks to labour productivity have had a transitory effect, implying that policies are likely to have only shortterm effects. © 2010 Taylor & Francis.",Scopus,2-s2.0-77957034684
English,Article,1997,"Han H.-L., Ogaki M.","Consumption, income and cointegration",International Review of Economics and Finance,6,2,,107,117,,12,10.1016/S1059-0560(97)90019-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031498745&doi=10.1016%2fS1059-0560%2897%2990019-8&partnerID=40&md5=5dda04533995541b3d7bf720d5c6f40d,"This paper examines the long-run relationship of consumption and income in the United States. It was shown by King et al. (1991) in a version of the permanent income hypothesis that log levels of consumption and income are cointegrated with a known cointegrating vector (1,-1)′. This implies a restriction that the cointegrating vector, which eliminates the stochastic trends, also eliminates the deterministic trends arising from the drift terms of difference stationary variables. Two different methodologies are used to test this deterministic cointegration restriction in this study. The first methodology is the canonical cointegrating regression proposed by Park (1992). The second methodology is testing stationarity of the difference of log consumption and log income, by assuming the cointegrating vector to be (1,-1)′. Two test statistics for the null hypothesis of stationarity are employed. One is the G(p,q) test proposed by Park and Choi (1988) and the other is proposed by Kwiatkowski et al. (1992). The results of the study indicate that the deterministic cointegration restriction cannot be rejected. JEL: E21, C32 © 1997 JAI Press Inc.",Scopus,2-s2.0-0031498745
English,Article,1994,"Tsuji A., Kaneko Y., Yamaguchi K.",Bactericidal Activities Of Isepamicin And Ofloxacin Against Pseudomonas Aeruginosa Evaluated Using An In Vitro Pharmacokinetic Simulation System,The Japanese Journal of Antibiotics,47,8,,1006,1012,,1,10.11553/antibiotics1968b.47.1006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028168990&doi=10.11553%2fantibiotics1968b.47.1006&partnerID=40&md5=43b9c9f63cd59163cc29ae88338fe5a8,"Bactericidal activities of isepamicin and ofloxacin against Pseudomonas aeruginosa E7 were examined using an in vitro computer programmed pharmacokinetic simulation system. Concentration of the drugs simulated the pharmacokinetic data obtained in healthy human adult subjects after 200 mg or 400 mg I.M. administration of isepamicin (ISP), and 300 mg P.O. administration of ofloxacin (OFLX). P. aeruginosa cell counts were depressed by ISP or OFLX to -4 log-5 log levels of magnitude below the initial cell counts. The time required in recovery of cell counts to the initial level after treatment with ISP (200 mg) was longer than that after OFLX (300 mg) treatment. An excellent synergistic effect was observed between ISP and OFLX. © 1994, Japan Antibiotics Research Association. All rights reserved.",Scopus,2-s2.0-0028168990
English,Article,1991,"Goto H., Goto M., Oka S., Urayama K., Kimura S., Shimada K.",Evaluation of antibacterial activity of sparfloxacin using in-vitro pharmacokinetic system,CHEMOTHERAPY,39,,,54,58,,1,10.11250/chemotherapy1953.39.Supplement4_54,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025898933&doi=10.11250%2fchemotherapy1953.39.Supplement4_54&partnerID=40&md5=bfe013ebd57dee9252489afdf4bf6249,"Sparfloxacin is characterized by its long half-life (T1/2: 16 hours). Based on this feature, its availability as a “once a day” treatment was assessed in an in vitro computer-programmed pharmacokinetic system using a one—compartment open model. Viable cell counts of Haemophilus influenzae as well as Escherichia coli decreased to undetectable levels at 3 hours after the administration of sparfloxacin in both the “300 mg once a day and “150 mg twice a day” simulation models. In Streptococcus pneumoniae and Staphylococcus aureus, sparfloxacin treatments of 300 mg once or 150 mg twice depressed the cells to more than 2 log levels without marked regrowth until 24 hours. The cell counts of Pseudomonas aeruginosa treated once with 300 mg of sparfloxacin were lower than those with 150 mg twice. In the former, no detectable cells were found until 7 hours. Thus, sparfloxacin showed a sufficient antibacterial activity in a model simulating wonce a day” administration. © 1991, Japanese Society of Chemotherapy. All rights reserved.",Scopus,2-s2.0-0025898933
English,Article,2021,"Hussein N., Hassan R., Fahey M.T.",Effect of pavement condition and geometrics at signalised intersections on casualty crashes,Journal of Safety Research,76,,,276,288,,,10.1016/j.jsr.2020.12.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100545429&doi=10.1016%2fj.jsr.2020.12.021&partnerID=40&md5=90dcaba1903c5ff9f03c9d54c3828148,"Introduction: This study investigated the effects of pavement surface condition and other control factors on casualty crashes at signalized intersections. It involved conducting a before and after study for road surface condition and situational factors. It also included assessing the effects of geometric characteristics on safety performance of signalized intersections post resurfacing to control for the effect of pavement surface condition. Pavement surface condition included roughness, rutting, and skid resistance. The control factors included traffic volume, light and surface moisture condition, and speed limit. The geometric characteristics included approach width, number of lanes, intersection depth, presence of median, presence of shared lane, and presence of bus stop. Method: To account for the repeated observations of the effect of light and surface moisture conditions in four occasions (day-dry, day-wet, night-dry and night-wet) Generalized Estimating Equation (GEE) with Negative Binomial (NB) and log link function was applied. For each signalized intersection in the sample, condition data are collected for the year before and after the year of surface treatment. Crash data, however, are collected for a minimum of three and maximum of five years before and after treatment years. Results: The results show that before treatment, light condition, road surface moisture condition, and skid resistance interaction with traffic volume are the significant contributors to crash occurrence. For after treatment; light condition, road surface moisture condition, their interaction product, and roughness interaction with light condition, surface moisture condition, and traffic volume are the significant contributors. The geometric variables that were found to have significant effects on crash frequency post resurfacing were approach width interactions with presence of shared lane, bus stop, or median. Conclusions: The findings confirm that resurfacing is significant in reducing crash frequency and severity levels. Practical Applications: The study findings would help for better understanding of how geometric characteristics can be improved to reduce crash occurrence. © 2021",Scopus,2-s2.0-85100545429
English,Article,2020,"Guinn N.R., Cooter M.L., Weiskopf R.B.",Lower hemoglobin concentration decreases time to death in severely anemic patients for whom blood transfusion is not an option,Journal of Trauma and Acute Care Surgery,88,6,,803,808,,2,10.1097/TA.0000000000002632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085532000&doi=10.1097%2fTA.0000000000002632&partnerID=40&md5=49a08b72c22a8a5d53c4f451ac1a5843,"BACKGROUND Anemia in patients who decline transfusion has been associated with increased morbidity and mortality. We hypothesized that the time to death decreases with increasing severity of anemia in patients for whom transfusion is not an option. METHODS With institutional review board approval, a retrospective review of registered adult blood refusal patients with at least one hemoglobin (Hb) value of 12.0 g/dL or less during hospital admission at a single institution from January 2004 to September 2015 was performed. The association of nadir Hb category and time to death (all-cause 30-day mortality) was determined using Kaplan-Meier plots, log rank tests, and Cox proportional hazard models. We investigated if there was a nadir Hb level between the values of 5.0 and 6.0 g/dL at which mortality risk significantly increased and then categorized nadir Hb by the traditional cut points and the newly identified ""critical"" cut point. RESULTS The study population included 1,011 patients. The Cox proportional hazard models showed a more than 50% increase in hazard of death per 1 g/dL decrease in Hb (adjusted hazard ratio [confidence interval], 1.55 [1.40-1.72]; p < 0.001). A Hb value of 5.0 g/dL was identified as defining ""critical anemia."" We found a strong association between anemia severity level and mortality (p < 0.001). Time to death was shorter (median, 2 days) in patients with critical anemia than in those having higher Hb (median time to death of 4 or 6 days, in severe or moderate anemia). CONCLUSION In anemic patients unable to be transfused, critical anemia was associated with a significantly and clinically important reduced time to death. LEVEL OF EVIDENCE Prognostic, level III. © Wolters Kluwer Health, Inc. All rights reserved.",Scopus,2-s2.0-85085532000
English,Conference Paper,2020,"Fang Z., Zhang Y., Zheng C., Wang X., Cheng M., Zhang H., Hong H.",Braking safety design and analysis for railway vehicles,"ASME International Mechanical Engineering Congress and Exposition, Proceedings (IMECE)",14,,V014T14A029,,,,,10.1115/IMECE2020-24673,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101211324&doi=10.1115%2fIMECE2020-24673&partnerID=40&md5=96a148a29e8f32e70e14fbe2ecfefd5c,"Brake is a safety critical system for railway vehicles and brake failures have caused many catastrophic accidents in the history. Detailed accident investigation reports are available and National Transportation Safety Board (NTSB) also makes safety recommendations to Federal Railroad Administration and the industry. However, there is limited research on how to improve the brake safety from the perspective of design, system integration and safety analysis. In this paper, a framework for braking safety design and analysis is introduced, which includes four parts: failure alarming system, safety design, safety analysis and preventative maintenance. For failure alarming, according to the severity level, the failures will be notified to the operator, to Operation Control Center (OCC) or saved for the maintainer. For safety design, redundant design for fail-safe feature, automatic braking, brake release, weight control, ergonomics design for easy operation and maintenance are discussed and several application examples are illustrated. In the safety analysis section, Preliminary Hazard Analysis (PHA) as a semi-quantitative analysis, Failure Modes, Effects, and Criticality Analysis (FMECA) as a bottom-up method and Fault Tree Analysis as a top-down method are used. The hazards details, system assurance actions and closure references are recorded in the Hazard Tracking Log (HTL) to ensure all the safety related items are well tracked and documented. Preventative Maintenance (PM) which is regularly performed on the brake components to lessen the likelihood of failing is also discussed in combination with the reliability prediction and safety analysis for a balance of safety and economy. The safety design framework and principles introduced in this paper can also be applied into other railway systems, such as Propulsion, Signaling, Doors, etc. and may provide insights to similar industries such as automotive, energy and so on. Copyright © 2020 ASME",Scopus,2-s2.0-85101211324
English,Article,2018,"Flórez-Tanus Á., Parra D., Zakzuk J., Caraballo L., Alvis-Guzmán N.",Health care costs and resource utilization for different asthma severity stages in Colombia: A claims data analysis 11 Medical and Health Sciences 1117 Public Health and Health Services,World Allergy Organization Journal,11,1,26,,,,9,10.1186/s40413-018-0205-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056895911&doi=10.1186%2fs40413-018-0205-4&partnerID=40&md5=9f263028a6065f2573c12eea41a7ae50,"Background: Asthma is one of the most common chronic respiratory conditions worldwide. Asthma-related economic burden has been reported in Latin America, but knowledge about its economic impact to the Colombian health care system and the influence of disease severity is lacking. This study estimated direct medical costs and health care resource utilization (HCRU) in patients with asthma according to severity in Colombia. Methods: This study identified all-age patients who had at least one medical event linked to an asthma diagnosis (CIE-10: J45-J46) between 2004 and 2014. Patients were selected if they had a continuous enrollment and uninterrupted insurance coverage between January 1-2015 and December 31-2015 and were categorized into 4 different severity levels using a modified algorithm based on Leidy criteria. Healthcare utilization and costs were estimated in a 1-year period after the identification period. A Generalized Linear Model (GLM) with gamma distribution and log link was used to analyze costs adjusting for patient demographics. Results: A total of 20,410 patients were included: 69.5% had mild intermittent, 18.0% mild persistent, 6.9% moderate persistent and 5.5% severe persistent asthma; with mean costs (SD) of $67 (134), $482 (1506), $1061 (1983), $2235 (3426) respectively (p < 0.001). The mean total direct cost was estimated at $331 (1278) per patient. Medication and hospitalization had the higher proportion in total costs (46% and 31% respectively). General physician visits was the most used service (57.2%) and short-acting β-2 agonists the most used medication (24%). Conclusions: Health services utilization and direct costs of asthma were highly related to disease severity. Nationwide health policies aimed at the effective control of asthma are necessary and would play an important role in reducing the associated economic impact. © 2018 The Author(s).",Scopus,2-s2.0-85056895911
English,Article,2018,"Bağbaba A., Şen B., Delen D., Uysal B.S.",An Automated Grading and Diagnosis System for Evaluation of Dry Eye Syndrome,Journal of Medical Systems,42,11,227,,,,2,10.1007/s10916-018-1086-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054592081&doi=10.1007%2fs10916-018-1086-3&partnerID=40&md5=b13eadc9f0b5183f957756687d0611a4,"This article describes methods used to determine the severity of Dry Eye Syndrome (DES) based on Oxford Grading Schema (OGS) automatically by developing and applying a decider model. The number of dry punctate dots occurred on corneal surface after corneal fluorescein staining can be used as a diagnostic indicator of DES severity according to OGS; however, grading of DES severity exactly by carefully assessing these dots is a rather difficult task for humans. Taking into account that current methods are also subjectively dependent on the perception of the ophtalmologists coupled with the time and resource intensive requirements, enhanced diagnosis techniques would greatly contribute to clinical assessment of DES. Automated grading system proposed in this study utilizes image processing methods in order to provide more objective and reliable diagnostic results for DES. A total of 70 fluorescein-stained cornea images from 20 patients with mild, moderate, or severe DES (labeled by an ophthalmologist in the Keratoconus Center of Yildirim Beyazit University Ataturk Training and Research Hospital) used as the participants for the study. Correlations between the number of dry punctate dots and DES severity levels were determined. When automatically created scores and clinical scores were compared, the following measures were observed: Pearson’s correlation value between the two was 0.981; Lin’s Concordance Correlation Coefficients (CCC) was 0.980; and 95% confidence interval limites were 0.963 and 0.989. The automated DES grade was estimated from the regression fit and accordingly the unknown grade is calculated with the following formula: Gpred = 1.3244 log(Ndots) - 0.0612. The study has shown the viability and the utility of a highly successful automated DES diagnostic system based on OGS, which can be developed by working on the fluorescein-stained cornea images. Proper implemention of a computationally savvy and highly accurate classification system, can assist investigators to perform more objective and faster DES diagnoses in real-world scenerios. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Scopus,2-s2.0-85054592081
English,Article,2017,Lee W.-J.,Anomaly detection and severity prediction of air leakage in train braking pipes,International Journal of Prognostics and Health Management,8,Special Issue 7,,,,12,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037683118&partnerID=40&md5=0893fb233d47d2bda9e8c220e446b924,"Air leakage in braking pipes is a commonly encountered mechanical defect on trains. A severe air leakage will lead to braking issues and therefore decrease the reliability and cause train delays or stranding. However, air leakage is difficult to be detected via visual inspection and therefore most air leakage defects are run to fail. In this research we present a framework that not only can detect air leakages but also predicts the severity of air leakages so that action plans can be determined based on the severity level. The proposed contextual anomaly detection method detects air leakages based on the on/off logs of a compressor. Air leakage causes failure in the context when the compressor idle time is short than the compressor run time, that is, the speed of air consumption is faster than air generation. In our method the logistic regression classifier is adopted to model two different classes of compressor behavior for each train separately. The logistic regression classifier defines the boundary separating the two classes under normal situations and models the distribution of the compressor idle time and run time separately using logistic functions. The air leakage anomaly is further detected in the context that when a compressor idle time is erroneously classified as a compressor run time. To distinguish anomalies from outliers and detect anomalies based on the severity degree, a density-based clustering method with a dynamic density threshold is developed for anomaly detection. The results have demonstrated that most air leakages can be detected one to four weeks before the braking failure and therefore can be prevented in time. Most importantly, the contextual anomaly detection method can pre-filter anomaly candidates and therefore avoid generating false alarms. To facilitate the decisionmaking process, the logistic function built on the compressor run time is further used together with the duration of an air leakage to model the severity of the air leakage. By building the prediction model on the severity, the remaining useful life of the air braking pipe until it reaches a certain level of severity can be estimated. © 2017, Prognostics and Health Management Society. All rights reserved.",Scopus,2-s2.0-85037683118
English,Article,2011,"Rose D., Paris T., Crews E., Wu S.S., Sun A., Behrman A.L., Duncan P.",Feasibility and effectiveness of circuit training in acute stroke rehabilitation,Neurorehabilitation and Neural Repair,25,2,,140,148,,32,10.1177/1545968310384270,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954456175&doi=10.1177%2f1545968310384270&partnerID=40&md5=e8aae9bdc8c5c0b36049de7d96fc8638,"Background: Task-specificity, repetition and progression are key variables in the acquisition of motor skill however they have not been consistently implemented in post-stroke rehabilitation. Objective: To evaluate the effectiveness of a stroke rehabilitation plan of care that incorporated task-specific practice, repetition and progression to facilitate functional gain compared to standard physical therapy for individuals admitted to an inpatient stroke unit. Methods: Individuals participated in either a circuit training (CTPT) model (n = 72) or a standard (SPT) model (n = 108) of physical therapy, 5 days/week. Each 60 minute circuit training session, delivered according to severity level, consisted of four functional mobility tasks. Daily exercise logs documented both task repetition and progression. Results: The CTPT model was successfully implemented in an acute rehabilitation setting. The CTPT group showed a significantly greater improved change in gait speed from hospital admission to discharge than the SPT group (0.21± 0.25 m/sec vs. 0.13± 0.22 m/sec; p = 0.03). The difference between groups occurred primarily among those who were ambulatory upon admission. There were no significant differences between the two cohorts at 90 days post-stroke as measured by the FONE-FIM, SF-36 and living location. Conclusions: Therapy focused on systematically progressed functional tasks can be successfully implemented in an inpatient rehabilitation stroke program. This circuit-training model resulted in greater gains in gait velocity over the course of inpatient rehabilitation compared to the standard model of care. Community-based services following hospital discharge to maintain these gains should be included in the continuum of post-stroke care. © The Author(s) 2011.",Scopus,2-s2.0-79954456175
English,Conference Paper,2006,"Dondo M.G., Japkowicz N., Smith R.",AutoCorrel: A neural network event correlation approach,Proceedings of SPIE - The International Society for Optical Engineering,6241,,62410N,,,,2,10.1117/12.665041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747338065&doi=10.1117%2f12.665041&partnerID=40&md5=9b3524d72cb57ce1f24dd52b82d57c03,"Intrusion detection analysts are often swamped by multitudes of alerts originating from installed intrusion detection systems (IDS) as well as logs from routers and firewalls on the networks. Properly managing these alerts and correlating them to previously seen threats is critical in the ability to effectively protect a network from attacks. Manually correlating events can be a slow tedious task prone to human error. We present a two-stage alert correlation approach involving an artificial neural network (ANN) autoassociator and a single parameter decision threshold-setting unit. By clustering closely matched alerts together, this approach would be beneficial to the analyst. In this approach, alert attributes are extracted from each alert content and used to train an autoassociator. Based on the reconstruction error determined by the autoassociator, closely matched alerts are grouped together. Whenever a new alert is received, it is automatically categorised into one of the alert clusters which identify the type of attack and its severity level as previously known by the analyst. If the attack is entirely new and there is no match to the existing clusters, this would be appropriately reflected to the analyst. There are several advantages to using an ANN based approach. First, ANNs acquire knowledge straight from the data without the need for a human expert to build sets of domain rules and facts. Second, once trained, ANNs can be very fast, accurate and have high precision for near real-time applications. Finally, while learning, ANNs perform a type of dimensionality reduction allowing a user to input large amounts of information without fearing an efficiency bottleneck. Thus, rather than storing the data in TCP Quad format (which stores only seven event attributes) and performing a multi-stage query on reduced information, the user can input all the relevant information available and instead allow the neural network to organise and reduce this knowledge in an adaptive and goal-oriented fashion.",Scopus,2-s2.0-33747338065
English,Article,2012,"de Souza Gomes Guarino E., Scariot A.O.",Tree seedling survival and growth in logged and undisturbed seasonal deciduous forest fragments in central Brazil,Journal of Forest Research,17,2,,193,201,,6,10.1007/s10310-011-0294-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857918259&doi=10.1007%2fs10310-011-0294-1&partnerID=40&md5=2bae196ed291b9fa5fe22d428a8365aa,"We evaluated the survival and growth of Amburana cearensis, Cedrela fissilis, and Sterculia striata seedlings in three seasonally tropical dry forest fragments that were subjected to different logging levels (intact, intermediately and heavily logged). In each fragment, we planted 40 seedlings of each species and monitored these over a period of 1 year. The highest seedling survival rate (64%) occurred in the heavily logged fragment, which, however, also had the highest mortality risk for all species during the dry season. Only S. striata seedlings had different survival rates among the fragments. Height and diameter growth were higher at sites with higher canopy openness in the wet season. The survival and growth rates of seedlings planted in logged fragments indicate that this technique can be applied to restore and enrich logged forests of the Paranã River Basin. © 2011 The Japanese Forest Society and Springer.",Scopus,2-s2.0-84857918259
English,Article,2018,"Hsu T.-C., Chang C.-C., Yuan M.-H., Chang C.-Y., Chen Y.-H., Lin C.-F., Ji D.-R., Shie J.-L., Manh D.V., Wu C.-H., Chiang S.-W., Lin F.-C., Lee D.-J., Huang M., Chen Y.-H.",Upgrading of Jatropha-seed residue after mechanical extraction of oil via torrefaction,Energy,142,,,773,781,,9,10.1016/j.energy.2017.10.046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032178523&doi=10.1016%2fj.energy.2017.10.046&partnerID=40&md5=a8d1452555e1221bb6465db85e233ce9,"This study examined the torrefaction performance of Jatropha-seed residue after mechanical screw-press extraction of oil (denoted as JME) at fixed torrefaction temperature (Tr) of 533, 553 and 573 K in the torrefaction time (tr) range of 10–60 min. The characteristics results of torrefied JME (JMET) indicated that the mass yield decreases while dry-basis high heating value (HHD) increases with increasing Tr and tr. At tr = 20 min or longer and Tr of 533–573 K, the HHD of obtained JMET is suitable for B1/B2/D Taiwan power industry fuel requirement. In addition, the H/C and O/C atomic ratios of JMET are better (i.e., lower) than those of torrefied wood and close to those of lignite and sub-bituminous coal. The satisfactory energy densification (ED) of about 1.3 can be achieved for Case 1 at higher Tr of 573 K and shorter tr of 30 min with log severity factor (logSF) of 7.37 or Case 2 at lower Tr of 553 K and longer tr of 50 min with logSF of 7.00. The results indicated that Case 2 exhibits a lower value of SF than Case 1 at the same level of satisfactory ED, which is more beneficial for the torrefaction of JME. © 2017",Scopus,2-s2.0-85032178523
English,Article,2020,Kaufmann S.,COVID-19 outbreak and beyond: the information content of registered short-time workers for GDP now- and forecasting,Swiss Journal of Economics and Statistics,156,1,12,,,,,10.1186/s41937-020-00053-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090985213&doi=10.1186%2fs41937-020-00053-x&partnerID=40&md5=38c7b208d05ee5b51809a21affd7588a,"The number of short-time workers from January to April 2020 is used to now- and forecast quarterly GDP growth. We purge the monthly log level series from the systematic component to extract unexpected changes or shocks to log short-time workers. These monthly shocks are included in a univariate model for quarterly GDP growth to capture timely, current-quarter unexpected changes in growth dynamics. Included shocks additionally explain 24% in GDP growth variation. The model is able to forecast quite precisely the decrease in GDP during the financial crisis. It predicts a mean decline in GDP of 5.7% over the next two quarters. Without additional growth stimulus, the GDP level forecast remains persistently 4% lower in the long run. The uncertainty is large, as the 95% highest forecast density interval includes a decrease in GDP as large as 9%. A recovery to pre-crisis GDP level in 2021 lies only in the upper tail of the 95% highest forecast density interval. © 2020, The Author(s).",Scopus,2-s2.0-85090985213
English,Article,2018,"Rahman M., Mustafa M.",Determining total CEO compensation of selected US public companies,International Journal of Managerial Finance,14,2,,170,187,,2,10.1108/IJMF-03-2017-0047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042866835&doi=10.1108%2fIJMF-03-2017-0047&partnerID=40&md5=a9995d2d7ddec490e6c19091467ff6d6,"Purpose: The purpose of this paper is to explore the effects of total assets, stock performances, CEOs’ tenures, ages, and board sizes on total CEO compensations of 249 publicly listed US companies over a nine-year period from 2004-2012. Design/methodology/approach: Pedroni’s panel cointegration, generalized method of moments, and dynamic ordinary least squares methodologies are applied. Findings: All variables are non-stationary in log-levels. The findings show significant positive effects of total assets and stock performances on total CEO compensations. The effects of CEO’s tenure and age as well as board size on total CEO compensation deem negative. However, short-run net interactive feedback effects are generally positive with some exceptions. Research limitations/implications: The above variables matter in rewarding the CEOs. They should be carefully weighed in for proper formulation of CEO compensation policy. Originality/value: This paper applies relatively new econometric tools for a large panel data set. This work considers some new variables for determining CEO compensation in USA. The findings are relatively new with empirical originality. © 2018, Emerald Publishing Limited.",Scopus,2-s2.0-85042866835
English,Book Chapter,2018,Thirlwall A.P.,John McCombie’s contribution to the applied economics of growth in a closed and open economy,Alternative Approaches in Macroeconomics: Essays in Honour of John McCombie,,,,23,56,,,10.1007/978-3-319-69676-8_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046930433&doi=10.1007%2f978-3-319-69676-8_2&partnerID=40&md5=0ed79a7af12a7ae7767c2ca28ac7c222,"Tony Thirlwall, in this chapter, entitled ‘John McCombie’s Contribution to the Applied Economics of Growth in a Closed and Open Economy’, focuses on John McCombie’s major contributions to our understanding of growth rate differences between countries. This chapter is divided into three parts. The first part deals with Kaldor’s growth laws and particularly John’s work on Verdoorn’s Law-its estimation, and resolving the static/dynamic paradox that increasing returns are found when the growth of labour productivity is regressed on the growth of manufacturing output but not when the log level of productivity is regressed on the level of manufacturing output. The second part outlines John’s contribution to the theory of balance of payments-constrained growth, particularly showing that the dynamic Harrod trade multiplier result can be interpreted as the Hicks super multiplier. The third part shows that Kaldor’s first law of growth, that manufacturing is the engine of growth, is also a reduced form of an export-led growth model, because the export growth of countries is closely related to the growth of manufactured exports. © The Author(s) 2018.",Scopus,2-s2.0-85046930433
English,Article,2013,"Chen Y.-H., Quan L.",Rational speculative bubbles in the Asian stock markets: Tests on deterministic explosive bubbles and stochastic explosive root bubbles,Journal of Asset Management,14,3,,195,208,,2,10.1057/jam.2013.13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885658532&doi=10.1057%2fjam.2013.13&partnerID=40&md5=a81d095350942a7f56f10af356a06ae4,"The standard theory of asset pricing, in which a long-run relationship should exist between stock prices and dividends if there are no deterministic explosive bubbles, assumes the constancy of expected returns. However, the investor's expected returns are more likely to be time varying, which have led to the modification for the tests of rational bubble. One modification is that the tests should be applied to the log levels of stock price and dividend for allowing the detection of the stochastic explosive root bubble, which incorporates the possibility of time-varying expected returns. Accordingly, we test the existence or otherwise of both types of rational bubbles in the Asian stock markets by applying the unit root tests and the cointegration analyses. The empirical results suggest that the rational bubbles exist in the stock markets of Japan, Singapore, Korea, Taiwan, Thailand, Malaysia, Indonesia and Philippine, whereas Hong Kong is found to have no rational bubbles. © 2013 Macmillan Publishers Ltd. 1470-8272.",Scopus,2-s2.0-84885658532
English,Article,2012,"Feige I., Schwartz M.D., Stewart I.W., Thaler J.",Precision jet substructure from boosted event shapes,Physical Review Letters,109,9,92001,,,,54,10.1103/PhysRevLett.109.092001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865585256&doi=10.1103%2fPhysRevLett.109.092001&partnerID=40&md5=7ef1085dd97651cc8fd4cb7a5a881d7c,"Jet substructure has emerged as a critical tool for LHC searches, but studies so far have relied heavily on shower MonteCarlo simulations, which formally approximate QCD at the leading-log level. We demonstrate that systematic higher-order QCD computations of jet substructure can be carried out by boosting global event shapes by a large momentum Q and accounting for effects due to finite jet size, initial-state radiation (ISR), and the underlying event (UE) as 1/Q corrections. In particular, we compute the 2-subjettiness substructure distribution for boosted Z→qq̄ events at the LHC at next-to-next-to-next-to-leading-log order. The calculation is greatly simplified by recycling known results for the thrust distribution in e +e - collisions. The 2-subjettiness distribution quickly saturates, becoming Q independent for Q 400GeV. Crucially, the effects of jet contamination from ISR/UE can be subtracted out analytically at large Q without knowing their detailed form. Amusingly, the Q= and Q=0 distributions are related by a scaling by e up to next-to-leading-log order. © 2012 American Physical Society.",Scopus,2-s2.0-84865585256
English,Article,2007,"Mccombie J.S.L., Roberts M.",Returns to scale and regional growth: The static-dynamic verdoorn law paradox revisited,Journal of Regional Science,47,2,,179,208,,40,10.1111/j.1467-9787.2007.00505.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247339326&doi=10.1111%2fj.1467-9787.2007.00505.x&partnerID=40&md5=d73567900ec572e40cff6572469d2689,"It has long been an article of faith amongst regional economists that increasing returns to scale are necessary to explain the punctiform location of economic activity and population. However, there is no consensus in the empirical literature over whether returns to scale are constant or increasing. A notable example of this lack of agreement is provided by the static-dynamic Verdoorn law paradox. While the dynamic Verdoorn law (specified using growth rates) yields estimates of substantial increasing returns to scale, the static Verdoorn law (specified using log-levels) indicates only the presence of constant returns to scale. In this paper, we explain the static-dynamic Verdoorn law paradox by showing that estimates of returns to scale obtained using the static law are subject to a spatial aggregation bias, which biases the estimates towards constant returns to scale. We illustrate our arguments by means of simulation exercises. The results obtained hold general lessons for applied economic analysis using spatial data. © Blackwell Publishing, Inc. 2007.",Scopus,2-s2.0-34247339326
English,Article,2006,"Wernsdörfer H., Le Moguédec G., Constant T., Mothe F., Nepveu G., Seeling U.",Modelling of the shape of red heartwood in beech trees (Fagus sylvatica L.) based on external tree characteristics,Annals of Forest Science,63,8,,905,913,,14,10.1051/forest:2006074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845484822&doi=10.1051%2fforest%3a2006074&partnerID=40&md5=080ce2a5b37090c8d7aa9da42966b313,"The shape of red heartwood in beech was studied on 16 trees, based on the mean red heart radius at about every 2 m along the stem axis up to the crown base. The longitudinal red heart shape was modelled by sections of bell-shaped curves, given by an exponential function with a fourth order polynomial term. Using individual tree parameters for the red heart width, length and height, the observed red heart shapes were closely described by the model. An approach of a predictive model at the standing tree level was developed for estimating these parameters from the diameter at breast height, height of the crown base related to total tree height and height of a possible red heart initiation point. Remaining issues concerning the model structure should be analysed on a higher number of samples. An application of the model at the log level could be developed. © INRA, EDP Sciences, 2006.",Scopus,2-s2.0-33845484822
English,Article,1978,"Amati D., Petronzio R., Veneziano G.",Relating hard QCD processes through universality of mass singularities,"Nuclear Physics, Section B",140,1,,54,72,,213,10.1016/0550-3213(78)90313-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002762030&doi=10.1016%2f0550-3213%2878%2990313-9&partnerID=40&md5=4a3f239f782e2bc432677a5ffc67e33e,"Hard QCD processes involving final jets are studied and compared by means of a simple approach to mass singularities. This is based on the Lee-Nauenberg-Kinoshita theorem and on a rather subtle use of gauge invariance in hard collinear gluon bremsstrahlung. One-loop results are easily derived for processes involving any number of initial quarks and/or currents. The method greatly simplifies the computation of higher-order loops at the leading log level and our preliminary results allow us to conclude that the crucial features encountered at the one-loop level will persist. We are thus able to relate different hard processes and to show that suitable ratios of cross sections, being free from mass singularities, can be computed perturbatively, as usually assumed in QCD-inspired parton models. We are also able to relate our universal leading mass singularities to leading scaling violations and to extend therefore the results of the operator product expansion method to processes outside the range of the light-cone analysis. Some delicate points caused by confinement-related singularities (e.g., narrow resonance poles) are also discussed. © 1978.",Scopus,2-s2.0-0002762030
English,Article,2009,"Iyer S., Saunders W.B., Stemkowski S.",Economic burden of postoperative Ileus associated with colectomy in the United States,Journal of Managed Care Pharmacy,15,6,,485,494,,200,10.18553/jmcp.2009.15.6.485,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149089004&doi=10.18553%2fjmcp.2009.15.6.485&partnerID=40&md5=7f0f97092270bee825b1a4f93c0ccd0d,"BACKGROUND: Postoperative ileus, a transient impairment of gastrointestinal motility, is a common cause of delay in return to normal bowel function after abdominal surgery. Colectomy surgery patients who develop postoperative ileus could have greater health care resource utilization, including prolonged hospitalization, compared with those who do not develop post-operative ileus. Very few studies have assessed the impact of postoperative ileus on resource utilization and costs using retrospective analysis of administrative databases. OBJECTIVE: To assess health care utilization and costs in colectomy surgery patients who developed postoperative ileus versus those who did not. METHODS: A retrospective cohort study design was used. Adult patients with a principal procedure code for colectomy (ICD-9-CM procedure codes 45.71-45.79), discharged between January 1,2004, and December 31, 2004, were identified from the Premier Perspective database of inpatient records from more than 500 hospitals In the United States. The colectomy patients were further classified for the presence of postoperative ileus, identified by the presence, in any diagnosis field on the administrative patient records, of a code for paralytic Ileus (ICD-9-CM code 560.1) and/or digestive system complications (ICD-9-CM code 997.4) during the inpatient stay. Code 997.4 was used to account for cases in which postoperative ileus would be reported as a complication of anastomosis, as could be the case in colectomy surgeries. Hospital length of stay (LOS) and hospitalization costs were compared using t-tests. Multivariate analyses were performed with log-transformed LOS and log-transformed cost as the dependent variables. Patient demographics, mortality risk, disease severity, admission source, payment type (retrospective/prospective), and hospital characteristics were used as covariates. RESULTS: A total of 17,876 patients with primary procedure code for colectomy were identified, of whom 3,115 (17.4%) patients were classified for presence of postoperative ileus (including paralytic ileus only [n=1,216; 6.8%], digestive system complications only [n=383; 2.1%], or both [n=1,516; 8.5%]). A majority of the colectomy patients with and without postoperative ileus, respectively, were male (54.1% vs. 50.3%, P<0.001), Caucasian (70.5% vs. 69.3%, P=0.170), and aged 51-64 years (51.1% vs. 49.7%, P=0.143). The mean [SD] hospital LOS was significantly longer in patients with postoperative ileus (13.8 [13.3] days) compared with patients without postoperative ileus (8.9 [9.5] days; P< 0.001). Presence of post-operative ileus was found to be a significant predictor of LOS (P< 0.001) in the regression model, controlling for covariates. Female gender (P= 0.002), greater severity level (P< 0.001), and hospital bed size of more than 500 (P= 0.013) were other significant predictors of hospital LOS. Presence of postoperative ileus was found to be a significant predictor of hospitalization costs (P<0.001), controlling for covariates. CONCLUSION: Postoperative ileus in colectomy patients is a significant predictor of hospital resource utilization. Copyright© 2009, Academy of Managed Care Pharmacy. All rights reserved.",Scopus,2-s2.0-69149089004
English,Article,2020,"Hayashi H., Fukutomi Y., Mitsui C., Kajiwara K., Watai K., Kamide Y., Nakamura Y., Hamada Y., Tomita Y., Sekiya K., Tsuburai T., Izuhara K., Wakahara K., Hashimoto N., Hasegawa Y., Taniguchi M.",Omalizumab for aspirin hypersensitivity and leukotriene overproduction in aspirin-exacerbated respiratory disease,American Journal of Respiratory and Critical Care Medicine,201,12,,1488,1498,,16,10.1164/rccm.201906-1215OC,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086523736&doi=10.1164%2frccm.201906-1215OC&partnerID=40&md5=47b6e581175cff3aa3fb780972b5d460,"Rationale: Aspirin-exacerbated respiratory disease is characterized by severe asthma, nonsteroidal antiinflammatory drug hypersensitivity, nasal polyposis, and leukotriene overproduction. Systemic corticosteroid therapy does not completely suppress lifelong aspirin hypersensitivity. Omalizumab efficacy against aspirin-exacerbated respiratory disease has not been investigated in a randomized manner. Objectives: To evaluate omalizumab efficacy against aspirin hypersensitivity, leukotriene E4 overproduction, and symptoms during an oral aspirin challenge in patients with aspirin-exacerbated respiratory disease using a randomized design. Methods: We performed a double-blind, randomized, crossover, placebo-controlled, single-center study at Sagamihara National Hospital between August 2015 and December 2016. Atopic patients (20–79 yr old) with aspirin-exacerbated respiratory disease diagnosed by systemic aspirin challenge were randomized (1:1) to a 3-month treatment with omalizumab or placebo, followed by a .18-week washout period (crossover design). The primary endpoint was the difference in area under logarithm level of urinary leukotriene E4 concentration versus time curve in the intent-to-treat population during an oral aspirin challenge. Measurements and Main Results: Sixteen patients completed the study and were included in the analysis. The area under the logarithm level of urinary leukotriene E4 concentration versus time curve during an oral aspirin challenge was significantly lower in the omalizumab phase (median [interquartile range], 51.1 [44.5–59.8]) than in the placebo phase (80.8 [interquartile range, 65.4–87.8]) (P, 0.001). Ten of 16 patients (62.5%) developed oral aspirin tolerance up to cumulative doses of 930 mg in the omalizumab phase (P, 0.001). Conclusions: Omalizumab treatment inhibited urinary leukotriene E4 overproduction and upper/lower respiratory tract symptoms during an oral aspirin challenge, resulting in aspirin tolerance in 62.5% of the patients with aspirin-exacerbated respiratory disease. Copyright © 2020 by the American Thoracic Society",Scopus,2-s2.0-85086523736
English,Article,2019,"Spiess B., Rinaldetti S., Naumann N., Galuschek N., Kossak-Roth U., Wuchter P., Tarnopolscaia I., Rose D., Voskanyan A., Fabarius A., Hofmann W.-K., Saußele S., Seifarth W.",Diagnostic performance of the molecular BCR-ABL1 monitoring system may impact on inclusion of CML patients in stopping trials,PLoS ONE,14,3,e0214305,,,,5,10.1371/journal.pone.0214305,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063345854&doi=10.1371%2fjournal.pone.0214305&partnerID=40&md5=920f3f95c3d93899e00d9b8261de718d,"In chronic myeloid leukemia (CML), the duration of deep molecular response (MR) before treatment cessation (MR 4 or deeper, corresponding to BCR-ABL1 0.01% on the International Scale (IS)) is considered as a prognostic factor for treatment free remission in stopping trials. MR level determination is dependent on the sensitivity of the monitoring technique. Here, we compared a newly established TaqMan (TM) and our so far routinely used LightCycler (LC) quantitative reverse transcription (qRT)-PCR systems for their ability to achieve the best possible sensitivity in BCR-ABL1 monitoring. We have comparatively analyzed RNA samples from peripheral blood mononuclear cells of 92 randomly chosen patients with CML resembling major molecular remission (MMR) or better and of 128 CML patients after treatment cessation (EURO-SKI stopping trial). While our LC system utilized ABL1, the TM system is based on GUSB as reference gene. We observed 99% concordance with respect to achievement of MMR. However, we found that 34 of the 92 patients monitored by TM/GUSB were re-classified to the next inferior MR log level, especially when LC/ABL1-based results were borderline to thresholds. Thirteen patients BCR-ABL1 negative in LC/ABL1 became positive after TM/GUSB analysis. In the 128 patients included in the EURO-SKI trial identical molecular findings were achieved for 114 patients. However, 14 patients were re-classified to the next inferior log-level by the TM/GUSB combination. Eight of these patients relapsed after treatment cessation; two of them were re-classified from MR 4 to MMR and therefore did not meet inclusion criteria anymore. In conclusion, we consider both methods as comparable and interchangeable in terms of achievement of MMR and of longitudinal evaluation of clinical courses. However, in LC/ABL1 negative samples, slightly enhanced TM/GUSB sensitivity may lead to inferior classification of clinical samples in the context of TFR. © 2019 Spiess et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",Scopus,2-s2.0-85063345854
English,Article,2012,"Altarejos-García L., Martínez-Chenoll M.L., Escuder-Bueno I., Serrano-Lombillo A.",Assessing the impact of uncertainty on flood risk estimates with reliability analysis using 1-D and 2-D hydraulic models,Hydrology and Earth System Sciences,16,7,,1895,1914,,14,10.5194/hess-16-1895-2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863824216&doi=10.5194%2fhess-16-1895-2012&partnerID=40&md5=5261203397a8ca7470d7c9fc5678f77e,"This paper addresses the use of reliability techniques such as Rosenblueth's Point-Estimate Method (PEM) as a practical alternative to more precise Monte Carlo approaches to get estimates of the mean and variance of uncertain flood parameters water depth and velocity. These parameters define the flood severity, which is a concept used for decision-making in the context of flood risk assessment. The method proposed is particularly useful when the degree of complexity of the hydraulic models makes Monte Carlo inapplicable in terms of computing time, but when a measure of the variability of these parameters is still needed. The capacity of PEM, which is a special case of numerical quadrature based on orthogonal polynomials, to evaluate the first two moments of performance functions such as the water depth and velocity is demonstrated in the case of a single river reach using a 1-D HEC-RAS model. It is shown that in some cases, using a simple variable transformation, statistical distributions of both water depth and velocity approximate the lognormal. As this distribution is fully defined by its mean and variance, PEM can be used to define the full probability distribution function of these flood parameters and so allowing for probability estimations of flood severity. Then, an application of the method to the same river reach using a 2-D Shallow Water Equations (SWE) model is performed. Flood maps of mean and standard deviation of water depth and velocity are obtained, and uncertainty in the extension of flooded areas with different severity levels is assessed. It is recognized, though, that whenever application of Monte Carlo method is practically feasible, it is a preferred approach. © Author(s) 2012. CC Attribution 3.0 License.",Scopus,2-s2.0-84863824216
English,Article,2009,"Paterno M.D., Maviglia S.M., Gorman P.N., Seger D.L., Yoshida E., Seger A.C., Bates D.W., Gandhi T.K.",Tiering Drug-Drug Interaction Alerts by Severity Increases Compliance Rates,Journal of the American Medical Informatics Association,16,1,,40,46,,157,10.1197/jamia.M2808,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57149146842&doi=10.1197%2fjamia.M2808&partnerID=40&md5=9b5e19196c69ff025909b754a584298f,"Objective: Few data exist measuring the effect of differentiating drug-drug interaction (DDI) alerts in computerized provider order entry systems (CPOE) by level of severity (""tiering""). We sought to determine if rates of provider compliance with DDI alerts in the inpatient setting differed when a tiered presentation was implemented. Design: We performed a retrospective analysis of alert log data on hospitalized patients at two academic medical centers during the period from 2/1/2004 through 2/1/2005. Both inpatient CPOE systems used the same DDI checking service, but one displayed alerts differentially by severity level (tiered presentation, including hard stops for the most severe alerts) while the other did not. Participants were adult inpatients who generated a DDI alert, and providers who wrote the orders. Alerts were presented during the order entry process, providing the clinician with the opportunity to change the patient's medication orders to avoid the interaction. Measurements: Rate of compliance to alerts at a tiered site compared to a non-tiered site. Results: We reviewed 71,350 alerts, of which 39,474 occurred at the non-tiered site and 31,876 at the tiered site. Compliance with DDI alerts was significantly higher at the site with tiered DDI alerts compared to the non-tiered site (29% vs. 10%, p < 0.001). At the tiered site, 100% of the most severe alerts were accepted, vs. only 34% at the non-tiered site; moderately severe alerts were also more likely to be accepted at the tiered site (29% vs. 10%). Conclusion: Tiered alerting by severity was associated with higher compliance rates of DDI alerts in the inpatient setting, and lack of tiering was associated with a high override rate of more severe alerts. © 2009 J Am Med Inform Assoc.",Scopus,2-s2.0-57149146842
English,Article,2005,Aiello L.P.,The effect of ruboxistaurin on visual loss in patients with moderately severe to very severe nonproliferative diabetic retinopathy: Initial results of the protein kinase C β inhibitor diabetic retinopathy study (PKC-DRS) multicenter randomized clinical trial,Diabetes,54,7,,2188,2197,,229,10.2337/diabetes.54.7.2188,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21344435254&doi=10.2337%2fdiabetes.54.7.2188&partnerID=40&md5=371379f234e9d105c547f8be82dc4c65,"The purpose of this study was to evaluate the safety and efficacy of the orally administered protein kinase C (PKC) β isoform-selective inhibitor ruboxistaurin (RBX) in subjects with moderately severe to very severe nonproliferative diabetic retinopathy (NPDR). In this multicenter, double-masked, randomized, placebo-controlled study, 252 subjects received placebo or RBX (8,16, or 32 mg/day) for 36-46 months. Patients had an Early Treatment Diabetic Retinopathy Study (ETDRS) retinopathy severity level between 47B and 53E inclusive, an ETDRS visual acuity of 20/125 or better, and no history of scatter (panretinal) photocoagulation. Efficacy measures included progression of DR, moderate visual loss (MVL) (doubling of the visual angle), and sustained MVL (SMVL). RBX was well tolerated without significant adverse effects but had no significant effect on the progression of DR. Compared with placebo, 32 mg/day RBX was associated with a delayed occurrence of MVL (log rank, P = 0.038) and of SMVL (P = 0.226). RBX reduction of SMVL was evident only in eyes with definite diabetic macular edema at baseline (10% 32 mg/day RBX vs. 25% placebo, P = 0.017). In multivariable Cox proportional hazard analysis, 32 mg/ day RBX significantly reduced the risk of MVL compared with placebo (hazard ratio 0.37 [95% CI 0.17-0.80], P = 0.012). In this clinical trial, RBX was well tolerated and reduced the risk of visual loss but did not prevent DR progression. © 2005 by the American Diabetes Association.",Scopus,2-s2.0-21344435254
Chinese,Article,2016,"Chen S., Du P.-Y., Zheng H.","Analysis of the Viral Load and Clinical Features of Children With Hand, Foot and Mouth Disease by Different Enteroviruses",Chinese General Practice,19,18,,2211,2215,,,10.3969/j.issn.1007-9572.2016.18.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048559695&doi=10.3969%2fj.issn.1007-9572.2016.18.020&partnerID=40&md5=0067696f01eaa94bfc1f0adeeb750108,"Objective: To analyze the viral load and clinical features of children with hand, foot and mouth disease(HFMD) by EV71 and CoxA16.Methods: A total of 430 children who were definitely diagnosed with HFMD in the Department of Pediatrics in Tangshan Women and Children Health-care Hospital from May to September in 2015 were enrolled.According to clinical symptoms, physical signs and species of virus, the children were divided into EV71 mild group(162 cases), EV71 severe group(87 cases), CoxA16 mild group(158 cases)and CoxA16 severe group(23 cases).Throat swabs of the children were collected and the RNA of EV71 and CoxA16 was detected by real-time fluorescent quantitative RT-qPCR method, and viral load was calculated.Length of disease, temperature, length of fever, dental ulcer, salivation, cough, hand rash, foot rash, somnolence, convulsion, vomit, change of consciousness, limb jitter, and myospasm of the children were recorded.Results: Cycle threshold(Ct value) shown by standard curve had highly negative correlation with the viral load(r=-1.000, P<0.01).The relation between the Ct value(X) and the logarithm of viral load(Y) was Y=-0.29X+13.03.CoxA16 mild group and CoxA16 severe group were lower than EV71 mild group and EV71 severe group in Ct value (P<0.05).There was no significant difference in the Ct value of HFMD children between EV71 mild group and EV71 severe group and between CoxA16 mild group and CoxA16 severe group(P>0.05).EV71 severe group and CoxA16 severe group had longer length of disease than EV71 mild group and CoxA16 mild group (P<0.05).EV71 severe group had longer length of disease than CoxA16 severe group (P<0.05).The four groups were not significantly different in the incidence rates of temperature ≥38.5℃, dental ulcer, salivation, hand rash and foot rash(P>0.05).EV71 severe group and CoxA16 severe group had higher incidence rate of the length of fever ≥3 d than EV71 mild group and CoxA16 mild group (P<0.007).EV71 severe group and CoxA16 severe group had higher incidence rate of temperature ≥38.5℃ plus length of fever ≥3 d than EV71 mild group and CoxA16 mild group, EV71 mild group was higher than CoxA16 mild group in the incidence rate of temperature ≥38.5℃ plus length of fever ≥3 d, and EV71 severe group was higher than CoxA16 severe group in the incidence rate of temperature ≥38.5℃ plus length of fever ≥3 d(P<0.007).CoxA16 mild group and CoxA16 severe group had higher incidence rates of cough, the number of hand rashes ≥15(both hands) and the number of foot rashes ≥15(both feet) than EV71 mild group and EV71 severe group(P<0.007).EV71 severe group and CoxA16 severe group had higher incidence rates of somnolence and vomit than EV71 mild group, and EV71 severe group was higher than CoxA16 severe group in the incidence rates of somnolence(P<0.01).Conclusion: The viral load of HFMD children varies with different types of enterovirus but is not different among different disease severity levels.HFMD children with different types of enterovirus and disease severity levels are different in the incidence rates of length of fever ≥3 d, temperature ≥38.5℃ plus length of fever ≥3 d, cough, the number of hand rashes ≥15(both hands) and the number of foot rashes ≥15(both feet), somnolence, convulsion, vomit, change of consciousness, limb jitter and myospasm. Copyright © 2016 by the Chinese General Practice.",Scopus,2-s2.0-85048559695
English,Article,2015,"Johnson D.A., Cummings T.F.","Effect of extended crop rotations on incidence of black dot, Silver scurf, and verticillium wilt of potato",Plant Disease,99,2,,257,262,,13,10.1094/PDIS-03-14-0271-RE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920948107&doi=10.1094%2fPDIS-03-14-0271-RE&partnerID=40&md5=a1af8d8f8e2a6b7c6ed151f8ea21e711,"Potato tubers were collected and evaluated for symptoms and signs of black dot, silver scurf, and Verticillium wilt to determine the effect of extended crop rotations on disease incidences in the Columbia Basin. Incidence of tubers with black dot collected from storage significantly decreased as the number of years between potato crops increased from 3 to 5 years and beyond and significantly increased as the number of previous potato crops increased to 16. The highest incidence of black dot (range of 73 to 98%) was from fields rotated out of potatoes for 1 to 3 years. The mean incidence of black dot was 56% for fields out of potatoes for 0 to 4 years and 12% for fields out of potatoes 5 and more years. A low incidence (0 to 9%) of black dot was detected at 15 years out of potatoes. Years out of potato and number of prior potato crops accounted for 71% of the variability associated with the incidence of black dot. Severity of black dot on tuber periderm peels significantly increased as incidence of tuber periderm peels with Colletotrichum coccodes increased. Coefficient of determination was 0.87 for log severity on regressed on black dot incidence. Incidence of silver scurf was highest from fields out of potatoes for 1 year. Incidence of silver scurf infected tubers significantly increased as the number of previous potato crops increased due to short rotations between potato crops. Incidence of tubers with Verticillium dahliae was not related to years between potato crops or number of previous potato crops. The present study confirmed that black dot can be reduced with rotations out of potatoes greater than 5 years. © 2015 The American Phytopathological Society.",Scopus,2-s2.0-84920948107
English,Conference Paper,2019,"Balmau O., Yuan H., Didona D., Guerraoui R., Arora A., Gupta K., Zwaenepoel W., Konka P.","Triad: Creating synergies between memory, disk and log in log structured key-value stores","Proceedings of the 2017 USENIX Annual Technical Conference, USENIX ATC 2017",,,,363,375,,32,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077460358&partnerID=40&md5=fdf21df1c8eab5be2bf1476e85d565e6,"We present TRIAD, a new persistent key-value (KV) store based on Log-Structured Merge (LSM) trees. TRIAD improves LSM KV throughput by reducing the write amplification arising in the maintenance of the LSM tree structure. Although occurring in the background, write amplification consumes significant CPU and I/O resources. By reducing write amplification, TRIAD allows these resources to be used instead to improve user-facing throughput. TRIAD uses a holistic combination of three techniques. At the LSM memory component level, TRIAD leverages skew in data popularity to avoid frequent I/O operations on the most popular keys. At the storage level, TRIAD amortizes management costs by deferring and batching multiple I/O operations. At the commit log level, TRIAD avoids duplicate writes to storage. We implement TRIAD as an extension of Facebook's RocksDB and evaluate it with production and synthetic workloads. With these workloads, TRIAD yields up to 193% improvement in throughput. It reduces write amplification by a factor of up to 4x, and decreases the amount of I/O by an order of magnitude. © USENIX Annual Technical Conference, USENIX ATC 2017. All rights reserved.",Scopus,2-s2.0-85077460358
English; Italian,Article,2019,"Garozzo R., Turco M.L., Santagati C.",Information models to manage complexity for an integrated knowledge project [Modelli informativi e gestione della complessitàper un progetto integrato di conoscenza],Disegno,2019,4,,225,236,,1,10.26375/disegno.4.2019.21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072179046&doi=10.26375%2fdisegno.4.2019.21&partnerID=40&md5=3bf344e2ce38b5d0abae35bcbe9e6f63,"The study aims to identify optimal workflows to create information models oriented to the management and the knowledge of architectural heritage in a state of ruin, through the analysis of the critical issues found in the parametric modeling of the existing artifact. The methodology is aimed at analysing possible criteria for the enhancement of the data detected in the transition from point cloud to a semantic model, and the management of the level of graphic detail (LoG, Level of Geometry) and information attributes (LoI, Level of Information), in order to define possible procedures to measure the Level of Reliability of the survey. The case study is the Mother Church of the ancient Misterbianco (Catania), one of the rare surviving vestiges of the eruption of Mount Etna in 1669 and the ear-thquake in Val di Noto in 1693. Thanks to its state of preservation and its cultural relevance, it represents the ideal case study for the proposed experimentation. (R.G., M.L.T., C.S.). © 2019, UID Unione Italiana Disegno. All rights reserved.",Scopus,2-s2.0-85072179046
English,Article,2018,"McCombie J.S.L., Spreafico M.R.M., Xu S.","Productivity growth of the cities of Jiangsu province, China: a Kaldorian approach",International Review of Applied Economics,32,4,,450,471,,4,10.1080/02692171.2017.1351529,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023767870&doi=10.1080%2f02692171.2017.1351529&partnerID=40&md5=ac6aa531cf46c33a1f96d2a7b09d2118,"This paper considers the determinants of economic growth of the cities of Jiangsu province, China, adopting a Kaldorian approach. It is found that there is a close correlation between the growth of non-industry and industry (Kaldor’s first law) that provides indirect evidence for the export-base theory. The paper discusses two competing explanations of the foundations of the Verdoorn law (Kaldor’s second law), which, in its simplest form, is the relationship between industrial productivity and output growth. It also considers the static–dynamic Verdoorn law paradox. This arises from the fact that estimating the Verdoorn law in log-levels often gives statistically insignificant estimates of the Verdoorn coefficient while the use of growth rates gives significant values of around one half. The results show that this does not occur when data for the cities are used. A plausible explanation for the paradox is that it results from spatial aggregation bias. It is also found that inter-province urban productivity disparities first increase, but subsequently decrease over the period considered. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",Scopus,2-s2.0-85023767870
English,Article,2014,Garcia X.,The value of rehabilitating urban rivers: the Yarqon River (Israel),Journal of Environmental Economics and Policy,3,3,,323,339,,4,10.1080/21606544.2014.923338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975631884&doi=10.1080%2f21606544.2014.923338&partnerID=40&md5=23983c8c8f12d983d076e84f90f31bde,"Rehabilitating the good ecological status of rivers in urban catchments can represent a wise decision since it can enhance the provision of valuable ecosystem services, such as aesthetic appreciation. The higher prices of houses located closer to rivers are a reflection of the willingness of households to pay for access to such service. The main objective of this study was to apply a hedonic pricing analysis to estimate the benefits generated by this ecosystem service due to the rehabilitation of the Yarqon River in Israel. During the last two decades, several projects and actions, such as the discharge of tertiary-quality effluents, have increased the quantity and quality of the river's water and improved its ecological state. Using a sample of 883 houses in the Tel-Aviv Metropolitan Area and selecting a mixed log-level functional form (R2 = 0.808), it was found that an increase of 1% in the distance to the Yarqon caused a 0.12% decrease in the price of a house. Finally, benefits are estimated and compared with the rehabilitation costs, showing that, even if no other ecosystem services are considered, the rehabilitation can prove to be beneficial to society. © 2014 Journal of Environmental Economics and Policy Ltd.",Scopus,2-s2.0-84975631884
Spanish,Article,2012,"Soto L., Valenzuela L., Lasserre J.P.","Effect of initial planting density in dynamic modulus of elasticity in standing trees and logs of 28 years old radiata pine plantation in sandy soil, Chile [Efecto de la densidad de plantación inicial en el módulo de elasticidad dinámico de árboles en pie y trozas de una plantación de pino radiata de 28 años, en la zona de renales, Chile]",Maderas: Ciencia y Tecnologia,14,2,,209,224,,6,10.4067/S0718-221X2012000200008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871453502&doi=10.4067%2fS0718-221X2012000200008&partnerID=40&md5=ec95582cd45cd847a26fffd9dd4aa97d,"The influence of initial planting density (DPI) on the dynamic modulus of elasticity (MOEd) was examinated at a 28 years old Pinus radiata D. Don spacing experiment with four treatments (2500, 1667, 833 and 625 stem·ha-1) growing on sandy soil, in the Biobío Region, Chile. The MOEd acoustic technology was determined using the method of time of flying (Tv) for standing trees and the resonance method (Res) for two logs of 5 m long, extracted from the tree base to the top of each tree. MOEd was not significantly infiuenced by DPI in high initial stocking treatments (2500, 1667 and 833 stem·ha-1), both standing trees and logs level. In contrast, the lowest initial stocking treatment (625 stem·ha-1) had the highest MOEd, significantly different in standing trees and the fi rst the log, and similar between the first and second log.",Scopus,2-s2.0-84871453502
English,Article,2009,"Benjamin J.G., Chui Y.H., Kershaw J.A.",Circular distribution of branches from plantation grown black spruce in Ontario,Northern Journal of Applied Forestry,26,1,,15,20,,3,10.1093/njaf/26.1.15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64249135706&doi=10.1093%2fnjaf%2f26.1.15&partnerID=40&md5=0b8328ef19526f98ba9c75b8e068dd50,"The literature is not consistent in descriptions related to branch location around a stem and, consequently, few models exist to predict distribution of branch azimuth. The objective of this study was to determine if branches in black spruce are uniformly distributed around the stem at the tree, log, and whorl levels with respect to branch size. Branch size limits were selected to reflect the largest branch per whorl and knot size limits were established by visual grading rules for 2 × 3 and 2 × 4 dimension lumber. Using Rayleigh's test of uniformity, branches are considered to be uniformly distributed around the stem for all branch size limits: between 40 and 80% at the tree level, over 70% at the log level, and virtually 100% at the whorl level. The findings of this study indicate that a simple random assignment (from a uniform distribution) of branches around the stem within each whorl is sufficient to properly describe branch location within black spruce tree growth models. © 2009 by the Society of American Foresters.",Scopus,2-s2.0-64249135706
English,Conference Paper,2019,"Yuan Y., Shi W., Liang B., Qin B.",An approach to cloud execution failure diagnosis based on exception logs in openstack,"IEEE International Conference on Cloud Computing, CLOUD",2019-July,,8814553,124,131,,6,10.1109/CLOUD.2019.00031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072338405&doi=10.1109%2fCLOUD.2019.00031&partnerID=40&md5=b510f5de0fbacd98527f7431ca023547,"Cloud is getting ubiquitous and scales up rapidly. It is critical to effectively detect and efficiently repair system anomalies for a robust cloud. Many efforts have been made to facilitate analysis of system problems with the readily-available and massive cloud logs. However, most tools can still not automatically recognize failures related to a specific cloud operating system task. To diagnose execution failures of a cloud, it is inevitable to monitor corresponding system tasks. In this paper, we propose a lightweight approach to identify cloud behaviors related to failed executions of the cloud operating system for failure diagnosis, by exploiting logs of ERROR logging level in a cloud. Instead of working on execution sequences extracted from logs for all system tasks, we focus on automated recognition of exception logs generated by a system task. These logs are critical snippets of execution traces for failure diagnosis of a cloud. In our work, exception logs are extracted and associated with the respective system task. Efforts can be reduced by comparing patterns of new error cloud behaviors with cloud behaviors met before. With experiments on OpenStack, a popular open source cloud operating system, we demonstrate that our work is effective and efficient for execution failure diagnosis of a cloud. Our approach can also be used as a complementary method for log-based troubleshooting tools concentrating on execution sequences. © 2019 IEEE.",Scopus,2-s2.0-85072338405
English,Article,2011,"Walsh R.P.D., Bidin K., Blake W.H., Chappell N.A., Clarke M.A., Douglas I., Ghazali R., Sayer A.M., Suhaimi J., Tych W., Annammala K.V.",Long-term responses of rainforest erosional systems at different spatial scales to selective logging and climatic change,Philosophical Transactions of the Royal Society B: Biological Sciences,366,1582,,3340,3353,,37,10.1098/rstb.2011.0054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054707985&doi=10.1098%2frstb.2011.0054&partnerID=40&md5=5d8e36e5efa206b1924b63cdaca275e9,"Long-term (21-30 years) erosional responses of rainforest terrain in the Upper Segama catchment, Sabah, to selective logging are assessed at slope, small and large catchment scales. In the 0.44 km2 Baru catchment, slope erosion measurements over 1990-2010 and sediment fingerprinting indicate that sediment sources 21 years after logging in 1989 are mainly road-linked, including fresh landslips and gullying of scars and toe deposits of 1994-1996 landslides. Analysis and modelling of 5-15 min stream-suspended sediment and discharge data demonstrate a reduction in stormsediment response between 1996 and 2009, but not yet to pre-logging levels. An unmixing model using bed-sediment geochemical data indicates that 49 per cent of the 216 t km-2 a-1 2009 sediment yield comes from 10 per cent of its area affected by road-linked landslides. Fallout 210Pb and 137Cs values from a lateral bench core indicate that sedimentation rates in the 721 km2 Upper Segama catchment less than doubled with initially highly selective, low-slope logging in the 1980s, but rose 7-13 times when steep terrain was logged in 1992-1993 and 1999-2000. The need to keep steeplands under forest is emphasized if landsliding associated with current and predicted rises in extreme rainstorm magnitude-frequency is to be reduced in scale. © 2011 The Royal Society.",Scopus,2-s2.0-80054707985
English,Article,1996,Plumptre A.J.,"Changes following 60 years of selective timber harvesting in the Budongo Forest Reserve, Uganda",Forest Ecology and Management,89,1-3,,101,113,,124,10.1016/S0378-1127(96)03854-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030301743&doi=10.1016%2fS0378-1127%2896%2903854-6&partnerID=40&md5=978ce2757e787d53cc20ae4ff0ab6071,"The controlled extraction of timber from the Budongo Forest Reserve began in the 1930s. From the start of the operations it was intended that the timber should be harvested on a sustainable yield basis and the first of several 10-year working plans for the forest was drawn up in 1933. This paper documents the volume of timber removed, the date of logging, date of arboricide treatment and volume of arboricide applied in most compartments in the forest. Four species of mahogany formed about 65% of the timber extracted and this remained almost constant over the years despite attempts to encourage the use of other species by the Uganda Forest Department. Forest type maps made from sets of aerial photographs taken in 1951 and 1990 showed an increase in 'mixed forest' at the expense of 'Cynometra forest'; one of the major aims of the arboricide treatment. Ordination of the basal areas of the common tree species showed that the geographical position of a compartment explained more of the variation in species distribution than the variation between adjacent logged and unlogged compartments. A more detailed inventory of all species of tree in eight compartments across the forest showed greater species richness in the west of the forest and greater species richness in logged compartments. Measures of forest structure showed that more than 50 years is required for the forest to recover to pre-logging levels.",Scopus,2-s2.0-0030301743
English,Article,2014,"Watt M.S., Trincado G.",Modelling between tree and longitudinal variation in green density within Pinus radiata: Implications for estimation of MOE by acoustic methods,New Zealand Journal of Forestry Science,44,1,16,,,10,7,10.1186/s40490-014-0016-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907221240&doi=10.1186%2fs40490-014-0016-5&partnerID=40&md5=0f1da5345a690679cc15f8a6e4b3acef,"Background: Instruments based on resonance are widely used in the forest industry to predict modulus of elasticity (MOE) and segregate logs of varying quality for different end uses for fast growing softwoods such as Pinus radiata D. Don. Predictions of MOE, made using resonance instruments, often assume a constant green density, ρg, of 1,000 kg m−3. However, little research has been done to test the robustness of this assumption. The objective of this research was to describe changes in predictive precision of MOE as ρg is increasingly well characterised.Methods: Longitudinal measurements of velocity, V, and ρg taken from eighty 17-year old unthinned P. radiata trees growing at two sites in Chile were used to calculate MOE. Predictions of MOE were then made by substituting measurements of ρg for values predicted by the following models (i) Model 1 - assuming a constant ρg of 1,000 kg m−3, (ii) Model 2 - using the mean tree ρg of 914 kg m−3, (iii) Model 3 - using a model with fixed effects to account for the mean longitudinal variation in ρg, (iv) Model 4 – inclusion of previous terms and random effects to account for tree level variation and (v) Model 5 – inclusion of previous effects (in model 4) and a random quadratic term. Differences in MOE determined from measurements of ρg and the five predictions of ρg were expressed as both a percentage difference, (D) and an absolute percentage difference (Da) to assess precision and bias.Results: At the tree level, values for mean D and Da (in brackets) were −9.9 (10.4)%, −0.459 (5.49)%, −0.262 (4.15)%, −0.045 (0.232)% and −0.0406 (0.189)%, for Models 1, 2, 3, 4 and 5, respectively. At the log level, considerable longitudinal bias in D was evident for Model 1 where over-prediction of MOE was greatest between relative heights of &gt;0.1 − 0.4, with D reaching maximum values of −33.8% between relative heights of &gt; 0.1 − 0.2.Conclusion: Assuming constant ρg can result in substantial error in estimates of MOE using acoustic instruments particularly when predictions are made at the log level. The mixed effects modelling approach described here demonstrates a useful method for characterising variation in ρg allowing more accurate estimates of MOE to be made using acoustic methods. © 2014, Watt and Trincado.",Scopus,2-s2.0-84907221240
English,Article,2011,"Banerjee R., Naessens J.M., Seferian E.G., Gajic O., Moriarty J.P., Johnson M.G., Meltzer D.O.",Economic implications of nighttime attending intensivist coverage in a medical intensive care unit,Critical Care Medicine,39,6,,1257,1262,,54,10.1097/CCM.0b013e31820ee1df,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957646225&doi=10.1097%2fCCM.0b013e31820ee1df&partnerID=40&md5=c673ccb11340acd2da6dcc483b29fae5,"Objective: Our objective was to assess the cost implications of changing the intensive care unit staffing model from on-demand presence to mandatory 24-hr in-house critical care specialist presence. Design: A pre-post comparison was undertaken among the prospectively assessed cohorts of patients admitted to our medical intensive care unit 1 yr before and 1 yr after the change. Our data were stratified by Acute Physiology and Chronic Health Evaluation III quartile and whether a patient was admitted during the day or at night. Costs were modeled using a generalized linear model with log-link and γ-distributed errors. Setting: A large academic center in the Midwest. Patients: All patients admitted to the adult medical intensive care unit on or after January 1, 2005 and discharged on or before December 31, 2006. Patients receiving care under both staffing models were excluded. Intervention: Changing the intensive care unit staffing model from on-demand presence to mandatory 24-hr in-house critical care specialist presence. Measurements and Main Results: Total cost estimates of hospitalization were calculated for each patient starting from the day of intensive care unit admission to the day of hospital discharge. Adjusted mean total cost estimates were 61% lower in the post period relative to the pre period for patients admitted during night hours (7 pm to 7 am) who were in the highest Acute Physiology and Chronic Health Evaluation III quartile. No significant differences were seen at other severity levels. The unadjusted intensive care unit length of stay fell in the post period relative to the pre period (3.5 vs. 4.8) with no change in non-intensive care unit length of stay. Conclusions: We find that 24-hr intensive care unit intensivist staffing reduces lengths of stay and cost estimates for the sickest patients admitted at night. The costs of introducing such a staffing model need to be weighed against the potential total savings generated for such patients in smaller intensive care units, especially ones that predominantly care for lower-acuity patients. Copyright © 2011 by the Society of Critical Care Medicine and Lippincott Williams & Wilkins.",Scopus,2-s2.0-79957646225
English,Article,2021,"Korchenko A., Breslavskyi V., Yevseiev S., Zhumangalieva N., Zvarych A., Kazmirchuk S., Kurchenko O., Laptiev O., Sievierinov O., Tkachuk S.",Development of a Method for Constructing Linguistic Standards for Multi-Criteria Assessment of Honeypot Efficiency,Eastern-European Journal of Enterprise Technologies,1,2(109),,14,23,,,10.15587/1729-4061.2021.225346,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105688889&doi=10.15587%2f1729-4061.2021.225346&partnerID=40&md5=59ffc5a2130823ba2e7ccecfdc28cea1,"One of the pressing areas that is developing in the field of information security is associated with the use of Honeypots (virtual decoys, online traps), and the selection of criteria for determining the most effective Honeypots and their further classification is an urgent task. The main products that implement virtual decoy technologies are presented. They are often used to study the behavior, approaches and methods that an unauthorized party uses to gain unauthorized access to information system resources. Online hooks can simulate any resource, but more often they look like real production servers and workstations. A number of fairly effective developments are known that are used to solve the problems of detecting attacks on information system resources, which are based on the apparatus of fuzzy sets. They showed the effectiveness of the appropriate mathematical apparatus, the use of which, for example, to formalize the approach to the formation of a set of reference values that will improve the process of determining the most effective Honeypots. For this purpose, many characteristics have been formed (installation and configuration process, usage and support process, data collection, logging level, simulation level, interaction level) that determine the properties of online traps. These characteristics became the basis for developing a method for the formation of standards of linguistic variables for further selection of the most effective Honeypots. The method is based on the formation of a Honeypots set, subsets of characteristics and identifier values of linguistic estimates of the Honeypot characteristics, a base and derived frequency matrix, as well as on the construction of fuzzy terms and reference fuzzy numbers with their visualization. This will allow classifying and selecting the most effective virtual baits in the future © 2021, A. Korchenko, V. Breslavskyi, S. Yevseiev, N. Zhumangalieva, A. Zvarych, S. Kazmirchuk, O. Kurchenko, O. Laptiev, O. Sievierinov, S. Tkachuk This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0)",Scopus,2-s2.0-85105688889
Japanese,Article,2016,Yamada T.,Long-term effects of selective logging on tropical forest ecosystems — A case study of the Pasoh forest reserve,Japanese Journal of Ecology,66,2,,275,282,,1,10.18960/seitai.66.2_275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021301172&doi=10.18960%2fseitai.66.2_275&partnerID=40&md5=01a9604b8bc484d608e70609e1480a54,"The degradation of tropical forests is a major environmental issue related to global warming and biodiversity conservation, and therefore deserves the attention of various international and non-governmental organizations. Commercial timber extraction is believed to be the main driver of degradation in tropical forests. Currently, production forests cover more area than protected forests. In addition, most of the remaining forests are designated for timber production by national forest services. Therefore, the forests remaining after selective logging will play important roles in maintaining and upgrading ecosystem services such as maintenance of carbon stocks and biodiversity in degraded tropical forests. Degraded forest structure and species composition caused by selective logging may be recovered through secondary forest succession. However, it remains unclear how long it takes tropical forests to recover from selective logging. This paper addresses this question from the viewpoints of forest structure, forest illumination, demographic parameters, biomass and tree biodiversity, using data derived from the Pasoh Forest Reserve, Malaysia where selective logging was performed in the 1950s. After selective logging the forest structure, forest illumination, and demographic parameters of this forest were still statistically distinguishable from unlogged forests even 50 years after a logging operation. Forest biomass had almost recovered to the pre-logging level 50 years after the logging operation concluded. However, after logging the biodiversity of the logged forest was quite different from that of the unlogged forest, suggesting that it will take quite a long time—more than 60 years—for biodiversity to recover after logging. © 2016, Tohoku University. All rights reserved.",Scopus,2-s2.0-85021301172
English,Article,2010,"Bren L., Lane P., Hepworth G.",Longer-term water use of native eucalyptus forest after logging and regeneration: The Coranderrk experiment,Journal of Hydrology,384,1-2,,52,64,,33,10.1016/j.jhydrol.2010.01.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77649187524&doi=10.1016%2fj.jhydrol.2010.01.007&partnerID=40&md5=b14503c3b68f5063f9d0e89a23e4d513,"The Coranderrk Project, located 70 km east of Melbourne, is Australia's longest-running paired catchment study. Three small catchments (""Slip Creek"", ""Blue Jacket Creek"", and ""Picaninny Creek"") originally carrying old growth mountain ash and mixed species have been gauged since 1958. In 1971/72 Picaninny Creek catchment was substantially clear-felled and regenerated, predominantly with mountain ash (Eucalyptus regnans). The annual flow increased for the next 3 years (relative to the control), reaching a peak of almost 300 mm increase relative to the control catchment. Flow then declined; 8 years after the logging the annual flow had returned to pre-logging levels. Annual flow then continued to decline with a maximum reduction of around 200 mm per annum. Thirty-four years after logging the flow was still below the pre-treatment flow and showing no sign of recovery, although there were year to year variations associated with rainfall and drought. In 1972/73 Blue Jacket Creek had a ""sawmiller selection"" cutting in which approximately 50% of the basal area was removed. The response was a similar but muted version of the Picaninny Creek response. A statistical analysis used 1 year of the pre-treatment period to establish error limits. This showed that in both cases there was a statistically significant increase in flow followed by a statistically significant decrease in flow relative to old growth. Measurement of two of the catchments is continuing. © 2010 Elsevier B.V. All rights reserved.",Scopus,2-s2.0-77649187524
English,Article,2006,"Bleher B., Uster D., Bergsdorf T.","Assessment of threat status and management effectiveness in Kakamega Forest, Kenya",Biodiversity and Conservation,15,4,,1159,1177,,65,10.1007/s10531-004-3509-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745442838&doi=10.1007%2fs10531-004-3509-3&partnerID=40&md5=84f9b50e982cf48f4351f5c13b7f6150,"To counteract an increasing biodiversity decline, parks and protected areas have been established worldwide. However, many parks lack adequate management to address environmental degradation. To improve management strategies simple tools are needed for an assessment of human impact and management effectiveness of protected areas. This study quantifies the current threats in the heavily fragmented and degraded tropical rainforest of Kakamega, western Kenya. We recorded seven disturbance parameters at 22 sites in differently managed and protected areas of Kakamega Forest. Our data indicate a high level of human impact throughout the forest with illegal logging being most widespread. Furthermore, logging levels appear to reflect management history and effectiveness. From 1933 to 1986, Kakamega Forest was under management by the Forest Department and the number of trees logged more than 20 years ago was equally high at all sites. Since 1986, management of Kakamega Forest has been under two different organizations, i.e. Forest Department and Kenya Wildlife Service. The number of trees logged illegally in the last 20 years was significantly lower at sites managed by the Kenya Wildlife Service. Finally, logging was lower within highly protected National and Nature Reserves as compared to high logging within the less protected Forest Reserves. Reflecting management effectiveness as well as protection status in Kakamega Forest, logging might therefore provide a valuable quantitative indicator for human disturbance and thus an important tool for conservation managers. Logging might be a valuable indicator for other protected areas, too, however, other human impact such as e.g. hunting might also prove to be a potential indicator. © Springer 2006.",Scopus,2-s2.0-33745442838
English,Conference Paper,2020,"Bharkad V.S., Chavan M.K.",Optimizing Root Cause Analysis Time Using Smart Logging Framework for Unix and GNU/Linux Based Operating System,Advances in Intelligent Systems and Computing,1077,,,519,528,,,10.1007/978-981-15-0936-0_55,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081702795&doi=10.1007%2f978-981-15-0936-0_55&partnerID=40&md5=00ef124ef1623cfa31650a1ae7683291,"The computer activity records are used for statistical purposes, backup, recovery, and root cause analysis of failure on application. These records are referred as a log. The log files are written for recording incoming dialogs, debug, error, status of an application and certain transaction details, by the operating system or other control program. The logs generated by an application that can be referred by user that may be helpful in the event of failure. For example, in a file transfer, FTP program generates a log file consist of date, time, source and destination, etc. In this number of logs generated by the application uses too much disk space. If the logging is tuned down (e.g., by lowering the log level) then the disk space usage is less, but then not enough information is available for debugging issue. To address this problem we proposed a Smart Logging Framework. The Smart Logging Framework provides the feature such as In-memory logging, In-memory packet capturing and Zoom-in log viewer. © 2020, Springer Nature Singapore Pte Ltd.",Scopus,2-s2.0-85081702795
English,Article,2016,"Aksu F., Uran H., Dülger Altiner D., Sandikçi Altunatmaz S.",Effects of different packaging techniques on the microbiological and physicochemical properties of coated pumpkin slices,Food Science and Technology,36,3,,549,554,,3,10.1590/1678-457X.00432,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010931898&doi=10.1590%2f1678-457X.00432&partnerID=40&md5=5a98a5d1b3561b967f85feb3ab42fc01,"In this study the effects of zein film coating along with benzoic acid on the quality of sliced pumpkin samples, which were packaged with different techniques were investigated. The samples were allocated into different groups and were treated with different processes. Following processing, the samples were stored at +4 °C for twenty days. Physicochemical and microbiological analyses were carried out on the samples once every five days during the storage period. According to color analysis, the L* value was observed to have significantly decreased in the processed and packaged samples in comparison with the control group. Besides, a* and b* values increased in all groups. It was determined that zein film alone did not exhibit the expected effectiveness against moisture loss in the samples. According to the results of microbiological analysis, a final decrease at approximately 1.00 log level was determined in total count of mesophilic aerobic bacteria (TMAB) in the group which was vacuum packaged in PVDC with zein coating when compared with the initial TMAB. Furthermore, no molding occurred in zein-coated group on the last day of the storage period, while massive mold growth was noted in the group which was packaged without any pretreatment procedure. © 2016, Sociedade Brasileira de Ciencia e Tecnologia de Alimentos, SBCTA. All rights reserved.",Scopus,2-s2.0-85010931898
English,Article,2016,"Bobuş-Alkaya G., Eser E., İbrahim Ekiz H.",Evaluation of different detachment methods for the bacterial recovery from parsley surface,"Journal of Food, Agriculture and Environment",14,2,,34,37,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966587119&partnerID=40&md5=7073d78581ffaa42a97e8cec362edc6c,"Reliable quantification of microbial growth in food samples has typically been difficult primarily due to the complicacy of food matrices. Therefore, initial sampling and resuspension procedures become critical to the subsequent determination of microbial load. This paper presents a comparison of bacteria recovery methods. Vortexing, homogenization with ultraturrax and stomaching methods were used to remove cells from the parsley surface. For this purpose three types of parsley (the small, curly and large leaf structure) were inoculated with non-pathogenic Escherichia coli ATCC 25922 strain by the initial levels of 9.5, 6 and 3 log cfu/ml bacterial culture. Detachment methods were applied and all methods yielded the approximately equal number of bacteria for each inoculation level. Means of recovered bacteria number were found as 8.1, 6.1 and 2.6 log cfu/g, respectively. It was concluded that E. coli was recovered in large amounts using all methods. Additionally, total number of aerobic bacteria and total coliform bacteria counts of parsley samples which were purchased from retail markets were determined for sixteen different samples and found as approximately 5.3 cfu/g log level. © 2016, World Food Ltd. and WFL Publishers. All Rights Reserved.",Scopus,2-s2.0-84966587119
English,Conference Paper,2012,"Bag A., Rodi S., Pillai K.",Performance testing of super fast application,CMG 2012 International Conference,,,,,,9,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883061195&partnerID=40&md5=f8a501c27003940313f2c0d709a76a38,"Performance Testing (both unit and system) is useful to understand application and sub-component(s) performance characteristics. System/Application performance testing gives insight into the overall performance of the application and is useful to debug performance issues before and during production phase. For High Performance Computing (HPC) applications, unit performance testing/proof-of-concept (POC) is very important to help choose from the different options (technology, design, data structures etc). For HPC applications, doing system performance testing becomes very complex as simulating very high input throughputs may necessitate construction of custom load injectors. Also, in HPC applications with complex workflows, simulating proper transaction mix and maintaining stable backend database size becomes complex. Monitoring different data points (latency, throughput) and debugging (log levels) is also tricky because of the overheads those incur. Unit performance testing of critical components involves complex workload and throughput modeling to determine achievable performance. This paper outlines two examples, one for POC through unit performance testing of a very high throughput application at architecture and design phase. The other is of system performance testing of a very high throughput application for SLA certification and tuning purpose.",Scopus,2-s2.0-84883061195
English,Conference Paper,2019,"Al-Bulushi N., Kraishan G., Hursan G.","Capillary pressure corrections, quality control and curve fitting workflow","International Petroleum Technology Conference 2019, IPTC 2019",,,,,,,1,10.2523/19514-ms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085851895&doi=10.2523%2f19514-ms&partnerID=40&md5=05a6271e2e82efaf90679aa40e3f4bd1,"Capillary pressure is a crucial step in reservoir properties definition and distribution during static and dynamic modelling. It is a key input into saturation height modelling (SHM) process, understanding the fluid distribution and into reservoir rock typing process. Capillary pressure models provide an insight into field dynamic for the identification of swept zones and provide another calibration besides the log calculated saturation. Capillary pressure curve tends to be more complex in carbonates in comparison to sandstone reservoirs because of post deposition processes that impact the rock flow properties, hence complex pore throat size distribution (uni-modal, bi-modal or tri-modal). Therefore, accurate determination of this property is the cornerstone in the reservoir characterization process. Capillary pressure can be obtained using several experimental techniques, such as mercury injection (MICP), centrifuge (CF) and porous plate (PP). Each method has its own inherited advantages and disadvantages. The MICP method tends to be faster, cheaper and provides a full spectrum of pore throat size of a plug. Whereas, the PP method can be carried out at reservoir conditions with minimum required corrections. In this paper, a detailed workflow for quality control capillary pressure is discussed. The workflow is subdivided into three main parts: Instrumental and experimental level, core measurement level and logs level. Experimental level starts with proper designing the actual procedure of the capillary pressure experiment. Parameters such as pore volume, bulk volume and grain density are investigated at core measurement level. In geological-petrography montage, all petrography data; X-Ray Diffraction (XRD), Scanning Electron Microscope (SEM), thin section and computed tomography scan (CT) are used along with the capillary pressure curve for assessment. Comparing various methodologies of experimental technique carried out on twin plugs, if exist, are also investigated. The capillary pressure that passes the previous QC steps is used as input into saturation-point comparison as a logs level QC. The saturation calculated from capillary pressure is compared to log-derived water saturation eliminating any issues with porosity and permeability of the trims and provides insight to the uncertainty level in the model. As an additional step, the MICP measurements are fitted with bi-modal Gaussian basis functions with two practical benefits. First, the quality of this fitting is a useful indicator for the evaluation of pore structure complexity and the identification erroneous measurements. Second, the fitting parameters are useful inputs for geological interpretation, rock typing and SHM. This rapid and automated workflow is a useful tool for screening, processing and integration of large-scale capillary pressure data sets, a key step in integrated reservoir description, characterization and modelling. © 2019, International Petroleum Technology Conference",Scopus,2-s2.0-85085851895
English,Conference Paper,2011,"Rao X., Wang H., Shi D., Chen Z., Cai H., Zhou Q., Sun T.",Identifying faults in large-scale distributed systems by filtering noisy error logs,Proceedings of the International Conference on Dependable Systems and Networks,,,5958800,140,145,,9,10.1109/DSNW.2011.5958800,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052147473&doi=10.1109%2fDSNW.2011.5958800&partnerID=40&md5=324fccd612bde4443b8a49d2bb521090,"Extracting fault features with the error logs of fault injection tests has been widely studied in the area of large scale distributed systems for decades. However, the process of extracting features is severely affected by a large amount of noisy logs. While the existing work tries to solve the problem by compressing logs in temporal and spatial views or removing the semantic redundancy between logs, they fail to consider the co-existence of other noisy faults that generate error logs instead of injected faults, for example, random hardware faults, unexpected bugs of softwares, system configuration faults or the error rank of a log severity. During a fault feature extraction process, those noisy faults generate error logs that are not related to a target fault, and will strongly mislead the resulted fault features. We call an error log that is not related to a target fault a noisy error log. To filter out noisy error logs, we present a similarity-based error log filtering method SBF, which consists of three integrated steps: (1) model error logs into time series and use haar wavelet transform to get the approximate time series; (2) divide the approximate time series into sub time series by valleys; (3) identify noisy error logs by comparing the similarity between the sub time series of target error logs and the template of noisy error logs. We apply our log filtering method in an enterprise cloud system and show its effectiveness. Compared with the existing work, we successfully filter out noisy error logs and increase the precision and the recall rate of fault feature extraction.1 © 2011 IEEE.",Scopus,2-s2.0-80052147473
English,Article,2020,"Sicks B., Hönes K., Spellerberg B., Hessling M.",Blue LEDs in endotracheal tubes may prevent ventilator-associated pneumonia,"Photobiomodulation, Photomedicine, and Laser Surgery",38,9,,571,576,,5,10.1089/photob.2020.4842,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092292517&doi=10.1089%2fphotob.2020.4842&partnerID=40&md5=d4c59629b982a77efa2ff2f916d99879,"Background: Ventilator-associated pneumonia (VAP) is one of the most common nosocomial infections in intensive care units, which not only leads to prolonged hospital stays and higher treatment costs but is also associated with high mortality. Objective: An approach to equip endotracheal tubes (ETT) with blue LEDs for photoinactivation of bacterial pathogens in the trachea is suggested and tested on a Staphylococcus strain. Materials and methods: With 48 small 450 nm LEDs, integrated in a conventional ETT, a homogenous irradiation intensity of up to 13.4 mW/cm2 on the outer endotracheal surface is achieved and used for the irradiation of a Staphylococcus carnosus solution in an experimental tracheal model. Results: Applying LED currents of 5 and 10 mA, the bacterial concentration in the test solution is successfully reduced by three log levels within 9 and 6 h, respectively. Conclusions: From a technical and medical point of view, the approach of integrating blue LEDs in an ETT is very promising and should be further investigated, since it may prevent VAP. While equipping an ETT with LEDs produces additional costs, cutting the rate of VAP can also bring a major financial relief for health care systems. © 2020 Mary Ann Liebert Inc.. All rights reserved.",Scopus,2-s2.0-85092292517
English,Article,2020,"Muthu Pandi K., Somasundaram K.",Virtual infrastructure provisioning virtual machine with machine learning prediction in green cloud computing,Journal of Green Engineering,10,8,,4334,4352,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091597411&partnerID=40&md5=49996417f1b5d648a644662201da215f,"Cloud computing is one of the emerging technology in the world it refer different technologies, services concepts and architectures. Cloud has provided everything as a service to end users, medium and large scale enterprises across globally. The datacenters contribute major role in the cloud, it has different architecture, service provided by cloud providers. The datacenters level required lots of optimization, it will helps to gain the less energy consumption, less pollution and carbon emission, provide more powerful performance to consumer with less cost, QoS within SLA, innovation. To achieve these datacenter levels required continuously monitoring, analysis and take decisions on the log level, monitoring metrics, thresholds. The datacenters resource objects using training and prediction on usage during provisioning and server consolidation. The application workflow request and response experiment analyses. There selections of energy selection on datacenter. The agent based monitoring, agent less script based monitoring, automation, artificial intelligence, neural network. This paper mainly concentrated analysis metrics, thresholds on cloud, capacity analysis, application workflow and Resource such as CPU, RAM utilization and prediction using Machine Learning techniques such as LR, RantomTree, RandomForest and 10 folds cross validation in cloud. The data center power source, architecture selection and usage of resource in usage optimum will helps us to reduce carbon emission and green cloud computing. © 2020 Alpha Publishers. All rights reserved.",Scopus,2-s2.0-85091597411
English,Article,2014,"Williams R.T., Swanlund A., Miller S., Konstantopoulos S., Eno J., van der Ploeg A., Meyers C.",Measuring Instructional Differentiation in a Large-Scale Experiment,Educational and Psychological Measurement,74,2,,263,279,,4,10.1177/0013164413507724,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894337139&doi=10.1177%2f0013164413507724&partnerID=40&md5=c77bf952619bdabb6527df1aa5946d4b,"This study operationalizes four measures of instructional differentiation: one for Grade 2 English language arts (ELA), one for Grade 2 mathematics, one for Grade 5 ELA, and one for Grade 5 mathematics. Our study evaluates their measurement properties of each measure in a large field experiment: the Indiana Diagnostic Assessment Tools Study, which included two consecutive cluster randomized trials (CRTs) of the effects of interim assessments on student achievement. Each log was designed to measure instructional practices as they were implemented for eight randomly selected students in the participating teachers' classrooms. A total of 592 teachers from 127 schools took part in this study. Logs were administered 16 times in each experiment. Item responses to the logs were scaled using the Rasch model and reliability estimates for the differentiation measures were evaluated at the log level (observations within teachers), the teacher level, and the school level. Estimated reliability was above.70 for each of the log- and teacher-level measures. At the school level, reliability estimates were lower for Grade 5 ELA and mathematics. The variance between teachers and schools on the scaled differentiation measures was substantially less than within-teacher variation. These results provide preliminary evidence that teacher instructional logs may provide useful measures of instructional differentiation in elementary grades at multiple levels of aggregation. © The Author(s) 2013.",Scopus,2-s2.0-84894337139
English,Article,2013,"Nelson H.M., Singh R.K., Avula R.Y., Toledo R.T.",Flux behavior and quality of effluent from a poultry processing plant treated by membrane bioreactor,International Journal of Food Engineering,10,1,,51,57,,1,10.1515/ijfe-2012-0008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896779752&doi=10.1515%2fijfe-2012-0008&partnerID=40&md5=f1f8c730f5a9d3828af9a3096d8bbc12,"Membrane bioreactor (MBR) provided with spiral wound modules of polyacrylonitrile ultrafiltration membranes was used to treat the wastewater obtained from primary and secondary processing operations of a poultry plant. The membrane bioreactor consisted of 3 tanks; an aerobic bioreactor, anoxic settling tank, and a third tank from which a permeate was drawn across ultrafiltration membranes for final discharge to a municipal sewer or for reuse in the processing of raw product. The Cleaning and backflush schedules were conducted to determine the best regimen for maximum permeate flux and for their effect on retention times in each of the biological treatment tanks. Continuous operation of MBR for 24 h period with no backflushing resulted in flux decay that led to a substantial change in retention times. The best operating cycle was found to be 1 h filtration followed by 120 s backflush or 20 min filtration followed by 20 s backflush. Significant reduction in particle size, COD and BOD (>90%) and reduction of microbial load by 4 - log levels in MBR effluent made it fit for reuse. © 2014 by Walter de Gruyter Berlin / Boston.",Scopus,2-s2.0-84896779752
English,Article,2012,"Haberbeck L.U., Alberto da Silva Riehl C., de Cássia Martins Salomão B., Falcão de Aragão G.M.",Bacillus coagulans spore inactivation through the application of oregano essential oil and heat,LWT - Food Science and Technology,46,1,,267,273,,26,10.1016/j.lwt.2011.09.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83955161693&doi=10.1016%2fj.lwt.2011.09.021&partnerID=40&md5=7364113cac1b47a1d868f9516939d8e5,"The present study evaluated the effect of thermal (temperature) and thermochemical (temperature. +. oregano essential oil (EO)) inactivation of Bacillus coagulans spores in Nutrient Broth (NB) adjusted at 4°Brix and pH of 4.2. Thermal treatments included temperatures between 95 and 103°C. For thermochemical treatment, first temperature was fixed at 100°C and EO concentration varied between 250 and 1000μg/g. Thermochemical treatment significantly reduced the time needed to reduce a 6. log level of spores compared to thermal treatment, for example around 1.4. min with 400μg/g of EO. Then, EO concentration was fixed at 400μg/g and temperature varied between 90 and 100°C. Although the first results showed a faster spore reduction with 500μg/g, the fixed EO concentration was 400μg/g, since it represents a lower organoleptic impact and also a significant reduction in the spores' resistance. For instance, at 97°C and 400μg/g, about 4.3. min was needed to reduce the spores in 6. log, without the EO this time was 5.0. min. These findings indicate that oregano EO may be used to render B. coagulans spores more susceptible to the lethal effect of heat. © 2011 Elsevier Ltd.",Scopus,2-s2.0-83955161693
English,Article,2011,"Øvrum A., Vestøl G.I., Høibø O.A.","Modelling the effects of timber length, stand- and tree properties on grade yield of picea abies timber",Scandinavian Journal of Forest Research,26,2,,99,109,,5,10.1080/02827581.2010.534110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951837834&doi=10.1080%2f02827581.2010.534110&partnerID=40&md5=942f71e94e93b6713f619584af4f55df,"When the grade is determined by the worst part of a board, the grade yield will decrease with increasing timber length. This length effect varies due to longitudinal variation in the grading features and their appearance on the sawn surface. In this study models identifying the length effect's dependence on site, stand, tree and log-level characteristics have been developed. The study comprised boards from 160 Norway spruce trees (Picea abies (L.) Karst.) sampled from six sites in Norway which were selected based on variation in the occurrence of ramicorn branches, forked trees and sinuosity of stems. The boards were visually graded according to appearance by Nordic Timber and strength was graded by INSTA 142. The negative effect of increasing length on grade yield was strongest at stump level and decreased upwards in the trees. The biggest trees within a stand were most affected by an increase in length. In addition to timber length, position of the board within the tree was the most important factor influencing grade yield. The variation in grade yield within stands was greater than between stands for this material. The models predicting grade probabilities seemed to fit within a 10% margin. © 2011 Taylor & Francis.",Scopus,2-s2.0-79951837834
English,Article,2008,Fair R.C.,Testing price equations,European Economic Review,52,8,,1424,1437,,7,10.1016/j.euroecorev.2008.06.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55949109308&doi=10.1016%2fj.euroecorev.2008.06.004&partnerID=40&md5=ec5962efc5f9bb12b4e9037439f86316,"How inflation and unemployment are related in both the short run and long run is perhaps the key question in macroeconomics. This paper tests various price equations using quarterly U.S. data from 1952 to the present. Issues treated are the following. (1) Estimating price and wage equations in which wages affect prices and vice versa versus estimating ""reduced-form"" price equations with no wage explanatory variables. (2) Estimating price equations in (log) level terms, first difference (i.e., inflation) terms, and second difference (i.e., change in inflation) terms. (3) The treatment of expectations. (4) The choice and functional form of the demand variable. (5) The choice of the cost-shock variable. The results suggest that the best specification is a price equation in level terms imbedded in a price-wage model, where the wage equation is also in level terms. The best cost-shock variable is the import price deflator, and the best demand variable is the unemployment rate. There is some evidence of a nonlinear effect of the unemployment rate on the price level at low values of the unemployment rate. Many of the results in this paper are contrary to common views in the literature, but the empirical support for them is strong. © 2007 Elsevier B.V. All rights reserved.",Scopus,2-s2.0-55949109308
English,Article,2008,"Prasad M.M., Seenayya G.",The impact of salinicoccus roseus and heat treatment of salt on the shelf life of cured sciaenids (johnius dussumieri dussumieri's croaker),Journal of Aquatic Food Product Technology,17,3,,253,265,,2,10.1080/10498850802183372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650490687&doi=10.1080%2f10498850802183372&partnerID=40&md5=cf274f89783f02377c1be47c2a402cf4,"Salinicoccus roseus is the most commonly occurring bacterium in salt-cured fish having red discoloration. In the present work, sterile crystalline salt and semiground salt were inoculated with 6 logs level of this bacterium. Half of each of the salt batches was heat-treated at 80°C for 30 min in order to inactivate the inoculated bacteria, while the remaining halves were used as positive controls. The four salt qualities were used in curing of sciaenids (Johnius dussumieri or Dussumieri's croaker). Red discoloration, bacteriological, physical, and biochemical quality parameters were monitored at quarterly intervals for 1 year. The control sciaenids were discarded at the end of 3 mo due to complete red discoloration. The heat-treated crystalline and semiground salt-cured fish were free from red discoloration for 6 and 9 months of storage, respectively. The quality degradation, measured as increases in total volatile nitrogen (TVN) and peroxide values (PV), was also considerably slower in fish cured with heattreated salt than in the control. Furthermore, the development of TVN and PV values was significantly slower in fish cured with heat-treated semiground salt than in fish cured with heat-treated crude salt. The present study revealed that using heat-treated solar salt to cure fish enhances shelf life 6 to 9 months longer than that of the control.",Scopus,2-s2.0-67650490687
English,Article,2006,Marx G.,Simulating fibrin clotting time,Medical and Biological Engineering and Computing,44,1-2,,79,85,,6,10.1007/s11517-005-0007-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745448878&doi=10.1007%2fs11517-005-0007-z&partnerID=40&md5=d553059384dc9a148db46348f1033a6c,"The clotting time (CT) of fibrinogen mixed with thrombin decreased, then increased with increasing fibrinogen levels. By contrast, log CT decreased monotonically with respect to the log level of activating enzyme (thrombin or reptilase). Here, the CT was determined over a large range of fibrinogen concentration (to 100 mg ml-1) at a fixed level of enzyme. A new parameter, [Fib]min, the minimal fibrinogen concentration required for thrombin or reptilase-instigated phase change (coagulation), was determined as [Fib]min =0.2±0.05 μM fibrinogen. A dynamic simulation program (Stella) was employed to organize simulations based on simple and complex coagulation mechanisms, which generated CT values. The successful simulation aimed at forming [Fib]minand ""recognized"" the binding of unreacted fibrinogen with intermediate fibrin protofibrils. The ""virtual data"" mimicked the biphasic experimental CT values over a wide range of concentrations. Fibrinogen appeared to act in three modalities: as a thrombin substrate; as a precursor of fibrin; and as a competitor for fibrin protofibrils. The optimized simulation may provide a basis for predicting CT in more complex systems, such as pathological plasmas or whole blood or at high concentrations encountered with fibrin sealant. © International Federation for Medical and Biological Engineering 2006.",Scopus,2-s2.0-33745448878
English,Article,2000,"Kibblewhite R.P., Riddell M.J.C.",Wood and kraft fibre property variation within and among nine trees of Eucalyptus nitens,Appita Journal,53,3,,237,244,,15,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003012318&partnerID=40&md5=fc5b0aad3b4f4e89415b0176e6889203,"Understanding the distributions of wood and fibre properties among logs within trees is of importance to the wood processor since it will allow selective log utilisation to maximise product quality and minimise processing costs. Such advantages can be at both the solidwood and reconstituted product levels. Furthermore, understanding of the levels of variation among both trees and logs will allow breeding programs/strategies and rotation ages etc. to be directed to product requirements at the log level. For nine 15-year-old E. nitens trees, the wood and kraft fibre property variation is extremely high among trees and among the four or five 5.5 m logs of each tree. Some critical property distribution trends within trees are: • Chip basic density, xylose and ash contents increase linearly with increasing height (log number) from the ground. • Chip lignin increases, and glucose content decreases, non-linearly from log 2 to the toplog, but mainly show reversed trends from log 1 to log 2. Hence, kraft pulp yields can be expected to be particularly low for logs 4 and 5. • Kraft fibre length, perimeter and coarseness initially increase from log 1 to log 2 but then decrease markedly thereafter. Hence, numbers of fibres/g are high, and handsheet bulk values particularly low for kraft pulps made from logs 4 and 5.",Scopus,2-s2.0-0003012318
English,Article,2000,"Lipp M.D.W., Jaehnichen G., Golecki N., Fecht G., Reichl R., Heeg P.","Microbiological, microstructure, and material science examinations of reprocessed Combitubes® after multiple reuse",Anesthesia and Analgesia,91,3,,693,697,,12,10.1213/00000539-200009000-00037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033863371&doi=10.1213%2f00000539-200009000-00037&partnerID=40&md5=eeae1b27833be24f552bd822cee8a0af,"Reprocessing (repeated cleaning, disinfection, and sterilization) and reuse of single-use medical devices has been performed safely with some devices. The aim of our study was to analyze whether reprocessing of the Combitubes® (Kendall-Sheridan, Argyll, NY) airway device, used for emergency endotracheal intubation and difficult airway management, is possible and can be performed appropriately and safely. Microbiological, microstructure, and material science examinations were performed with unused, as well as multiple re-used and reprocessed Combitubes®. The reprocessing procedure consisted of a cleaning, a disinfection, a final inspection, and a sterilization. Microbiological examinations of multiple reused and reprocessed Combitubes® found no test organisms in quantitative cultures. A microbial reduction between four and five log levels compared with nonreprocessed tubes was found. Microstructure analysis for the examination of topographical alterations and changes in the chemical composition of the surface demonstrated nonsignificant alterations between new and reprocessed medical devices. In material science examinations, cuff burst pressures were not different between unused and multiple reprocessed Combitubes®. The results of all examinations proved that the decontamination process is adequately effective, and that no significant superficial alterations are generated by the multiple reuse and reprocessing of the Combitubes®. To assure uniformly good results, a quality management system must be established and only validated methods should be used.",Scopus,2-s2.0-0033863371
English,Article,2015,"Burivalova Z., Lee T.M., Giam X., Sekercioglu Ç.H., Wilcove D.S., Koh L.P.",Avian responses to selective logging shaped by species traits and logging practices,Proceedings of the Royal Society B: Biological Sciences,282,1808,20150164,,,8,47,10.1098/rspb.2015.0164,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929997046&doi=10.1098%2frspb.2015.0164&partnerID=40&md5=670973e29c94fc46720935ecf29d7587,"Selective logging is one of the most common forms of forest use in the tropics.Although the effects of selective logging on biodiversity have been widely studied, there is little agreement on the relationship between life-history traits and tolerance to logging. In this study, we assessed how species traits and logging practices combine to determine species responses to selective logging, based on over 4000 observations of the responses of nearly 1000 bird species to selective logging across the tropics. Our analysis shows that species traits, such as feeding group and body mass, and logging practices, such as time since logging and logging intensity, interact to influence a species’ response to logging. Frugivores and insectivores were most adversely affected by logging and declined further with increasing logging intensity. Nectarivores and granivores responded positively to selective logging for the first two decades, after which their abundances decrease below pre-logging levels. Larger species of omnivores and granivores responded more positively to selective logging than smaller species from either feeding group, whereas this effect of body size was reversed for carnivores, herbivores, frugivores and insectivores. Most importantly, species most negatively impacted by selective logging had not recovered approximately 40 years after logging cessation. We conclude that selective timber harvest has the potential to cause large and long-lasting changes in avian biodiversity.However, our results suggest that the impacts can bemitigated to a certain extent through specific forest management strategies such as lengthening the rotation cycle and implementing reduced impact logging. © 2015 The Author(s) Published by the Royal Society. All rights reserved.",Scopus,2-s2.0-84929997046
English,Article,2002,"Burrows N.D., Ward B., Cranfield R.",Short-term impacts of logging on understorey vegetation in a jarrah forest,Australian Forestry,65,1,,47,58,,13,10.1080/00049158.2002.10674852,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036507611&doi=10.1080%2f00049158.2002.10674852&partnerID=40&md5=ce58307d9aaee90136464e98723260db,"In 1985, new silvicultural treatments were implemented in jarrah (Eucalyptus marginata) forests available for wood production. As part of a scientific investigation into the ecological impacts of two of these treatments, gap cutting and shelterwood cutting, a survey was conducted 4 years after logging to examine the effects of these treatments on understorey vegetation species richness and abundance. Sampling scale was found to be an important factor affecting the results and subsequent interpretation of impacts. At the coupe scale, native plant species richness in unlogged coupe buffers was similar to that in adjacent logged patches. However, the mean number of species per 1 m2 was 20%-30% higher in the unlogged buffers than the logged patches. At all sampling scales, the abundance (number of individual plants) of native plants was 20%-35% higher in the buffers, but the abundance of introduced (weed) species was significantly higher in the logged patches. The abundance of weeds, which are mostly annual grasses and short-lived herbs, is likely to diminish with time. The time to recovery of native species abundance and the ecological significance of this is uncertain. Given the reported low seedling regeneration rate and limited dispersal capacity of many woody shrubs and perennial herbs, they are unlikely to return to pre-logging levels in the medium term. We attribute the reduction in the abundance of native plants mainly to mechanical soil disturbance, which ranged from 60% to 80% of the area of logged coupes, physical damage to the vegetation associated with logging and to intense heating of the topsoil during the post-logging silvicultural burn. Recommendations are made for reducing the negative impacts of logging operations on the understorey. © 2002 Taylor &amp; Francis Group, LLC.",Scopus,2-s2.0-0036507611
English,Article,2021,"Esteghamati A., Sayyahfar S., Ghaemi H., Joulani M., Moradi Y., Talebi A.",A comparison of post vaccination hepatitis b surface antibody level on the large and appropriate for gestational age infants,Clinical and Experimental Vaccine Research,10,1,,47,51,,,10.7774/cevr.2021.10.1.47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102750706&doi=10.7774%2fcevr.2021.10.1.47&partnerID=40&md5=8a1b0a9558696753b5dd37ac5e567460,"Purpose: The aim of this study was to compare the hepatitis B surface antibody (HBs Ab) titer 1 month after the 4th dose of hepatitis B vaccine administration on the large and appropriate for gestational age infants. Materials and Methods: This cross-sectional study was conducted on 7-month-old cases (n=132) divided into two groups of 2–4 kg (group 1: appropriate for gestational age, 63 cases) and >4 kg (group 2: large for gestational age, 69 cases), whom were vaccinated with a four-dose schedule of hepatitis B vaccine in 2016, Tehran, Iran. Results: Mean birth weight of the groups was 2.98±0.528 and 4.19±0.190 kg, respectively. Hepatitis B surface antigen and hepatitis B core antibody were negative in all cases. HBs Ab level in group 1 and 2 was 13,701.00±11,744.439 and 8,997.15±2,827.191, respectively (95% confidence interval of difference,-7,607.44 to-1,800.25). There was a significant difference between the two groups in antibody titration and antibody logarithm level (p=0.002, p=0.0001). Conclusion: Birth weight may affect the response to the hepatitis B virus vaccine administration. © Korean Vaccine Society.",Scopus,2-s2.0-85102750706
English,Short Survey,2017,"Hessling M., Spellerberg B., Hoenes K.",Photoinactivation of bacteria by endogenous photosensitizers and exposure to visible light of different wavelengths - A review on existing data,FEMS Microbiology Letters,364,2,fnw270,,,,47,10.1093/femsle/fnw270,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014413627&doi=10.1093%2ffemsle%2ffnw270&partnerID=40&md5=af832b5e76ed284e515af08890d4c21b,"Visible light has strong disinfectant properties, a fact that is not well known in comparison to the antibacterial properties of UV light. This review compiles the published data on bacterial inactivation caused by visible light and endogenous photosensitizers. It evaluates more than 50 published studies containing information on about 40 different bacterial species irradiated within the spectral range from 380 to 780 nm. In the available data a high variability of photoinactivation sensitivity is observed, which may be caused by undefined illumination conditions. Under aerobic conditions almost all bacteria except spores should be reduced by at least three log-levels with a dose of about 500 J cm-2 of 405 nm irradiation, including both Gram-positive as well as Gram-negative microorganisms. Irradiation of 470 nm is also appropriate for photoinactivating all bacteria species investigated so far but compared to 405 nm illumination it is less effective by a factor between 2 and 5. The spectral dependence of the observed photoinactivation sensitivities gives reason to the assumption that a so far unknown photosensitizer may be involved at 470 nm photoinactivation. © FEMS 2016. All rights reserved.",Scopus,2-s2.0-85014413627
English,Article,2013,"Moore J.R., Lyon A.J., Searles G.J., Lehneke S.A., Ridley-Ellis D.J.",Within- and between-stand variation in selected properties of Sitka spruce sawn timber in the UK: Implications for segregation and grade recovery,Annals of Forest Science,70,4,,403,415,,36,10.1007/s13595-013-0275-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878683906&doi=10.1007%2fs13595-013-0275-y&partnerID=40&md5=955e91dc884b31735ae77e46f512fdc6,"Context: Information on wood properties variation is needed by forest growers and timber processors to best utilise the available forest resource and to guide future management. Aim: This study aims to quantify the variation in selected properties of Sitka spruce (Picea sitchensis (Bong.) Carr.) structural timber. Methods: Twelve harvest-age stands were selected, ten trees per site were felled and processed into 301 logs. Dynamic modulus of elasticity (MOE dyn) was measured on each tree and log using portable acoustic instruments. Logs were processed into structural timber and its MOE and bending strength was determined. Results: Overall, the timber satisfied the MOE, bending strength and density requirements for the C16 strength class. Approximately 25 % of the total variation in timber mechanical properties was attributed to between-stand differences, with the remaining 75 % attributed to within-stand differences. A series of equations were developed to predict site, tree and log-level variation in timber properties. Conclusion: Knowledge of the site and stand factors that are associated with differences in timber properties can assist with segregation of the current resource. Portable acoustic tools can also be used to increase the stiffness of sawn timber by segregating out individual trees and logs that will yield low stiffness timber. © 2013 INRA and Springer-Verlag France.",Scopus,2-s2.0-84878683906
English,Article,2012,"Haaken D., Dittmar T., Schmalz V., Worch E.",Influence of operating conditions and wastewater-specific parameters on the electrochemical bulk disinfection of biologically treated sewage at boron-doped diamond (BDD) electrodes,Desalination and Water Treatment,46,1-3,,160,167,,26,10.1080/19443994.2012.677523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862138553&doi=10.1080%2f19443994.2012.677523&partnerID=40&md5=567ac623d7252aeda4dc7041e88f9042,"The aim of this study was to investigate an electrochemical process for bulk disinfection of biologically treated sewage. The influence of operating conditions (current density and flow rate) on the electrochemical formation of free chlorine in the sewage was determined. Furthermore, the effect of wastewater-specific parameters on the inactivation of Escherichia coli was studied. The disinfection capacity is primarily influenced by the concentration of electrochemically produced free chlorine. The production rate of free chlorine is independent of the flow rate within the range of 25-125 L h-1. The investigations have also shown that the electrochemical disinfection of E. coli in secondary effluents with BDD electrodes proceeds effectively at an electric charge input of 0.1-0.15 Ah L-1 corresponding to an energy expenditure of 2.0-2.6 kWh m-3. The electrochemically generated concentration of free chlorine (c = 0.4-0.6 mg L-1) is sufficient for an E. coli reduction of four log levels under the following conditions: after-reaction time of 15-20 min, T &gt; 6°C, pH &lt; 8.5 and DOC &lt; 22 mg L-1. The formation of organic by-products (AOX, THMs) was marginal to moderate. The inorganic by-products chlorate (1.2 mg L-1) and perchlorate (18 mg L-1) were produced in considerable concentrations. © 2012 Desalination Publications. All rights reserved.",Scopus,2-s2.0-84862138553
English,Article,2007,"Benjamin J., Chui Y.H., Zhang S.Y.",A method to assess lumber grade recovery improvement potential for black spruce logs based on branchiness,Forest Products Journal,57,12,,34,41,,7,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349143967&partnerID=40&md5=f623f703938dde452bafc4f47c39fce6,"A log-level lumber grade assessment method based on branchiness was developed to bring lumber grade considerations into forest management planning. Existing methods focus primarily on mean or maximum knot size per log. The method developed in this study is based on branch size and location on log surface, internal knot shape, and log size. Assuming a cylindrical log shape with a central pith, a log transformed linear regression model was developed to predict minimum horizontal branch angle (branch azimuth) between successive knots, with respect to log size, that would produce at least one piece of lumber at a desired grade, by product, from the center cant using either an edge or centerline knot pattern. The minimum difference in horizontal branch angle between successive branches decreased with increasing log size if product specifications were held constant and increased with increasing product width if log size remained constant. The above method was demonstrated using a sample of logs from three initial spacings (1.8 m, 2.7 m, and 3.6 m). Although lumber grade recovery improvement potential varied from 0 percent to 40 percent across spacings, no clear trend was evident for improvement potential by spacing at the product and grade level based on a chi-square analysis using a 2×3 contingency table (χ0.05.32 = 7.815, χedge2 = 3.979, and χcenterline 2 = 2.392). © Forest Products Society 2007.",Scopus,2-s2.0-38349143967
English,Article,2016,"Patel A., Hasak S., Cassell B., Ciorba M.A., Vivio E.E., Kumar M., Gyawali C.P., Sayuk G.S.",Effects of disturbed sleep on gastrointestinal and somatic pain symptoms in irritable bowel syndrome,Alimentary Pharmacology and Therapeutics,44,3,,246,258,,32,10.1111/apt.13677,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977508995&doi=10.1111%2fapt.13677&partnerID=40&md5=788b882e32758e523a61078074d30035,"Background: Sleep disturbances are common, and perhaps are even more prevalent in irritable bowel syndrome (IBS). Aims: To determine the effect of measured sleep on IBS symptoms the following day, IBS-specific quality of life (IBS-QOL) and non-GI pain symptoms. Methods: IBS patients' sleep patterns were compared to healthy individuals via wrist-mounted actigraphy over 7 days. Daily bowel pain logs (severity, distress; 10-point Likert) stool pattern (Bristol scale) and supporting symptoms (e.g. bloating, urgency; 5-point Likert) were kept. Validated measures, including the GI Symptom Rating Scale-IBS, Visceral Sensitivity Index, Pittsburgh Sleep Quality Index and the IBS-Quality of Life were collected. Mediation analysis explored the relationship between sleep, mood and bowel symptoms. Results: Fifty subjects (38.6 ± 1.0 years old, 44 female; 24 IBS and 26 healthy controls) completed sleep monitoring. IBS patients slept more hours per day (7.7 ± 0.2 vs. 7.1 ± 0.1, P = 0.008), but felt less well-rested. IBS patients demonstrated more waking episodes during sleep (waking episodes; 12.1 vs. 9.3, P < 0.001). Waking episodes predicted worse abdominal pain (P ≤ 0.01) and GI distress (P < 0.001), but not bowel pattern or accessory IBS symptoms (P > 0.3 for each). Waking episodes negatively correlated with general- and IBS-specific QOL in IBS (r = −0.58 and −0.52, P < 0.001 for each). Disturbed sleep effects on abdominal pain were partially explained by mood as an intermediate. Conclusions: Sleep disturbances are more common in irritable bowel syndrome, and correlate with IBS-related pain, distress and poorer irritable bowel syndrome-related quality of life. Disturbed sleep effects extend beyond the bowel, leading to worse mood and greater somatic pain in patients with the irritable bowel syndrome. © 2016 John Wiley & Sons Ltd",Scopus,2-s2.0-84977508995
English,Article,2001,"Cirera E., Plasència A., Ferrando J., Seguí-Gómez M.",Factors associated with severity and hospital admission of motor-vehicle injury cases in a southern European urban area,European Journal of Epidemiology,17,3,,201,208,,27,10.1023/A:1017961921607,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034790407&doi=10.1023%2fA%3a1017961921607&partnerID=40&md5=9f5ad0063ab49b9cea2a7e37559815df,"Objectives: To describe the characteristics of motor-vehicle (MV) injury cases admitted to Emergency departments (ED), and to assess factors related to injury severity and hospital admission. Setting: Subjects were MV injury patients, aged 16 or more, admitted to four EDs in the city of Barcelona (Spain), from July 1995 to June 1996. Methods: Cross-sectional design. The data analyzed were obtained from the information routinely transmitted from the EDs to the Municipal Institute of Health, based on the processing of ED logs. Severity was assessed with the Abbreviated Injury Scale and the Injury Severity Score. Univariate and bivariate descriptive statistical analyses were performed, as well as multiple logistic regressions. Results: For the 3791 MV-injury cases included in the study period, a larger contribution of cases was noted for males (63.1%), for cases younger than 30 years (55.3%) and for motorcycle or moped occupants (47.1%). After adjusting for age, sex and the presence of multiple injuries, pedestrians, followed by moped and motorcycle occupants were at a higher risk of a more severe injury (OR: 1.77, 1.61 and 1.50 respectively). Correspondingly, these user groups also showed a higher likelihood of a hospital admission (OR: 2.03, 1.92 and 2.00 respectively), when attended to in an ED. Injury cases attended to in the ED during night hours (OR: 2.06) were also at a higher risk of a hospital admission. Conclusions: In Barcelona, pedestrians and two-wheel MV occupants, besides accounting for two-thirds of MV injury cases, are the user groups with a greater risk of a more severe injury, as well as a higher chance of a hospital admission, independently of demographic and health care factors.",Scopus,2-s2.0-0034790407
English,Article,2010,"Park E.S., Park J., Lomax T.J.",A fully Bayesian multivariate approach to before-after safety evaluation,Accident Analysis and Prevention,42,4,,1118,1127,,69,10.1016/j.aap.2009.12.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955318543&doi=10.1016%2fj.aap.2009.12.026&partnerID=40&md5=1cbf2dc4b5e1cc6fb2174b1a1d87c6ed,"This paper presents a fully Bayesian multivariate approach to before-after safety evaluation. Although empirical Bayes (EB) methods have been widely accepted as statistically defensible safety evaluation tools in observational before-after studies for more than a decade, EB has some limitations such that it requires a development and calibration of reliable safety performance functions (SPFs) and the uncertainty in the EB safety effectiveness estimates may be underestimated when a fairly large reference group is not available. This is because uncertainty (standard errors) of the estimated regression coefficients and dispersion parameter in SPFs is not reflected in the final safety effectiveness estimate of EB. Fully Bayesian (FB) methodologies in safety evaluation are emerging as the state-of-the-art methods that have a potential to overcome the limitations of EB in that uncertainty in regression parameters in the FB approach is propagated throughout the model and carries through to the final safety effectiveness estimate. Nonetheless, there have not yet been many applications of fully Bayesian methods in before-after studies. Part of reasons is the lack of documentation for a step-by-step FB implementation procedure for practitioners as well as an increased complexity in computation. As opposed to the EB methods of which steps are well-documented in the literature for practitioners, the steps for implementing before-after FB evaluations have not yet been clearly established, especially in more general settings such as a before-after study with a comparison group/comparison groups. The objectives of this paper are two-fold: (1) to develop a fully Bayesian multivariate approach jointly modeling crash counts of different types or severity levels for a before-after evaluation with a comparison group/comparison groups and (2) to establish a step-by-step procedure for implementing the FB methods for a before-after evaluation with a comparison group/comparison groups. The fully Bayesian multivariate approach introduced in this paper has additional advantages over the corresponding univariate approaches (whether classical or Bayesian) in that the multivariate approach can recover the underlying correlation structure of the multivariate crash counts and can also lead to a more precise safety effectiveness estimate by taking into account correlations among different crash severities or types for estimation of the expected number of crashes. The new method is illustrated with the multivariate crash count data obtained from expressways in Korea for 13 years to assess the safety effectiveness of decreasing the posted speed limit. © 2009 Elsevier Ltd.",Scopus,2-s2.0-77955318543
English,Article,2018,Azar S.A.,Gold and US money demand,Economics and Business Letters,7,3,,108,114,,,10.17811/ebl.7.3.2018.108-114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058018697&doi=10.17811%2febl.7.3.2018.108-114&partnerID=40&md5=6a0da32000b8e0d87394f44f4b9c4500,"This letter is about the long run cointegration relation of the US money demand function that incorporates a gold price variable. A three-equation model is jointly constructed and estimated. The first equation has real gold prices, as a dependent variable, and real money, the real dollar index, a scale variable, and the lagged cointegration residual as independent variables. All the variables are in first-differences of the logs except the cointegration residual. The second equation is the cointegration regression with the same variables in log levels. And the third equation is a GARCH model of the conditional variance of residuals. Two different scale variables are chosen: the industrial production index and the real personal disposable income. Both variables produce close estimates. All coefficients are of the correct expected sign and are statistically different from zero. The evidence presented is highly supportive of the model. In particular we find long run money neutrality, and long run constant economies of scale for both scale variables. Moreover, both the short run and long run elasticities of the real dollar index are also unitary. Surprisingly real money and each one of the two scale variables, have no short run effects on the log of real gold prices, but have only long run effects. One can no more exclude gold from the US money demand without incurring a mis-specification. In this regard gold may be the missing variable that produces the structural breaks found in the literature. © 2018, Oviedo University Press. All rights reserved.",Scopus,2-s2.0-85058018697
English,Article,2017,"Tirloni E., Bernardi C., Gandolfi G., Cattaneo P., Stella S.",What happens to the microflora of retail sushi in the warm season?,Journal of Food and Nutrition Research,5,2,,95,100,,1,10.12691/jfnr-5-2-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049232167&doi=10.12691%2fjfnr-5-2-4&partnerID=40&md5=174d9bfe1692828741d87f0e8294516c,"Sushi is a perishable ready to eat product composed by several raw ingredients, and the storage temperature is crucial in the maintenance of satisfactory hygiene. Aim of this study was the microbiological characterization of sushi mimicking a thermal abuse likely occurring in the summer season. Mixed sushi (rolls and nigiri) produced in a small scale factory in Northern Italy was stored for 2 h at 12°C (“transport‖) and subsequently for 4 days at 8°C (“home storage‖) and daily submitted to microbiological analyses coupled with the control of organoleptic quality. Total viable Count was above 5 Log since the production day and was mainly constituted by Pseudomonas spp.; the values increased during storage overcoming the 6 Log level from day 3, and reaching level above 8 Log CFU/g at the last sampling time. From a sensorial point of view, from the second day a decay in odour and colour was observed. LAB showed a gradual increase never overcoming 6 Log CFU/g, while Enterobacteriaceae increased and overcame 4 Log CFU/g after 2 days. Yeasts showed a moderate growth (always <5 Log CFU/g) while Bacillus cereus, Staphylococci and Clostridia were generally below the detection limits. Listeria monocytogenes was never detected. A reduction of shelf-life from 3 to 2 days should be applied especially in particular warm months in order to limit bacterial replication. © Science and Education Publishing 2017.",Scopus,2-s2.0-85049232167
English,Article,2014,"Kaspari O., Lemmer K., Becker S., Lochau P., Howaldt S., Nattermann H., Grunow R.",Decontamination of a BSL3 laboratory by hydrogen peroxide fumigation using three different surrogates for Bacillus anthracis spores,Journal of Applied Microbiology,117,4,,1095,1103,,8,10.1111/jam.12601,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908260933&doi=10.1111%2fjam.12601&partnerID=40&md5=0885084a4d990b368961414aabbb58c4,"Aims: Two independent trials investigated the decontamination of a BSL3 laboratory using vaporous hydrogen peroxide and compared the effect on spores of Bacillus cereus, Bacillus subtilis and Bacillus thuringiensis as surrogates for Bacillus anthracis spores, while spores of Geobacillus stearothermophilus served as control. Methods and Results: Carriers containing 1·0 × 106 spores were placed at various locations within the laboratory before fumigation with hydrogen peroxide following a previously validated protocol. Afterwards, carriers were monitored by plating out samples on agar and observing enrichment in nutrient medium for up to 14 days. Three months later, the experiment was repeated and results were compared. On 98 of 102 carriers, no viable spores could be detected after decontamination, while the remaining four carriers exhibited growth of CFU only after enrichment for several days. Reduction factors between 4·0 and 6·0 log levels could be reached. Conclusions: A validated decontamination of a laboratory with hydrogen peroxide represents an effective alternative to fumigation with formaldehyde. Spores of B. cereus seem to be more resistant than those of G. stearothermophilus. Significance and Impact of the Study: The results of this study provide important results in the field of hydrogen peroxide decontamination when analysing the effect on spores other than those of G. stearothermophilus. © 2014 The Society for Applied Microbiology.",Scopus,2-s2.0-84908260933
English,Article,2012,"Anil Kumar M.R., Johnson K.M., Ponmurugan P.",Evaluation of bacterial removal efficiency of domestic water purification systems with different technologies widely used in India,"Asian Journal of Microbiology, Biotechnology and Environmental Sciences",14,2,,263,266,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863617994&partnerID=40&md5=cc59825961085f6f4bf4433d8850f358,"Bacterial disinfection using ultraviolet radiation is a world wide technology for drinking water purification. The efficiency of this technology depends on certain important parameters like the UV intensity, the exposure time, the area, clarity of the water etc. Ultrafiltration is a membrane technology which removes the bacteria and its porosity is 0.01 micron. Nanofiltration removes the bacteria and this membrane porosity is 0.001 micron size. Reverse osmosis is a high end membrane technology with a porosity of 0.0001 micron removes bacteria. Brominated resin beads bacterial removal efficiency depends upon the clarity of the input water, flow rate, contact time etc.. This paper is indented to assist the reader about the various technology's capabilities to remove the pathogenic bacteria from drinking water. Standardized suspensions of test bacterial strains each having a cell density of approximately 105 cfu/mL were mixed with the inlet water in a pre unit reservoir and passed through the system. Samples were collected from the outlet after each passage and tested for bacterial viability. Test procedures were based on internationally accepted principles for the evaluation of point of use water purification units, including a standard test protocol of the United States Environmental Protection Agency. Reduction in numbers of seeded test organisms at several log levels higher than those expected in water for which the unit is intended, was determined by the cultivation of viable organisms. © Global Science Publications.",Scopus,2-s2.0-84863617994
English,Article,1984,"Hilgers R.D., Standefer J.C., Rutledge J.M., Ampuero F.",Trophoblastic cell sensitivity to 8-day chemotherapy in nonmetastatic gestational trophoblastic neoplasia,Gynecologic Oncology,17,3,,386,393,,5,10.1016/0090-8258(84)90226-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021337230&doi=10.1016%2f0090-8258%2884%2990226-9&partnerID=40&md5=1b6def0ac462da46bd6bf2d6d710d03e,"Serial radioimmunoassay determinations of serum βhCG and methotrexate were compared in two patients with nonmetastatic gestational trophoblastic neoplasia (NMGTN) treated with Goldstein's modification of Bagshawe's intermediate-dose methotrexate-citrovorum factor rescue-treatment program. Pretreatment βhCG levels (mlU/ml) ranged within the outer limits of the 103 log level. Following intravenous methotrexate, sharp serum peaks between 10-6 and 10-5 M were observed. Plasma disappearance was rapid with a 3 log drop noted within 24 hr to levels incapable of inhibiting DNA synthesis. βhCG levels manifested a 1 to 1.5 log drop over the 8 days of chemotherapy and complete remission was noted within 5 to 6 weeks of the first dose of methotrexate. No significant clinical or laboratory toxicity was observed. Although cell culture studies show that 100% of cell death can be achieved with serum levels of 10-5 M in methotrexate-resistant choriocarcinoma, similar data do not exist for previously untreated trophoblastic neoplastic cells. These preliminary observations suggest that serum methotrexate levels are important for establishing sensitivity levels in a heterogeneous population of trophoblastic cells in NMGTN and that the total dose of methotrexate may be safely preselected on the basis of the pretreatment βhCG. © 1984.",Scopus,2-s2.0-0021337230
English,Article,2012,"Saner P., Loh Y.Y., Ong R.C., Hector A.","Carbon stocks and fluxes in tropical lowland dipterocarp rainforests in Sabah, Malaysian Borneo",PLoS ONE,7,1,e29642,,,,71,10.1371/journal.pone.0029642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855335546&doi=10.1371%2fjournal.pone.0029642&partnerID=40&md5=de1602bf89008cf29b2d6afde11614f0,"Deforestation in the tropics is an important source of carbon C release to the atmosphere. To provide a sound scientific base for efforts taken to reduce emissions from deforestation and degradation (REDD+) good estimates of C stocks and fluxes are important. We present components of the C balance for selectively logged lowland tropical dipterocarp rainforest in the Malua Forest Reserve of Sabah, Malaysian Borneo. Total organic C in this area was 167.9 Mg C ha -1±3.8 (SD), including: Total aboveground (TAGC: 55%; 91.9 Mg C ha -1±2.9 SEM) and belowground carbon in trees (TBGC: 10%; 16.5 Mg C ha -1±0.5 SEM), deadwood (8%; 13.2 Mg C ha -1±3.5 SEM) and soil organic matter (SOM: 24%; 39.6 Mg C ha -1±0.9 SEM), understory vegetation (3%; 5.1 Mg C ha -1±1.7 SEM), standing litter (&lt;1%; 0.7 Mg C ha -1±0.1 SEM) and fine root biomass (&lt;1%; 0.9 Mg C ha -1±0.1 SEM). Fluxes included litterfall, a proxy for leaf net primary productivity (4.9 Mg C ha -1 yr -1±0.1 SEM), and soil respiration, a measure for heterotrophic ecosystem respiration (28.6 Mg C ha -1 yr -1±1.2 SEM). The missing estimates necessary to close the C balance are wood net primary productivity and autotrophic respiration. Twenty-two years after logging TAGC stocks were 28% lower compared to unlogged forest (128 Mg C ha -1±13.4 SEM); a combined weighted average mean reduction due to selective logging of -57.8 Mg C ha -1 (with 95% CI -75.5 to -40.2). Based on the findings we conclude that selective logging decreased the dipterocarp stock by 55-66%. Silvicultural treatments may have the potential to accelerate the recovery of dipterocarp C stocks to pre-logging levels. © 2012 Saner et al.",Scopus,2-s2.0-84855335546
English,Article,2009,"Kreutzweiser D., Capell S., Good K., Holmes S.",Sediment deposition in streams adjacent to upland clearcuts and partially harvested riparian buffers in boreal forest catchments,Forest Ecology and Management,258,7,,1578,1585,,25,10.1016/j.foreco.2009.07.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68949145932&doi=10.1016%2fj.foreco.2009.07.005&partnerID=40&md5=bcac65b9af707be7f770d5a68e2a318a,"The rates of fine sediment deposition were compared among three logged and three reference stream reaches 2-3 years before and 3-4 years after logging to assess the environmental impacts of partial harvesting as a novel riparian management strategy for boreal forest streams. The partial-harvest logging resulted in 10, 21 and 28% average basal area removal from riparian buffers at the three logged sites, adjacent to upland clearcut areas. No significant differences from pre-logging or reference-site sedimentation patterns were detected for two of the three logged sites. At the site with the most intense riparian logging (WR2), significant increases of 3-5 times higher than pre-logging or reference levels were detected in fine inorganic sediment (250-1000 μm) load and accumulation in the first year after logging, but no significant change was detected in fine organic sediments or very fine sediments (0.5-250 μm). The increased inorganic sediment deposition at WR2 was temporary with no significant differences from reference or pre-logging levels detectable by summer of the second post-logging year. Logging impacts on fine sedimentation in streams appeared to have been mitigated by careful logging practices including winter harvesting in riparian areas to reduce ground disturbance, and a tendency to avoid immediate (within 3-5 m) stream-side areas. Where it is feasible and advisable to conduct partial harvesting in riparian buffers of boreal forest streams, the logging can be conducted without posing significant risk of increased sediment inputs to streams when careful logging practices are followed. Crown Copyright © 2009.",Scopus,2-s2.0-68949145932
English,Article,2017,"Filipescu C.N., Trofymow J.A., Koppenaal R.S.",Late-rotation nitrogen fertilization of Douglas-fir: Growth response and fibre properties,Canadian Journal of Forest Research,47,1,,134,138,,9,10.1139/cjfr-2016-0306,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027419146&doi=10.1139%2fcjfr-2016-0306&partnerID=40&md5=1dd7712bcb82ce2cc3647393bdb00de3,"Late-rotation fertilization of Douglas-fir (Pseudotsuga menziesii (Mirb.) Franco) 5 to 10 years before harvesting is a common management practice in British Columbia and the US Pacific Northwest. Despite widespread operational application, knowledge on the impact of late-rotation fertilization on forests, especially fibre properties, is lacking. In this study, we evaluate the growth response and fibre properties following nitrogen fertilization in a productive second-growth coastal Douglas-fir site at age 57 years. Destructive sampling of dominant and co-dominant trees in fertilized and control plots 5 years after fertilization indicated significant gain in stem volume (30%-40%) that was uniformly distributed along the stem. There were no discernible effects on wood quality at the log level in terms of resonance acoustic velocity. However, fibre properties within breast height tree rings indicated significant reductions of ring wood density (by 8%), earlywood density (17%), latewood percentage (10%), and modulus of elasticity (8%). Tracheid dimensions declined in earlywood (reduction of wall thickness by 15%), latewood (radial diameter by 8%), and fibre length (by 6%). Results indicate that late-rotation nitrogen fertilization of Douglas-fir may lead to a significant growth response with only minimal reduction of fibre properties. It is possible that the negative impact on fibre properties could become more significant for repeated applications or higher rates of nitrogen fertilization. © 2017, Canadian Science Publishing. All rights reserved.",Scopus,2-s2.0-85027419146
English,Review,2012,"Ghilencea D.M., Lee H.M., Park M.",Tuning supersymmetric models at the LHC: A comparative analysis at two-loop level,Journal of High Energy Physics,2012,7,46,,,,30,10.1007/JHEP07(2012)046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864483638&doi=10.1007%2fJHEP07%282012%29046&partnerID=40&md5=626e38c55f09abb8fcbc8bf6fcdfabdc,"We provide a comparative study of the fine tuning amount (Δ) at the two-loop leading log level in supersymmetric models commonly used in SUSY searches at the LHC. These are the constrained MSSM (CMSSM), non-universal Higgs masses models (NUHM1, NUHM2), non-universal gaugino masses model (NUGM) and GUT related gaugino masses models (NUGMd). Two definitions of the fine tuning are used, the first (Δmax) measures maximal fine-tuning w.r.t. individual parameters while the second (Δq) adds their contribution in"" quadrature"". As a direct consequence of two theoretical constraints (the EW minimum conditions), fine tuning (Δq) emerges at the mathematical level as a suppressing factor (effective prior) of the averaged likelihood (l) under the priors, under the integral of the global probability of measuring the data (Bayesian evidence p(D)). For each model, there is little difference between Δq, Δmax in the region allowed by the data, with similar behaviour as functions of the Higgs, gluino, stop mass or SUSY scale (msusy = (mt̄1 mt̄2)1/2) or dark matter and g-2 constraints. The analysis has the advantage that by replacing any of these mass scales or constraints by their latest bounds one easily infers for each model the value of Δq, Δq or vice versa. For all models, minimal fine tuning is achieved for Mhiggs near 115 GeV with aΔq ≈ Δmax ≈ 10 to 100 depending on the model, and in the CMSSM this is actually a global minimum. Due to a strong (≈ exponential) dependence of Δ on Mhiggs, for a Higgs mass near 125 GeV, the above values of Δq ≈ Δmax increase to between 500 and 1000. Possible corrections to these values are briefly discussed. © SISSA 2012.",Scopus,2-s2.0-84864483638
English,Article,2008,"Hatt S.R., Mohney B.G., Leske D.A., Holmes J.M.",Variability of Stereoacuity in Intermittent Exotropia,American Journal of Ophthalmology,145,3,,556,5610,,53,10.1016/j.ajo.2007.10.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149142209&doi=10.1016%2fj.ajo.2007.10.028&partnerID=40&md5=d122054c8a5410615173831d4b843304,"Purpose: Distance stereoacuity is used to monitor deterioration of intermittent exotropia (IXT), but variability of stereoacuity has not been studied rigorously. The purpose of this study was to assess the variability of stereoacuity over one day in children with IXT. Design: Prospective cohort study. Methods: Twelve children with IXT were recruited. Stereoacuity was assessed using the Frisby Davis Distance test and the Distance Randot test at distance, and the Frisby and Preschool Randot tests at near. Tests were repeated three or four times over the day, with at least two hours between assessments. The main outcome measure was variable stereoacuity defined as a change by two or more log levels between any two time points over the day. Results: Variable stereoacuity at distance was found in five (42%) of 12 patients. Four (33%) of 12 patients demonstrated variable results using the Distance Randot test, three of whom also showed variable results using the Frisby Davis Distance test. One patient had variable results using the Frisby Davis Distance test only. Nine (75%) of 12 patients completed near stereoacuity testing; two (22%) of nine showed variable near stereoacuity. Two (22%) of nine showed variable results using the Preschool Randot test, one (11%) of whom also had variable results using the Frisby test. In some cases, stereoacuity changed from measurable stereoacuity on one assessment to nil on another. Conclusions: Nearly half of children with IXT show marked changes in stereoacuity over the course of a single day. When based on isolated measures, an apparent change in distance stereoacuity between visits should be interpreted with caution. © 2008 Elsevier Inc. All rights reserved.",Scopus,2-s2.0-39149142209
English,Article,1999,"Grabow W.O., Clay C.G., Dhaliwal W., Vrey M.A., Müller E.E.","Elimination of viruses, phages, bacteria and Cryptosporidium by a new generation Aquaguard point-of-use water treatment unit.",Zentralblatt für Hygiene und Umweltmedizin = International journal of hygiene and environmental medicine,202,5,,399,410,,14,10.1016/s0934-8859(99)80005-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033193661&doi=10.1016%2fs0934-8859%2899%2980005-4&partnerID=40&md5=5e46a16db1b8e604521f8d4f6645b2f9,"The elimination of human viruses, phages, bacteria and Cryptosporidium oocysts by a new generation commercial Aquaguard purifier for the domestic treatment of drinking water, has been evaluated. The unit basically consists of a candle prefilter, activated carbon filter and ultraviolet irradiation compartment. Drinking water seeded with selected laboratory test strains of resistant micro-organisms was passed through the unit. Similar tests were carried out with sewage-contaminated river water and secondary treated waste water containing naturally occurring organisms. Test procedures were based on internationally accepted principles for the evaluation of point-of-use water treatment units, including a standard test protocol of the United States Environmental Protection Agency. Reduction in numbers of seeded test organisms at several log levels higher than those expected in water for which the unit is intended, was determined by the cultivation of viable organisms. In the case of seeded viruses and Cryptosporidium parvum oocysts the qualitative absence of nucleic acid was determined by the reverse transcriptase polymerase chain reaction (RT-PCR). At the design flow rate of one litre per minute, numbers of polio, hepatitis A, adeno types 2 and 41, rota SA11, human rota and astro viruses, as well as somatic and MS2 coliphages, and Escherichia coli, Streptococcus faecalis, Clostridium perfringens, total coliform bacteria, enterococci, heterotrophic bacteria and C. parvum oocysts, were reduced by more than 99.999% in all waters tested. This efficiency conforms to specifications for such units. The quality of the treated water was well within microbiological limits of international specifications for drinking water.",Scopus,2-s2.0-0033193661
English,Article,1999,"Shirtz J.T., Soli T.C., Allen W.E., Stellwag E.J., McConnell T.J.",Evaluation of the effects of fragmented steam exposure cycles on the survival of bacterial spores,PDA Journal of Pharmaceutical Science and Technology,53,1,,11,22,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032903266&partnerID=40&md5=a9a03c22be9eda8150b37f0d4d8a436b,"The purpose of this study was to examine the population and resistance characteristics of bacterial spores which have been exposed to an abbreviated steam sterilization cycle. The philosophy of many pharmaceutical manufacturers is to require a second complete terminal sterilization cycle in the event of an unplanned interruption during the terminal sterilization of a production batch. The impact of abbreviated steam sterilization cycles was examined for their effect on the survivability and resistance of bacterial spores following an inadequate sterilization cycle. Steam sterilization cycles of two minutes and four minutes were performed on separate groups of Biological Indicator spore strips. These groups were then held at room temperature and re-exposed to a range of sterilization conditions after 24, 48, and 72 hours, i.e., start cycle, abort, hold, start cycle, abort. Spore survivor curves were calculated and resistance estimations were determined. The results of the study indicated that the log level of the surviving spores remained fairly constant, but variability within groups increased as sterilization time increased. The resistance of these surviving spores, as measured by D value, also remained relatively constant throughout the holding period. Abbreviated cycles were similarly conducted on ampules containing a spore suspension, and the spore populations and moist heat resistances were determined over time. Contrary to the spore strip, the population of the subject ampules was less stable showing a gradual decline over the same observation period. The study also included a comparison of the surviving population of short and long fragmented cycles. The results of this study demonstrate that a second complete sterilization cycle is unnecessary to assure the absence of living matter in the sterilized units.",Scopus,2-s2.0-0032903266
English,Article,1995,"Thayer D.W., Boyd G., Huhtanen C.N.","Effects of ionizing radiation and anaerobic refrigerated storage on indigenous microflora, Salmonella, and Clostridium botulinum types A and B in vacuum-canned, mechanically deboned chicken meat",Journal of Food Protection,58,7,,752,757,,12,10.4315/0362-028X-58.7.752,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028815212&doi=10.4315%2f0362-028X-58.7.752&partnerID=40&md5=bb2019e89a01ad5a78cb45ec01d26a4d,"Vacuum-canned, commercial, mechanically deboned chieken meat was challenged with either Clostridium botulinum spores (20 strains of types A and B, proteolytic; final spore concentration of ca. 400/g of meat) or Salmonella enteritidis (ca. 104 CFU/ g of meat) followed by irradiation to 0, 1.5, and 3.0 kGy and storage at 5°C for 0, 2, and 4 weeks. None of the samples stored at 5°C developed botulinal toxin; however, when these samples were temperature abused at 28°C they became toxic within 18 h and had obvious signs of spoilage, i.e., swelling of the can and a putrid odor. During 4 weeks of refrigerated storage the log|10of the population of S. enteritidis in nonirradiated samples decreased from 3.86 to 2.58. S. enteritidis CFU were detectable in samples irradiated to 1.5 kGy at 0 weeks but not in samples irradiated to 3.0 kGy. Log levels of aerobic and facultative mesophiles increased during 4 weeks of refrigerated storage from 6.54 to 8.25, 4.03 to 8.14, and 2.84 to 5.23 in samples irradiated to 0, 1.5, and 3.0 kGy, respectively. Based on taxonomie analyses of 245 isolates, the bacterial populations depended upon radiation dose and storage time. The change was predominantly from gram-negative rods in nonirradiated samples to gram-positive streptococci in samples irradiated to 3.0 kGy and stored for 4 weeks. Spoilage organisms survived even the 3.0 kGy treatment. © International Association of Milk, Food and Environmental Sanitarians.",Scopus,2-s2.0-0028815212
English,Article,1990,"Henry F.J., Rahim Z.","Transmission of diarrhoea in two crowded areas with different sanitary facilities in Dhaka, Bangladesh",Journal of Tropical Medicine and Hygiene,93,2,,121,126,,26,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025220433&partnerID=40&md5=ac6346688421e6f81e4290e781f33f1c,"To determine the importance of water-borne and water-washed transmission of diarrhoea we compared the degree of contamination of children's hands and drinking water with their diarrhoeal morbidity. Diarrhoeal incidence in 137 children aged 1-6 years was obtained through fortnightly home visits during the calendar year 1985. Bacterial contamination of hands and drinking water was assessed semi-quantitatively by direct contact using agar-coated slides incorporating a selective medium permitting growth of Entero-bacteriaceae (Hygicult, Orion Diagnostica, Finland). Results were expressed as 2-day mean log of colony forming units per gram (cfu/g). Children were studied in two densely populated urban areas: 56 children in one area with latrines and tubewells and 81 children in the other without such facilities. Mean diarrhoea attack rates were lower in the better sanitary area (2.5 vs 3.2, P<0.05) as were mean log levels of water contamination (3.1 cfu/g vs 4.3 cfu/g, P<0.001). There was no significant correlation between water contamination and diarrhoeal incidence on an individual basis. However, in both areas diarrhoea incidence was significantly correlated with the degree of contamination of hands. After adjusting for age the risk of diarrhoea increased significantly for children with more contaminated hands in the unimproved area. This relationship strongly supports the promotion of handwashing as a method of controlling diarrhoeal diseases and, by implication, the greater importance of water quantity compared to quality.",Scopus,2-s2.0-0025220433
English,Article,1996,"Pinard M.A., Putz F.E.",Retaining forest biomass by reducing logging damage,Biotropica,28,3,,278,295,,291,10.2307/2389193,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030326698&doi=10.2307%2f2389193&partnerID=40&md5=8c0664090cdad2ec073e307dc2c6f308,"Global concern over rising atmospheric concentrations of carbon dioxide is stimulating development and implementation of policies aimed at reducing net greenhouse gas emissions by enhancing carbon sinks. One option for reducing net emissions is to lessen damage to residual forests during selective logging, thereby retaining additional carbon in biomass. A pilot carbon offset project was initiated in Sabah, Malaysia, in 1992 in which a power company provided funds to a timber concessionaire to implement guidelines aimed at reducing logging damage; in doing so, the utility gained potential credit towards future emissions reduction requirements To quantify the carbon retained due to this effort, we compared dipterocarp forests logged according to reduced-impact logging guidelines with forests logged by conventional methods in terms of the above- and below-ground biomass both before and after logging. Prior to logging, the forest stored approximately 400 Mg biomass ha-1, 17 percent of which was bdowground. High volumes of timber were removed from both of the logging areas (mean CNV = 154. RIL - 104 m'ha1) Forty-one percent of the unharvested trees &lt;60 cm DBH were severely damaged (uprooted and crushed) from logging in conventional logging areas in contrast to 15 percent in reduced-impact logging areas. Approximately 18 and 12 percent, respectively, of the remaining residual trees in conventional and reduced-impact logging areas suffered less severe damage (e. g., crown or bark damage). Mortality rates of the less severely damaged trees in all DBH classes were higher during the first year in conventional logging areas than in reduced-impact logging areas. One yr post harvest, conventional and reduced-impact logging areas contained biomass equivalent to about 44 percent and 67 percent of pre-logging levels, respectively. Approximately 62 percent of the difference in carbon retention was due to fewer trees killed in the reduced-impact logging areas; the remaining 38 percent was due to a lower mass of branches, stumps and waste wood from felled trees in reduced-impact logging areas. Mortality of damaged trees in both areas may contribute to net decreases in biomass for several years after logging. More and larger trees remained undamaged where reducedimpact logging was practiced, hence future biomass increment and yields of marketable timber are expected to be greater in the reduced-impact logging areas than in conventional logging areas.",Scopus,2-s2.0-0030326698
English,Article,2021,"Zhu T., Wang J., Ruan L., Xiong C., Yu J., Li Y., Chen Y., Lv M., Chen T.","General, Efficient, and Real-Time Data Compaction Strategy for APT Forensic Analysis",IEEE Transactions on Information Forensics and Security,16,,9417210,3312,3325,,,10.1109/TIFS.2021.3076288,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105083105&doi=10.1109%2fTIFS.2021.3076288&partnerID=40&md5=60ebd8d4292159504704e7ae94be61d3,"The damage caused by Advanced Persistent Threat (APT) attacks to governments and large enterprises is gradually escalating. Once an attack event is detected, forensic analysis will use the dependencies between system audit logs to rapidly locate intrusion points and determine the impact of the attacks. Due to the high persistence of APT attacks, huge amounts of data will be stored to meet the needs of forensic analysis, which not only brings great storage overhead, but also sharply increases the computing costs. To compact data without affecting forensic analysis, several methods have been proposed. However, in real-world scenarios, we meet the problems of weak cross-platform capability, large data processing overhead, and poor real-time performance, rendering existing data compaction methods difficult to meet the usability and universality requirements jointly. To overcome these difficulties, this paper proposes a general, efficient, and real-time data compaction method at the system log level; it does not involve internal analysis of the program or depend on the specific operating system type, and it includes two strategies: 1) data compaction of maintaining global semantics (GS), which determines and deletes redundant events that do not affect global dependencies, and 2) data compaction based on suspicious semantics (SS). Given that the purpose of forensic analysis is to restore the attack chain, SS performs context analysis on the remaining events from GS and further deletes the parts that are not related to the attack. The results of the real-world experiments show that the compaction ratios of our method to system events are as high as 4.36× to 13.18× and 7.86× to 26.99× on GS and SS, respectively, which is better than state-of-the-art studies. © 2005-2012 IEEE.",Scopus,2-s2.0-85105083105
English,Conference Paper,2020,"Kőrösi G., Farkas R.",MOOC Performance Prediction by Deep Learning from Raw Clickstream Data,Communications in Computer and Information Science,1244 CCIS,,,474,485,,4,10.1007/978-981-15-6634-9_43,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089211381&doi=10.1007%2f978-981-15-6634-9_43&partnerID=40&md5=6b266b3b9c3a1199c263e1890be22ee1,"Student performance prediction is a challenging problem in online education. One of the key issues relating to the quality Massive Open Online Courses (MOOC) teaching is the issue of how to foretell student performance in the future during the initial phases of education. While the fame of MOOCs has been rapidly increasing, there is a growing interest in scalable automated support technologies for student learning. Researchers have implemented numerous different Machine Learning algorithms in order to find suitable solutions to this problem. The main concept was to manually design features through cumulating daily, weekly or monthly user log data and use standard Machine Learners, like SVM, LOGREG or MLP. Deep learning algorithms could give us new opportunities, as we can apply them directly on raw input data, and we could spare the most time-consuming process of feature engineering. Based on our extensive literature survey, recent deep learning publications on MOOC sequences are based on cumulated data, i.e. on fine-engineered features. The main contribution of this paper is using raw log-line-level data as our input without any feature engineering and Recurrent Neural Networks (RNN) to predict student performance at the end of the MOOC course. We used the Stanford Lagunita’s dataset, consisting of log-level data of 130000 students and compared the RNN model based on raw data to standard classifiers using hand-crafted commulated features. The experimental results presented in this paper indicate the RNN’s dominance given its dependably superior performance as compared with the standard method. As far as we know, this will be the first work to use deep learning to predict student performance from raw log-line level students’ clickstream sequences in an online course. © 2020, Springer Nature Singapore Pte Ltd.",Scopus,2-s2.0-85089211381
English,Article,2019,"Strassl R., Doberer K., Rasoul-Rockenschaub S., Herkner H., Görzer I., Kläger J.P., Schmidt R., Haslacher H., Schiemann M., Eskandary F.A., Kikic E., Reindl-Schwaighofer R., Puchhammer-Stöckl E., Böhmig G.A., Bond G.",Torque teno virus for risk stratification of acute biopsyproven alloreactivity in kidney transplant recipients,Journal of Infectious Diseases,219,12,,1934,1939,,10,10.1093/infdis/jiz039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066932281&doi=10.1093%2finfdis%2fjiz039&partnerID=40&md5=14e4f23661c0e1dab773f8d5ede1384d,"Background. Drug-induced immunosuppression in kidney transplant recipients is crucial to prevent allograft rejection, but increases risk for infectious disease. Immunologic monitoring to tailor immunosuppressive drugs might prevent alloreactivity and adverse effects simultaneously. The apathogenic torque teno virus (TTV) reflects the immunocompetence of its host and might act as a potential candidate for a holistic monitoring. Methods. We screened all 1010 consecutive patients from the prospective Vienna Kidney Transplant Cohort Study for availability of allograft biopsies and adequately stored sera for TTV quantification by polymerase chain reaction. Results. Patients with acute biopsy-proven alloreactivity according to the Banff classification (n = 33) showed lower levels of TTV in the peripheral blood compared to patients without rejection (n = 80) at a median of 43 days before the biopsy. The risk for alloreactivity decreased by 10% per log level of TTV copies/mL (risk ratio,.90 [95% confidence interval,.84-.97]; P =.005). TTV levels &gt;1 × 106 copies/mL exclude rejection with a sensitivity of 94%. Multivariable generalized linear modeling suggests an independent association between TTV level and alloreactivity. Conclusions. TTV is a prospective biomarker for risk stratification of acute biopsy-proven alloreactivity in kidney transplant recipients and might be a potential tool to tailor immunosuppressive drug therapy. © 2019 The Author(s).",Scopus,2-s2.0-85066932281
English,Article,2019,"Zwirzitz B., Wetzels S.U., Rabanser I., Thalguter S., Dzieciol M., Wagner M., Mann E.",Culture-independent evaluation of bacterial contamination patterns on pig carcasses at a commercial slaughter facility,Journal of Food Protection,82,10,,1677,1682,,1,10.4315/0362-028X.JFP-19-103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072627676&doi=10.4315%2f0362-028X.JFP-19-103&partnerID=40&md5=b1b15a59c73329535c4e1363ee7898bb,"Traditionally, the microbiological status of meat is determined by culture-based techniques, although many bacteria are not able to grow on conventional media. The aim of this study was to obtain quantitative data on total bacterial cell equivalents, as well as taxa-specific abundances, on carcass surfaces during pig slaughter using quantitative real-time PCR. We evaluated microbial contamination patterns of total bacteria, Campylobacter, Escherichia coli, Lactobacillus group, Listeria monocytogenes, Salmonella, and Pseudomonas species throughout slaughtering and on different carcass areas. In addition, we compared contamination levels of breeding sow carcasses with fattening pig carcasses, and we assessed the efficacy of carcass polishing machines under two water amount conditions. Our results demonstrate that relevant meat-spoilage organisms show similar contamination patterns to total bacteria. The highest bacterial load was detected in the stunning chute (4.08 3 105 bacterial cell equivalents per cm2) but was reduced by 3 log levels after singeing and polishing (P, 0.001). It increased again significantly by a 4.73-fold change until the classification step. Levels of Campylobacter, Lactobacillus, and Pseudomonas species and of E. coli followed a similar trend but varied between 0 and 2.49 3 104 bacterial cell equivalents per cm2. Microbial levels did not vary significantly between sampled carcass areas for any analyzed taxa. Running the polishing machine with a low water amount proved to be less prone to microbial recontamination compared with a high water amount (17.07-fold change, P ¼ 0.024). In the studied slaughterhouse, slaughter of breeding sows did not produce microbiologically safe meat products (.104 cells per cm2) and the implementation of specific hazard analysis critical control point systems for the slaughter of breeding sows should be considered. A larger cohort from different abattoirs is needed to confirm our results and determine whether this is universally valid. Copyright ©, International Association for Food Protection",Scopus,2-s2.0-85072627676
English,Article,2018,"Strassl R., Schiemann M., Doberer K., Görzer I., Puchhammer-Stöckl E., Eskandary F., Kikić Ž., Gualdoni G.A., Vossen M.G., Rasoul-Rockenschaub S., Herkner H., Böhmig G.A., Bond G.",Quantification of torque teno virus viremia as a prospective biomarker for infectious disease in kidney allograft recipients,Journal of Infectious Diseases,218,8,,1191,1199,,31,10.1093/infdis/jiy306,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054994203&doi=10.1093%2finfdis%2fjiy306&partnerID=40&md5=f509a8411f46a6ad697f2864628be431,"Background. Drug-induced immunosuppression following kidney transplantation is crucial to prevent allograft rejection, but increases risk for infectious disease. Tailoring of drug dosing to prevent both rejection and infection is greatly desirable. The apathogenic and ubiquitous torque teno virus (TTV) reflects immunocompetence of the host and might be a potential candidate for immunologic monitoring. Methods. To assess TTV as an infection biomarker, virus load was prospectively quantified in peripheral blood of 169 consecutive renal allograft recipients at the Medical University Vienna. Results. Patients with infection showed higher TTV levels compared to patients without infection (4.2 × 10 8 copies/mL [interquartile range, IQR, 2.7 × 10 7 –1.9 × 10 9 ] vs 2.9 × 10 7 [IQR 1.0 × 10 6 –7.2 × 10 8 ]; P = .006). Differences in TTV load became evident almost 3 months before infection (median 77 days, IQR 19–98). Each log level of TTV copies/mL increased the odds ratio for infection by 23% (95% confidence interval 1.04–1.45; P = .014). TTV &gt;3.1 × 10 9 copies/mL corresponded to 90% sensitivity to predict infections. Logistic regression demonstrated independent association between TTV levels and infection. Conclusions. TTV quantification predicts infection after kidney transplantation and might be a potential tool to tailor immunosuppressive drug therapy. © The Author(s) 2018.",Scopus,2-s2.0-85054994203
English,Article,2018,"Yang F.-J., Shu K.-H., Chen H.-Y., Chen I.-Y., Lay F.-Y., Chuang Y.-F., Wu C.-S., Tsai W.-C., Peng Y.-S., Hsu S.-P., Chiang C.-K., Wang G., Chiu Y.-L.",Anti-cytomegalovirus IgG antibody titer is positively associated with advanced T cell differentiation and coronary artery disease in end-stage renal disease,Immunity and Ageing,15,1,15,,,,14,10.1186/s12979-018-0120-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049456407&doi=10.1186%2fs12979-018-0120-0&partnerID=40&md5=08db9a73978f307d08c10ea5e709b5d4,"Background: Accumulating evidence indicates that persistent human cytomegalovirus (HCMV) infection is associated with several health-related adverse outcomes including atherosclerosis and premature mortality in individuals with normal renal function. Patients with end-stage renal disease (ESRD) exhibit impaired immune function and thus may face higher risk of HCMV-related adverse outcomes. Whether the level of anti-HCMV immune response may be associated with the prognosis of hemodialysis patients is unknown. Results: Among 412 of the immunity in ESRD study (iESRD study) participants, 408 were HCMV seropositive and were analyzed. Compared to 57 healthy individuals, ESRD patients had higher levels of anti-HCMV IgG. In a multivariate-adjusted logistic regression model, the log level of anti-HCMV IgG was independently associated with prevalent coronary artery disease (OR = 1.93, 95% CI = 1.2~ 3.2, p = 0.01) after adjusting for age, sex, hemoglobin, diabetes, calcium phosphate product and high sensitivity C-reactive protein. Levels of anti-HCMV IgG also positively correlated with both the percentage and absolute number of terminally differentiated CD8+ and CD4+ CD45RA+ CCR7- TEMRA cells, indicating that immunosenescence may participate in the development of coronary artery disease. Conclusion: This is the first study showing that the magnitude of anti-HCMV humoral immune response positively correlates with T cell immunosenescence and coronary artery disease in ESRD patients. The impact of persistent HCMV infection should be further investigated in this special patient population. © 2018 The Author(s).",Scopus,2-s2.0-85049456407
English,Article,2018,"Cusato J., Boglione L., De Nicolò A., Favata F., Ariaudo A., Mornese Pinna S., Guido F., Avataneo V., Cantù M., Carcieri C., Cariti G., Di Perri G., D’Avolio A.",Vitamin D pathway gene polymorphisms and hepatocellular carcinoma in chronic hepatitis C-affected patients treated with new drugs,Cancer Chemotherapy and Pharmacology,81,3,,615,620,,4,10.1007/s00280-018-3520-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040788276&doi=10.1007%2fs00280-018-3520-0&partnerID=40&md5=3b3fdf6aa34618448b578c4766e2ef9d,"Purpose: Since HCV infection may lead to hepatocellular carcinoma (HCC) and vitamin D (deficiency) is related to cancer, we investigated if SNPs in genes involved in vitamin D pathway could predict HCV-related HCC presence in patients treated with new anti-HCV drugs. Methods: Patients with chronic hepatitis C and treated with direct-acting antivirals were enrolled. SNPs in VDR, CYP27B1, CYP24A1 and GC genes were assessed through real-time PCR. 258 patients were analyzed. Results: HCC was present in six patients, all taking sofosbuvir, all males and five/six had cirrhosis. HCV-RNA log levels at baseline were statistically different between patients with and without HCC. VDR FokI T > C SNP resulted associated with HCC: all the CC patients were free from HCC. An association between HCC presence and undetectable HCV-RNA at 1 month of therapy was suggested; cirrhosis was related to HCC. HCC risk factors were age, ribavirin administration, IL28Brs12979860CC and previous treatments; VDR FokICC, sex and insulin resistance were protective factors. Conclusions: These data highlighted vitamin D pathway gene SNPs and HCC relationship in the Italian population; further studies are required. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",Scopus,2-s2.0-85040788276
English,Article,2017,"Schiemann M., Puchhammer-Stöckl E., Eskandary F., Kohlbeck P., Rasoul-Rockenschaub S., Heilos A., Kozakowski N., Görzer I., Kikić Ž., Herkner H., Böhmig G.A., Bond G.",Torque Teno Virus Load-Inverse Association with Antibody-Mediated Rejection after Kidney Transplantation,Transplantation,101,2,,360,367,,41,10.1097/TP.0000000000001455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981717432&doi=10.1097%2fTP.0000000000001455&partnerID=40&md5=65fef86de9337d58e47e403e35393bc9,"Background Antibody-mediated rejection (AMR) represents one of the cardinal causes of late allograft loss after kidney transplantation, and there is great need for noninvasive tools improving early diagnosis of this rejection type. One promising strategy might be the quantification of peripheral blood DNA levels of the highly prevalent and apathogenic Torque Teno virus (TTV), which might mirror the overall level of immunosuppression and thus help determine the risk of alloimmune response. Methods To assess the association between TTV load in the peripheral blood and AMR, 715 kidney transplant recipients (median, 6.3 years posttransplantation) were subjected to a systematical cross-sectional AMR screening and, in parallel, TTV quantification. Results Eighty-six of these recipients had donor-specific antibodies and underwent protocol biopsy, AMR-positive patients (n = 46) showed only 25% of the TTV levels measured in patients without AMR (P = 0.003). In a generalized linear model, higher TTV levels were associated with a decreased risk for AMR after adjustment for potential confounders (risk ratio 0.94 per TTV log level; 95% confidence interval 0.90-0.99; P = 0.02). Conclusions Future studies will have to clarify whether longitudinal assessment of TTV load might predict AMR risk and help guide the type and intensity of immunosuppression to prevent antibody-mediated graft injury. © 2016 Wolters Kluwer Health, Inc. All rights reserved.",Scopus,2-s2.0-84981717432
English,Article,2016,"da Silva R.C., Diniz M.F.H.S., Alvim S., Vidigal P.G., Fedeli L.M.G., Barreto S.M.",Physical activity and lipid profile in the ELSA-Brasil study,Arquivos Brasileiros de Cardiologia,107,1,,10,18,,25,10.5935/abc.20160091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978984510&doi=10.5935%2fabc.20160091&partnerID=40&md5=d3a3338bca804eb72f2e7d52421e872c,"Background: Regular physical activity (PA) induces desirable changes in plasma levels of high- and low-density lipoproteins (HDL and LDL, respectively) and triglycerides (TG), important risk factors for cardiometabolic diseases. However, doubts whether intensity and duration have equivalent benefits remain. Objective: To assess the association of PA intensity and duration with HDL, LDL and TG levels. Methods: Cross-sectional study with 12,688 participants from the Brazilian Longitudinal Study of Adult Health (ELSA-Brasil) baseline, who were not on lipid-lowering medication. After adjustment for important covariates, multiple linear regression was used to assess the association of PA intensity and duration with HDL, LDL and TG (natural logarithm) levels. Results: Both moderate and vigorous PA and PA practice ≥ 150 min/week were significantly associated with higher HDL and lower TG levels. Vigorous PA was associated with lower LDL only on univariate analysis. After adjustments, moderate and vigorous PA increased mean HDL level by 0.89 mg/dL and 1.71 mg/dL, respectively, and reduced TG geometric mean by 0.98 mg/dL and 0.93 mg/dL, respectively. PA practice ≥ 150 min/week increased mean HDL level by 1.05 mg/dL, and decreased TG geometric mean by 0.98 mg/dL. Conclusion: Our findings reinforce the benefits of both PA parameters studied on HDL and TG levels, with a slight advantage for vigorous PA as compared to the recommendation based only on PA duration. © 2016, Arquivos Brasileiros de Cardiologia. All rights reserved.",Scopus,2-s2.0-84978984510
English,Article,2016,"Chien Y.-T., Hornig A., Lee C.",Soft-collinear mode for jet cross sections in soft collinear effective theory,Physical Review D,93,1,14033,,,,47,10.1103/PhysRevD.93.014033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956619072&doi=10.1103%2fPhysRevD.93.014033&partnerID=40&md5=43c27d8d54c658b0218ecb606bc9334a,"We propose the addition of a new ""soft-collinear"" mode to soft collinear effective theory (SCET) below the usual soft scale to factorize and resum logarithms of jet radii R in jet cross sections. We consider exclusive 2-jet cross sections in e+e- collisions with an energy veto Λ on additional jets. The key observation is that there are actually two pairs of energy scales whose ratio is R: the transverse momentum QR of the energetic particles inside jets and their total energy Q, and the transverse momentum ΛR of soft particles that are cut out of the jet cones and their energy Λ. The soft-collinear mode is necessary to factorize and resum logarithms of the latter hierarchy. We show how this factorization occurs in the jet thrust cross section for cone and kT-type algorithms at O(αs) and using the thrust cone algorithm at O(αs2). We identify the presence of hard-collinear, in-jet soft, global (veto) soft, and soft-collinear modes in the jet thrust cross section. We also observe here that the in-jet soft modes measured with thrust are actually the ""csoft"" modes of the theory SCET+. We dub the new theory with both csoft and soft-collinear modes ""SCET++."" We go on to explain the relation between the ""unmeasured"" jet function appearing in total exclusive jet cross sections and the hard-collinear and csoft functions in measured jet thrust cross sections. We do not resum logs that are nonglobal in origin, arising from the ratio of the scales of soft radiation whose thrust is measured at Qτ/R and of the soft-collinear radiation at 2ΛR. Their resummation would require the introduction of additional operators beyond those we consider here. The steps we outline here are a necessary part of summing logs of R that are global in nature and have not been factorized and resummed beyond leading-log level previously. © 2016 American Physical Society.",Scopus,2-s2.0-84956619072
English,Article,2015,"Al-Qadiri H., Sablani S.S., Ovissipour M., Al-Alami N., Govindan B., Rasco B.","Effect of oxygen stress on growth and survival of Clostridium perfringens, Campylobacter jejuni, and Listeria monocytogenes under different storage conditions",Journal of Food Protection,78,4,,691,697,,17,10.4315/0362-028X.JFP-14-427,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926364097&doi=10.4315%2f0362-028X.JFP-14-427&partnerID=40&md5=6352ff93a753ec80761906a77f52ee2e,"This study investigated the growth and survival of three foodborne pathogens (Clostridium perfringens, Campylobacter jejuni, and Listeria monocytogenes) in beef (7% fat) and nutrient broth under different oxygen levels. Samples were tested under anoxic (<0.5%), microoxic (6 to 8%), and oxic (20%) conditions during storage at 7°C for 14 days and at 22°C for 5 days. Two initial inoculum concentrations were used (1 and 2 log CFU per g of beef or per ml of broth). The results show that C. perfringens could grow in beef at 22°C, with an increase of approximately 5 log under anoxic conditions and a 1-log increase under microoxic conditions. However, C. perfringens could not survive in beef held at 7°C under microoxic and oxic storage conditions after 14 days. In an anoxic environment, C. perfringens survived in beef samples held at 7°C, with a 1-log reduction. A cell decline was observed at 2 log under these conditions, with no surviving cells at the 1-log level. However, the results show that C. jejuni under microoxic conditions survived with declining cell numbers. Significant increases in L. monocytogenes (5 to 7 log) were observed in beef held at 22°C for 5 days, with the lowest levels recovered under anoxic conditions. L. monocytogenes in refrigerated storage increased by a factor of 2 to 4 log. It showed the greatest growth under oxic conditions, with significant growth under anoxic conditions. These findings can be used to enhance food safety in vacuum-packed and modified atmosphere-packaged food products. Copyright ©, International Association for Food Protection.",Scopus,2-s2.0-84926364097
English,Article,2014,"Tau P., Mancon A., Mileto D., Di Nardo Stuppino S., Bottani G., Gismondo M.R., Galli M., Micheli V., Rusconi S.",Favorable therapeutic response with an antiretroviral salvage regimen in an HIV-1-positive subject infected with a CRF11-cpx virus,AIDS Research and Human Retroviruses,30,5,,480,483,,,10.1089/aid.2013.0264,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899890988&doi=10.1089%2faid.2013.0264&partnerID=40&md5=263e163207913b32082ed29ec42395d3,"HIV drug resistance still represents a crucial problem in antiretroviral therapy. We report a case of a naive patient, harboring a CRF11-cpx virus, which showed drug resistance mutations in the reverse transcriptase. A drug resistance genotyping test was performed for the pol (protease, reverse transcriptase, and integrase) and V3 regions. The initial clinical parameter results showed a 4 log level of HIV-RNA (12,090?cp/ml) and a very low CD4 + cell count (35 cells/μl). We designed an initial highly active antiretroviral therapy (HAART) regimen including lamivudine (3TC)+abacavir (ABC)+booster ritonavir (DRV/r). The virus was highly resistant to all nucleoside and nucleotide reverse transcriptase inhibitors (NRTIs) and nonnucleoside reverse transcriptase inhibitors (NNRTIs) except for ABC, tenofovir (TDF), and efavirenz (EFV) and was susceptible to all protease inhibitors (PIs) and integrase inhibitors (INIs). A salvage regimen including raltegravir (RAL)+DRV/r was started. Ten months later, the immunovirological status shows CD4+ 142/μl and HIV-RNA &lt;37?cp/ml. Our results demonstrate the effectiveness of a treatment combination that includes RAL+DRV/r in a patient infected with a complex X4-tropic CRF11-cpx virus. © Copyright 2014, Mary Ann Liebert, Inc. 2014.",Scopus,2-s2.0-84899890988
English,Article,2012,"Klu Y.A.K., Williams J.H., Phillips R.D., Chen J.",Survival of Lactobacillus rhamnosus GG as Influenced by Storage Conditions and Product Matrixes,Journal of Food Science,77,12,,M659,M663,,17,10.1111/j.1750-3841.2012.02969.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871232476&doi=10.1111%2fj.1750-3841.2012.02969.x&partnerID=40&md5=1c9267e825e46eb96b665e09896edefb,"Mortality resulting from diarrhea especially that occurs in children younger than 5 y of age ranks 3rd among all deaths caused by infectious diseases worldwide. Probiotics such as Lactobacillus rhamnosus GG are clinically shown to effectively reduce the incidence of diarrhea in children. A food substrate is one of the major factors regulating the colonization of microorganisms in human gastrointestinal tracts. Peanut butter is a nutritious, low-moisture food that could be a carrier for probiotics. In this study, we observed the influence of storage conditions and product matrixes on the survival of L. rhamnosus GG. Cells of L. rhamnosus GG were inoculated into full fat or reduced fat peanut butter at 107 CFU/g. Inoculated peanut butter was stored at 4, 25, or 37 °C for 48 wk. Samples were drawn periodically to determine the populations of L. rhamnosus GG. Results showed that there was no significant decrease in the viable counts of L. rhamnosus GG in products stored 4 °C. The survivability of L. rhamnosus GG decreased with increasing storage temperature and time. Product matrixes did not significantly affect the survival of L. rhamnosus GG except at 37 °C. Populations of L. rhamnosus GG were preserved at &gt;6 logs in products stored at 4 °C for 48 wk and at 25 °C for 23 to 27 wk. At 37 °C, the 6-log level could not be maintained for even 6 wk. The results suggest that peanut butter stored at 4 and 25 °C could serve as vehicles to deliver probiotics. © 2012 Institute of Food Technologists®.",Scopus,2-s2.0-84871232476
English,Article,2012,"Levin M.F., Snir O., Liebermann D.G., Weingarden H., Weiss P.L.",Virtual Reality Versus Conventional Treatment of Reaching Ability in Chronic Stroke: Clinical Feasibility Study,Neurology and Therapy,1,1,,1,15,,44,10.1007/s40120-012-0003-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977119181&doi=10.1007%2fs40120-012-0003-9&partnerID=40&md5=02053a2b7e4389442c70ebd9380e72d9,"Introduction: The objective of this study was to evaluate the potential of exercises performed in a 2D video-capture virtual reality (VR) training environment to improve upper limb motor ability in stroke patients compared to those performed in conventional therapy. Methods: A small sample randomized control trial, in an outpatient rehabilitation center with 12 patients with chronic stroke, aged 33-80 years, who were randomly allocated to video-capture VR therapy and conventional therapy groups. All patients participated in four clinical evaluation sessions (pre-test 1, pre-test 2, post-test, follow-up) and nine 45-minute intervention sessions over a 3-week period. Main outcomes assessed were Body Structure and Function (impairment: Fugl-Meyer Assessment [FMA]; Composite Spasticity Index [CSI]; Reaching Performance Scale for Stroke), Activity (Box and Blocks; Wolf Motor Function Test [WMFT]), and Participation (Motor Activity Log) levels of the International Classification of Functioning. Results: Improvements occurred in both groups, but more patients in the VR group improved upper limb clinical impairment (FMA, CSI) and activity scores (WMFT) and improvements occurred earlier. Patients in the VR group also reported satisfaction with the novel treatment. Conclusions: The modest advantage of VR over conventional training supports further investigation of the effect of video-capture VR or VR combined with conventional therapy in larger-scale randomized, more intense controlled studies. © 2012 The Author(s).",Scopus,2-s2.0-84977119181
English,Conference Paper,2012,"Antić D., Blagojević B.",Microbial immobilisation treatments of cattle hides - A novel approach to hide intervention strategy,CEFood 2012 - Proceedings of 6th Central European Congress on Food,,,,447,451,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926050602&partnerID=40&md5=be08a7d92ab6cbd2334ffc12d586e70b,"Cattle hide decontamination treatments have been recognized as an effective way to reduce microbial contamination of beef carcasses during slaughter and dressing in abattoirs. A range of related techniques - aimed at killing and/or removing pathogens on hides - have been considered in previously published studies. However, reported microbial reductions achievable were relatively limited: around 2-3 logs on decontaminated hides or about 1 log on resulting dressed carcasses. Consequently, to improve the effectiveness of hide treatments, a new approach - treatment to immobilize microbiota on cattle hide rather than to kill it - has been proposed. An insect-produced, natural, food-grade resin (Shellac) was evaluated as an on-hide microbiota-immobilizing agent in a laboratory models system using a sponge-swabbing microbiological sampling method. On hides spray-treated with a Shellac-in-ethanol solution, recoveries of general microbiota (total viable count of bacteria-TVC, Enterobacteriaceae counts and generic E. coli counts) were greatly reduced: up to 6.6 log10 CFU/cm2 reductions. The effects of the Shellac treatment were mainly due to immobilization of bacteria on hair by the resin, and to lesser extent due to bactericidal action of the ethanol. Furthermore, post-slaughter but pre-skinning treatment of hides with Shellac-in-ethanol solution, under practical conditions of small commercial abattoir operation, significantly reduced (up to 1.7 log) levels of general microbiota found on final beef carcasses. Overall, in both laboratory- and abattoir-based experiments, microbial reductions achievable by the Shellac-treatment of hides were superior - on both hides and beef - to those achievable by a control hide treatment using rinse-vacuum with sanitizer. Therefore, the Shellac treatment of hides can be considered as an effective alternative approach to hide decontamination strategies to improve beef safety.",Scopus,2-s2.0-84926050602
English,Article,2012,"Niedz R.P., Evens T.J., Hyndman S.E., Adkins S., Chellemi D.O.",In vitro shoot growth of Brugmansia × candida Pers,Physiology and Molecular Biology of Plants,18,1,,69,78,,4,10.1007/s12298-011-0100-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856371335&doi=10.1007%2fs12298-011-0100-8&partnerID=40&md5=511b3261ced5663aa2d711b2dbec51d4,"The objective of this study was to improve the growth of in vitro shoot cultures of Brugmansia × candida 'Creamsickle'. Several mineral nutrient experiments were conducted to determine the effect of NH 4 +, NO 3 -, K +, FeSO 4/EDTA, ZnSO 4, MnSO 4, and CuSO 4 on quality, leaf width and length, size and weight of shoot mass, and shoot number. The experiment to determine the levels of NH 4 +, NO 3 -, and K +, was conducted as a 2-component NH 4 +: K + mixture crossed by [NO 3 -] and resulted in an experimental design free of ion confounding and capable of separating the effects of proportion and concentration. The results of the NH 4 +-K +-NO 3 - experiment revealed a region in the design space where growth was significantly improved; the region generally had lower total nitrogen and lower NH 4 +:K + ratios than MS medium. The experiments to determine the appropriate levels of Fe, Zn, Mn, and Cu were conducted at six log levels ranging from 0 to 1 mM. Of the four metal salts tested, MnSO 4 had the least effect on in vitro shoot growth and its concentration was reduced from 0. 1 mM (MS level) to 0. 001 mM. CuSO 4 had large effects on in vitro shoot growth and was increased from 0. 0001 mM to 0. 001 mM. A 2-level factorial of NH 4 +-K +-NO 3 -, FeSO 4/EDTA, and ZnSO 4 was conducted and several formulations identified for their improvements of quality and growth. In addition to the changes to MnSO 4 and CuSO 4, these formulations were characterized by lower levels of NH 4 +, K +, NO 3 - and Zn, and higher levels of FeSO 4/EDTA. Overall, several nutrient formulations were identified as superior to MS medium for growth of in vitro shoot cultures of B. 'Creamsickle'. © 2011 Prof. H.S. Srivastava Foundation for Science and Society.",Scopus,2-s2.0-84856371335
English,Article,2010,"Hambach L., Aghai Z., Pool J., Kröger N., Goulmy E.",Peptide length extension skews the minor HA-1 antigen presentation toward activated dendritic cells but reduces its presentation efficiency,Journal of Immunology,185,8,,4582,4589,,6,10.4049/jimmunol.1000213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049511274&doi=10.4049%2fjimmunol.1000213&partnerID=40&md5=bf6792b721e265efca624115605e9714,"Minor histocompatibility Ags (mHags) are important targets of the graft-versus-leukemia effect after HLA-matched allogeneic stem cell transplantation. mHags are HLA-restricted polymorphic peptides expressed on normal and leukemia cells. Vaccination with hematopoiesis-restricted mHag peptides, such as HA-1, may boost the graft-versus-leukemia effect. However, some animal studies indicate that peptides exactly reflecting immunogenic T cell epitopes (short peptides [SPs]) induce tolerance that is potentially due to systemic Ag spreading. Peptide length extension (long peptides [LPs]) may optimize immune responses by restricting and prolonging Ag presentation on dendritic cells (DCs). In this study, we compared the in vitro characteristics and T cell-stimulatory capacities of a human 30-mer HA-1 LP with the 9-mer HA-1 SP. DCs presented the HA-1 LP and SP and expanded HA-1-specific cytotoxic T cell lines. As hypothesized, HA-1 LP presentation, but not SP presentation, was largely restricted to activated DCs and was nearly absent on other hematopoietic cells. However, DCs presented the HA-1 LP 2-3 log levels less efficiently than the SP. Finally, the decay of HA-1 LP and SP presentation on DCs was comparable. We conclude that HA-1 LP and SP differ in their in vitro characteristics and that only comparative clinical studies after allogeneic stem cell transplantation may reveal the optimal HA-1 vaccine. Copyright © 2010 by The American Association of Immunologists, Inc.",Scopus,2-s2.0-78049511274
Polish,Article,2010,"Neffe K., Kołozyn-Krajewska D.",Potential uses of probiotic bacteria in ripening meat products [Mozliwości zastosowania bakterii probiotycznych w dojrzewaja̧cych produktach miȩsnych],Zywnosc. Nauka. Technologia. Jakosc/Food. Science Technology. Quality,17,5,,167,177,,14,10.15193/zntj/2010/72/167-177,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650639246&doi=10.15193%2fzntj%2f2010%2f72%2f167-177&partnerID=40&md5=29a187b8e167616ac9d92e92f671b9bc,"The objective of the research conducted was to assess the potential growth and survival of selected probiotic strains in ripening meat products using an example of pork loins. The research material comprised raw pork loins and two probiotic strains Lactobacillus casei LOCK 0900 (Patent No.: P-382760) and Lactobacillus casei ŁOCK 0908 (Patent No.: P-382760). Three kinds of loin samples were analyzed in the experiment: two control samples (loins with and without 0.2% glucose additive), two samples with only Lactobacillus casei ŁOCK 0900 and ŁOCK 0908 probiotic strains added, and the samples with the probiotic strain and 0.2% glucose added. The loin samples prepared in this way ripened for 3 weeks at a temperature of 16°C. Then, the products analyzed were vacuum-packed and stored at 4° C during a period of 6 months. Three series of experiment were performed. Micro-biological analyses had to determine the count of lactic acid bacteria (LAB). They were made 3 weeks after the completion of the 3-week meat ripening process and 6 months after the storage of the vacuum-packed loins using TEMPO® (Biomerieux, France), i.e. an automated system of measuring the count of micro-organisms. It was found that the probiotic strains of Lactobacillus casei ŁOCK 0900 and ŁOCK 0908, added to the cured pork loins, grew and achieved a count of 107 log cfu/g, and in the samples with the additive of 0.2% glucose, their count was 108 log cfu/g. The count of probiotic bacteria in loins stored during a period of 6 months was by 1 to 2 log levels lower compared to the products after the ripening process, but this count was still high enough to consider those products to be probiotic.",Scopus,2-s2.0-78650639246
English,Article,2018,"Hu J., Herbohn J., Chazdon R.L., Baynes J., Wills J., Meadows J., Sohel M.S.I.",Recovery of species composition over 46 years in a logged Australian tropical forest following different intensity silvicultural treatments,Forest Ecology and Management,409,,,660,666,,21,10.1016/j.foreco.2017.11.061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037536947&doi=10.1016%2fj.foreco.2017.11.061&partnerID=40&md5=36c7f80c602ec957781978347ec17b23,"Currently, more than 400 million hectares of tropical forests worldwide are now being managed for timber production. Understanding the long-term responses of tropical forests to management practices is critical for managing tropical forests sustainably. To investigate the responses of tropical forest dynamics to different silvicultural treatment intensities, permanent plots were established in 1967 in an Australian tropical forest with four treatments: selective logging only as a control and selective logging followed by three differing intensity silvicultural treatments in 1969. We investigated changes in the number of species, species dominance, and species composition of trees (DBH ≥ 10 cm) from 1967 to 2015. Before selective logging, the number of tree species did not differ between the treatments, ranging from 70 to 75. Selective logging alone had small immediate effects on the number of species and species abundance distributions. After silvicultural treatment, the number of species in the low-intensity treatment, medium-intensity treatment and high-intensity treatment were reduced to 48, 42, and 18 respectively. The number of species in the control, low-intensity, and medium-intensity treatments recovered to their pre-logging levels within 46 years, but recovery in the high-intensity treatment was incomplete due to much greater initial species loss through silvicultural treatment. Silvicultural treatments increased species dominance, with the differences being progressively more marked as the level of treatment intensity increased. Over 46 years, tree species abundance distribution in the silvicultural treatments became more even and largely returned to pre-logging conditions, with more rapid recovery after low and intermediate silvicultural treatments. Following silvicultural treatments, species composition in the logging only, low-intensity and medium-intensity silvicultural treatments did not change markedly, whereas species composition was substantially altered by high-intensity silvicultural treatment and was subsequently distinct from the other treatments. Within 46 years following treatments, species composition in the high intensity treatment showed a recovery trajectory towards pre-logging conditions, leading to increased species compositional similarity among the four treatments. Increasing the intensity of silviculture treatment led to greater time required for recovery of species diversity, composition and compositional similarity. We recommend that high-intensity silvicultural treatments should be avoided if rapid recovery of species diversity and composition is the desired management outcome in tropical rainforests similar to those in our study area. © 2017 Elsevier B.V.",Scopus,2-s2.0-85037536947
English,Article,2011,"Ouédraogo D.-Y., Beina D., Picard N., Mortier F., Baya F., Gourlet-Fleury S.",Thinning after selective logging facilitates floristic composition recovery in a tropical rain forest of Central Africa,Forest Ecology and Management,262,12,,2176,2186,,24,10.1016/j.foreco.2011.08.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054993933&doi=10.1016%2fj.foreco.2011.08.009&partnerID=40&md5=e59718b9ad6871e23f6d35ddf1454f2c,"In the Congo Basin where most timber species are light-demanding, the low logging intensities commonly implemented (1-2 trees harvested ha-1) do not provide sufficient canopy gaps to ensure species regeneration. The regeneration of light-demanding timber species may therefore benefit from more intensive logging, or from post-harvest treatments such as thinning by poison girdling that increases light penetration. Little is known of the impact of post-harvest treatments on the floristic composition of tropical moist forests. This study therefore aimed to assess the effects of low and high selective logging (≃2.33 and 4.73 trees harvested ha-1, and ≃4.96 and 9.16m2ha-1 of basal area removed (logging+damage), respectively) - followed or not by thinning (≃21.14 trees thinned ha-1, and ≃6.57m2ha-1 of basal area removed) - on the floristic composition of a tropical moist forest in the Central African Republic, from 7 to 23 years after logging.We analyzed abundance data for 110 tree genera recorded every year for 14 years in 25 one-hectare permanent subplots. We used multivariate analysis to detect floristic variations between treatments and we assessed changes in floristic composition throughout the period. We compared floristic composition recovery between thinned and unthinned subplots, using unlogged subplots as a reference characterizing the pre-logging floristic composition.Logging and thinning had little impact on the floristic composition of the subplots as quantified 7 to 23 years later, though they did increase the proportion of pioneer species. Surprisingly, additional thinning at both logging levels failed to further distance floristic composition from that of the unlogged subplots, though it did increase disturbance intensity. Floristic composition recovery appeared to be facilitated when thinning was associated with logging. Thinning seemed to favor the growth and survival of non-pioneer species, to the detriment of pioneer species. These non-pioneer species could either be non-pioneer light demanders or shade-bearers. One explanation for this is that thinning by tree-poison girdling increased light availability without causing major damage to the forest, and thus increased the growth and survival of advance regeneration. The resulting enhanced competition then reduced the survival of pioneer species. © 2011 Elsevier B.V.",Scopus,2-s2.0-80054993933
English,Article,2003,"Taylor R.J., Regan T., Regan H., Burgman M., Bonham K.","Impacts of plantation development, harvesting schedules and rotation lengths on the rare snail Tasmaphena lamproides in northwest Tasmania: A population viability analysis",Forest Ecology and Management,175,1-3,,455,466,,6,10.1016/S0378-1127(02)00213-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037416089&doi=10.1016%2fS0378-1127%2802%2900213-X&partnerID=40&md5=636f010c4a3c92520276a351055bf34f,"Tasmaphena lamproides is a rare snail found in northwest Tasmania. The species is eliminated by logging but re-establishes a population in ∼20-year-old native forest regeneration and builds up to pre-logging levels by ∼60 years. Major plantation development is planned within the range of the species. It is unlikely that T. lamproides will reinvade areas converted to plantation. To aid the conservation of T. lamproides the managing authority planned to retain a ""biodiversity spine"" (i.e. a string of contiguous coupes (logging units) that would be regenerated to native forest rather than converted to plantation) within areas earmarked for major plantation development. A PVA was used to assess the comparative impacts of different forest management scenarios. The management scenarios modelled involved differing levels of reservation, differing levels of native forest regeneration or conversion to plantation, different rotation lengths for native forest regeneration and different temporal patterns of logging of native forest. In a forest block with a major reserve simulations indicated that the population would decline to around 50% of the original population and thereafter remain fairly stable. For a forest block earmarked for plantation development where no major reserves occurred, simulations indicated the population would undergo a steep decline to around 20% of the original population. The extent of the recovery of the population before re-harvesting of the native forest coupes depended on the extent of the biodiversity spine. The probability of reaching low absolute population levels (<one or two thousand individuals) varied with the degree of plantation development but not linearly. Increasing the length of the rotation of native forest coupes lowered the probability of reaching low absolute numbers as did increasing the spread of logging of coupes over the length of the rotation rather than logging all of the coupes over a short time span. The model allows the managing authority to design a management scenario that meets a specific quantitative goal, such as a less than 10% probability of numbers falling below 1000 individuals. It also allows them to choose between a mix of different forest management strategies that could all potentially provide the same level of conservation benefit. © 2002 Elsevier Science B.V. All rights reserved.",Scopus,2-s2.0-0037416089
English,Article,2020,"Zadeh P.N., Lümkemann N., Eichberger M., Stawarczyk B., Kollmuss M.","Differences in radiopacity, surface properties, and plaque accumulation for CAD/CAM-fabricated vs conventionally processed polymer-based temporary materials",Operative Dentistry,45,4,,407,415,,2,10.2341/19-057-L,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089305057&doi=10.2341%2f19-057-L&partnerID=40&md5=0f73f73dc6ec5bdc7d4e09066ec30d01,"Objective: To test computer-aided design/computer-aided manufacturing (CAD/CAM)-fabricated and conventionally processed polymer-based temporary materials in terms of radiopacity (RO), surface free energy (SFE), surface roughness (SR), and plaque accumulation (PA). Methods and Materials: Six temporary materials (n=10/n=10) were tested, including three CAD/CAM-fabricated (CC) materials-Art Bloc Temp (CC-ABT), Telio CAD (CC-TC), and VITA CAD Temp (CC-VCT)-and three conventionally processed (cp) materials: Integrity Multi Cure (cp-IMC), Luxatemp Automix Plus (cp-LAP), and Protemp 4 (cp-PT4). Zirconia acted as the control group (CG, n=10). RO was evaluated according to DIN EN ISO 13116. SFE was investigated using contact angle measurements. SR was measured using a profilometer. The PA tests were performed using three microorganisms: Streptococcus mutans, Actinomyces naeslundii, and Veillonella parvula. Data were analyzed using Kolmogorov-Smirnov, Kruskal-Wallis, Mann-Whitney U-, Dunn-Bonferroni, Wilcoxon, Levene, and Pearson tests and one-way analysis of variance with post hoc Scheffé test (a=0.05). Results: No radiopacity was observed for any CC material or cp-PT4. CG showed the highest RO, while no differences between cp-IMC and cp-LAP were found. CG showed lower SFE compared to all polymer temporary materials, except in the case of CC-TC. cp-LAP and cp-IMC presented higher SFE than did CC-TC and CG. CC-ABT presented lower initial SR values compared to cp-PT4 and cp-LAP. For cp-LAP, a higher initial SR was measured than for all CAD/CAM materials and cp-IMC. All specimens showed a certain amount of PA after the incubation period. A naeslundii and V parvula resulted in comparable PA values, whereas the values for S mutans were lower by one log level. CAD/CAM materials showed superior results for SR, SFE, and PA, whereas all materials lacked RO. © 2020 Indiana University School of Dentistry. All rights reserved.",Scopus,2-s2.0-85089305057
English,Article,2020,"Li H., Shang W., Adams B., Sayagh M., Hassan A.E.",A Qualitative Study of the Benefits and Costs of Logging from Developers&#x0027; Perspectives,IEEE Transactions on Software Engineering,,,,,,,4,10.1109/TSE.2020.2970422,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079463383&doi=10.1109%2fTSE.2020.2970422&partnerID=40&md5=ea166e56a0bce8c0040f66e1bc86776f,"Software developers insert logging statements in their source code to collect important runtime information of software systems. In practice, logging appropriately is a challenge for developers. Prior studies aimed to improve logging by proactively inserting logging statements in certain code snippets or by learning where to log from existing logging code. However, there exists no work that systematically studies developers&#x0027; logging considerations, i.e., the benefits and costs of logging from developers&#x0027; perspectives. Without understanding developers&#x0027; logging considerations, automated approaches for logging decisions are based primarily on researchers&#x0027; intuition which may not be convincing to developers. In order to fill the gap between developers&#x0027; logging considerations and researchers&#x0027; intuition, we performed a qualitative study that combines a survey of 66 developers and a case study of 223 logging-related issue reports. The findings of our qualitative study draw a comprehensive picture of the benefits and costs of logging from developers&#x0027; perspectives. We observe that developers consider a wide range of logging benefits and costs, while most of the uncovered benefits and costs have never been observed nor discussed in prior work. We also observe that developers use ad hoc strategies to balance the benefits and costs of logging. Developers need to be fully aware of the benefits and costs of logging, in order to better benefit from logging (e.g., leveraging logging to enable users to solve problems by themselves) and avoid unnecessary negative impact (e.g., exposing users&#x0027; sensitive information). Future research needs to consider such a wide range of logging benefits and costs when developing automated logging strategies. Our findings also inspire opportunities for researchers and logging library providers to help developers balance the benefits and costs of logging, for example, to support different log levels for different parts of a logging statement, or to help developers estimate and reduce the negative impact of logging statements. IEEE",Scopus,2-s2.0-85079463383
English,Article,2011,"Hadrich J.C., Wolf C.A.",Citizen complaints and environmental regulation of Michigan livestock operations,Journal of Animal Science,89,1,,277,286,,2,10.2527/jas.2010-3257,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650979328&doi=10.2527%2fjas.2010-3257&partnerID=40&md5=15b3f2b7eddbc9f37224847e6c5e61e7,"Citizen environmental complaints filed against agricultural producers in Michigan were examined to determine farm and community factors influencing citizen complaints and the subsequent response of the farmer. Secondary citizen environmental complaint data were obtained from the Michigan Department of Agriculture from 1998 to 2007 with 1,289 observations. Citizen complaints were grouped into 5 categories: odor, surface water, ground water, combination, and other complaints. Complaints were further classified as nonverified or verified where verified meant that the inspected farm was not complying with relevant generally accepted agricultural and management practices. These data were used to examine how farm characteristics affected the likelihood of a verified complaint. Odor and surface water complaints accounted for 75% of all complaints. A probit regression analysis was used to estimate the probability of a verified complaint as a function of complaint type, farm characteristics, county characteristics, and seasonal factors. Results from the probit regression analysis revealed that larger operations, poultry, and hog farms received more nonverified complaints than other livestock farms. Surface water issues were 17% more likely to be verified complaints compared with odor issues, of which the surface water complaints often originated from sources other than neighbors. In contrast, odor issues were more likely to result from accepted management practices requiring no mitigation. Farms that received a verified citizen complaint were required to mitigate the complaint by implementing corrective practices. A log-level (log Y) regression was used to evaluate how farm characteristics influenced the cost to implement corrective practices on those farms receiving a verified citizen complaint. Costs to implement corrective practices to mitigate verified complaints were greatest for dairy operations and surface water complaints. Corrective practices required to mitigate a surface water complaint were predicted to cost 46% more than an odor complaint with an estimated average cost of $7,442. The most expensive practices were associated with manure incorporation, stream bank fencing, and controlling runoff. ©2011 American Society of Animal Science.",Scopus,2-s2.0-78650979328
English,Article,2005,"Richter M.Y., Jakobsen H., Haeuw J.-F., Power U.F., Jonsdottir I.",Protective levels of polysaccharide-specific maternal antibodies may enhance the immune response elicited by pneumococcal conjugates in neonatal and infant mice,Infection and Immunity,73,2,,956,964,,10,10.1128/IAI.73.2.956-964.2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12844261149&doi=10.1128%2fIAI.73.2.956-964.2005&partnerID=40&md5=e9895a0854ecdd6eb83aac4c29098de2,"Maternal antibodies (MatAbs) may protect the offspring against infections but may also interfere with their immune responses to vaccination. We have previously shown that maternal immunization with pneumococcal polysaccharides (PPS) conjugated to tetanus protein (Pnc-TT) protected the offspring against infections caused by three important pediatric serotypes. To study the influence of MatAb on the immune response to Pnc-TT early in life, adult female mice were immunized twice with Pnc-TT of serotype 1 (Pnc1-TT), and their offspring received Pnc1-TT subcutaneously three times at 3-week intervals starting at 1 week (neonatal) or 3 weeks (infant) of age. High levels of PPS-1-specific MatAb (>3 log) in offspring of Pnc1-TT-immunized dams completely inhibited their anti-PPS-1 response elicited by Pnc1-TT. In contrast, low or moderate (∼1 to 2 log) levels of MatAb did not interfere with and even enhanced the immune response of the offspring, and a booster response to a second Pnc1-TT dose was observed. Carrier-specific MatAbs had little effect on the response of offspring to the conjugate. All Pnc1-TT-immunized offspring were protected against pneumococcal bacteremia and had reduced lung infection. These results demonstrate that in the presence of MatAb, Pnc1-TT may elicit a protective PPS-1-specific antibody response and prime for PPS-1-specific memory in young offspring. Importantly, low or moderate levels of PPS-1-specific MatAb not only provided protection against pneumococcal infections but also enhanced the immune response elicited by Pnc1-TT in neonatal and infant mice. This murine model will be used to develop novel strategies combining maternal and neonatal immunization to protect against infections caused by encapsulated bacteria in early life.",Scopus,2-s2.0-12844261149
English,Review,2003,"Chiorescu S., Grönlund A.",The visual grading system for Scots pine logs in relation to the quality of sideboards produced,Forest Products Journal,53,1,,53,60,,6,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037269563&partnerID=40&md5=9dd6918fd9445b615460c1724c21063d,"Presorting logs in the sawmill's log yard is one of the strategies used by the Swedish sawmilling industry in order to maximize profits. Almost 96 percent of Swedish sawmills perform the presorting of logs according to their dimension, most commonly by the top diameter. Only a very small number of sawmills use log grade information as a sorting criterion. However, the grade information at individual log level, together with the estimated volume, is always used for the calculation of log payment. The quality assessment of the log is done on a visual basis, i.e., the measurer grades the log on what he/she sees on the outside of the log. This judging should correspond to the expected quality of the Centerboards produced after sawing. Current studies have only related the centerboards' quality to the log quality grade arrived at by visual inspection. In view of the economic importance of the sideboards in the lumber yield and in the entire functioning of a sawmill, a real need for information on the recovery of sideboards in relation to log and centerboard quality is now arising and becoming a key issue. In this study, with the help of the entire Swedish Pine Stem Bank material, the relationship between the sideboard's quality and the log's quality and the relationship between the quality of sideboards and of centerboards were studied. Results show that sideboard quality cannot be predicted from the visual quality assessment of the logs. Nevertheless, fairly good correlation exists between sideboard quality and the quality of the corresponding centerboards. A financial analysis focusing on the relationship between the commercial values of the sawlogs and the corresponding lumber output for the Swedish Pine Stem Bank material is also included in this study. The study shows that the actual Swedish log pricing system should be improved because the commercial values of the logs are not in accord with the real values of the lumber produced.",Scopus,2-s2.0-0037269563
English,Article,1999,"Bryan P.J., Steffan R.J., DePaola A., Foster J.W., Bej A.K.",Adaptive response to cold temperatures in Vibrio vulnificus,Current Microbiology,38,3,,168,175,,50,10.1007/PL00006782,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032941986&doi=10.1007%2fPL00006782&partnerID=40&md5=343d1717349180a2aa4819d5a2daa74f,"The effectiveness of rapid chilling or freezing of oysters to reduce Vibrio vulnificus levels in shellfish may be compromised by product handling procedures that permit cold adaptation. When a V. vulnificus culture was shifted from 35°C to 6°C conditions, it underwent transition to a non- culturable state. Cells adapted to 15°C prior to change to 6°C condition, however, remain viable and culturable. In addition, cultures adapted to 15°C were able to survive better upon freezing at -78°C compared with cultures frozen directly from 35°C. Inhibition of protein synthesis by addition of chloramphenicol in a V. vulnificus culture immediately prior to the exposure to the adaptive temperature eliminated inducible cold tolerance. These results suggest that cold-adaptive 'protective' proteins may enhance survival and tolerance at cold temperatures. In addition, removal of iron from the growth medium by adding 2,2'-Dipyridyl prior to cold adaptation decreased the viability by approximately 2 logarithm levels. This suggests that iron plays an important role in adaptation at cold temperatures. Analysis of total cellular proteins on an SDS polyacrylamide gel electrophoresis, labeled with 35S-methionine during exposure at 15°C, showed elevated expressions of a 6-kDa and a 40-kDa protein and decreased expression of an 80-kDa protein. These results suggest that, for V. vulnificus, survival and tolerance at cold temperatures could be due to the expression of cold-adaptive proteins other than previously documented major cold shock proteins such as CS7.4 and CsdA. In this study, for the first time we have shown that exposure to an intermediate cold temperature (15°C) causes a cold adaptive response, helping this pathogen remain in culturable state when exposed to a much colder temperature (6°C). This adaptive nature to cold temperatures could be important for shellfish industry efforts to reduce the risk of V. vulnificus infection from consuming raw oysters.",Scopus,2-s2.0-0032941986
English,Article,2006,"Strittholt J.R., Dellasala D.A., Jiang H.",Status of mature and old-growth forests in the pacific northwest,Conservation Biology,20,2,,363,374,,45,10.1111/j.1523-1739.2006.00384.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645098454&doi=10.1111%2fj.1523-1739.2006.00384.x&partnerID=40&md5=3c8892e0ac83df56e9ef0d06ace6313d,"Nearly 10 million ha of federal lands in the Pacific Northwest have been managed under the Northwest Forest Plan since 1994. The plan reduced logging levels by 80%; only recently, however, have inventories on status and condition of mature and old-growth forests become available. Our objectives were to (1) determine the areal extent of old (>150 years) and mature (50-150 years) conifer forests based on 2000 Landsat 7 ETM+ imagery, (2) examine levels of protection, (3) determine the degree of additional protection afforded to old and mature conifer forests if late-successional reserves (LSRs) and inventoried roadless areas (IRAs) were fully protected, and (4) review management options to achieve greater protection of older forests. The historical extent of old-growth forest in the Pacific Northwest was roughly two-thirds (16,672,976 ha) of the total land area. Since the time of European settlement, approximately 72% of the original old-growth conifer forest has been lost, largely through logging and other developments. Of the remaining old growth, the Central and Southern Cascades and Klamath-Siskiyou account for nearly half. Mature conifer area (4,758,596 ha) nearly equaled the amount of old conifer. More than 78% of the old growth and 50% of mature forest were located on public lands. Approximately one-quarter (1,201,622 ha) of the old-growth conifer (or 7% of the historical old-growth area) was classified as GAP status 1 (strictly protected) or GAP status 2 (moderately protected). The total area of LSRs was slightly more than 3 million ha, approximately 36% (1,073,299 ha) of which contained old-growth conifer forest. Combined old and mature conifer within LSRs was approximately 59% of the total LSR area. The total amount of IRA for the Pacific Northwest was approximately 1,563,370 ha; of this, 526,912 ha (34%) was old growth. The combined area of old-growth conifer forest accounted for by protected areas (GAP 1 and 2), LSRs, and IRAs was 2,401,780 ha, which accounts for 66% of the old-growth conifer forests on public land, 51% of the old conifer in the region, and 14% of the amount that occurred historically. Outside these land designations, an additional 1,240,271 ha of old growth are on other public land and another 1,023,392 ha are on private lands throughout the Pacific Northwest. Our results indicate the need to periodically monitor status and condition of older forests and strengthen protections of old growth in the region. © 2006 Society for Conservation Biology.",Scopus,2-s2.0-33645098454
English,Article,2016,"Lauritsen J.L., Rezey M.L., Heimer K.","When Choice of Data Matters: Analyses of U.S. Crime Trends, 1973–2012",Journal of Quantitative Criminology,32,3,,335,355,,33,10.1007/s10940-015-9277-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952015007&doi=10.1007%2fs10940-015-9277-2&partnerID=40&md5=0cab64f3b03cfcbc4f2c9b02a0532eff,"Objectives: This study uses UCR and NCVS crime data to assess which data source appears to be more valid for analyses of long-term trends in crime. The relationships between UCR and NCVS trends in violence and six factors from prior research are estimated to illustrate the impact of data choice on findings about potential sources of changes in crime over time. Methods: Crime-specific data from the UCR and NCVS for the period 1973–2012 are compared to each other using a variety of correlational techniques to assess correspondence in the trends, and to UCR homicide data which have been shown to be externally valid in comparison with other mortality records. Log-level trend correlations are used to describe the associations between trends in violence, homicide and the potential explanatory factors. Results: Although long-term trends in robbery, burglary and motor vehicle theft in the UCR and NCVS are similar, this is not the case for rape, aggravated assault, or a summary measure of serious violence. NCVS trends in serious violence are more highly correlated with homicide data than are UCR trends suggesting that the NCVS is a more valid indicator of long-term trends in violence for crimes other than robbery. This is largely due to differences during the early part of the time series for aggravated assault and rape when the UCR data exhibited consistent increases in the rates in contrast to general declines in the NCVS. Choice of data does affect conclusions about the relationships between hypothesized explanatory factors and serious violence. Most notably, the reported association between trends in levels of gasoline lead exposure and serious violence is likely to be an artifact associated with the reliance on UCR data, as it is not found when NCVS or homicide trend data are used. Conclusions: The weight of the evidence suggests that NCVS data represent more valid indicators of the trends in rape, aggravated assault and serious violence from 1973 to the mid-1980s. Studies of national trends in serious violence that include the 1973 to mid-1980s period should rely on NCVS and homicide data for analyses of the covariates of violent crime trends. © 2016, Springer Science+Business Media New York.",Scopus,2-s2.0-84952015007
English,Article,2015,"Juneja V.K., Cadavez V., Gonzales-Barron U., Mukhopadhyay S.","Modelling the effect of pH, Sodium chloride and sodium pyrophosphate on the thermal resistance of Escherichia coli O157: H7 in ground beef",Food Research International,69,,,289,304,,8,10.1016/j.foodres.2014.11.050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921500041&doi=10.1016%2fj.foodres.2014.11.050&partnerID=40&md5=173fc2f1931f7b9902b0fe543ee7e75b,"The objective of this study was to assess the combined effects of temperature, pH, sodium chloride (NaCl), and sodium pyrophosphate (SPP) on the heat resistance of Escherichia coli O157:H7 in minced beef meat. A fractional factorial design consisted of four internal temperatures (55.0, 57.5, 60.0 and 62.5. °C), five concentrations of NaCl (0.0, 1.5, 3.0, 4.5 and 6.0. wt/wt.%) and SPP (0.0, 0.1, 0.15, 0.2 and 0.3. wt/wt.%), and five levels of pH (4.0, 5.0, 6.0, 7.0 and 8.0). The 38 variable combinations were replicated twice to provide a total of 76 survivor curves, which were modelled by a modified three-parameter Weibull function as primary model. The polynomial secondary models, developed to estimate the time to achieve a 3-log and a 5-log reduction, enabled the estimation of critical pH, NaCl and SPP concentrations, which are values at which the thermo-tolerance of E. coli O157:H7 reaches it maximum. The addition up to a certain critical concentration of NaCl (~. 2.7-4.7%) or SPP (~. 0.16%) acts independently to increase the heat resistance of E. coli O157:H7. Beyond such critical concentrations, the thermo-resistance of E. coli O157:H7 will progressively diminish. A similar pattern was found for pH with a critical value between 6.0 and 6.7, depending upon temperature and NaCl concentration. A mixed-effects omnibus regression model further revealed that the acidity of the matrix and NaCl concentration had a greater impact on the inactivation kinetics of E. coli O157:H7 in minced beef than SPP, and both are responsible for the concavity/convexity of the curves. When pH, SPP or NaCl concentration is far above or below from its critical value, the temperatures needed to reduce E. coli O157:H7 up to a certain log level are much lower than those required when any other environmental condition is at its critical value. Meat processors can use the model to design lethality treatments in order to achieve specific log reductions of E. coli O157:H7 in ready-to-eat beef products. © 2014 Elsevier Ltd.",Scopus,2-s2.0-84921500041
English,Article,2012,"Farrell R., Innes T.C., Harwood C.E.",Sorting eucalyptus nitens plantation logs using acoustic wave velocity,Australian Forestry,75,1,,22,30,,9,10.1080/00049158.2012.10676382,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859237440&doi=10.1080%2f00049158.2012.10676382&partnerID=40&md5=d715df7faf67a81208eaccb91f6f0ccb,"Acoustic wave velocity (AWV) was evaluated as a predictor of wood stiffness in plantation-grown Eucalyptus nitens. To represent the resource currently being directed to the structural timber market, five harvest sites were selected in NE and NW Tasmania spanning two age classes (8 y and 13–15 y) and three productivity classes. A total of 155 sawlogs, 5.5 m long, were cut from 137 harvested trees. AWV was measured in both standing trees and in sawlogs in the mill yard. Disk samples were collected from the butt end of each log to determine green and basic density. Logs were then sawn, and structural boards dried and finished according to commercial processing practice. One sample board per log was then tested for stiffness, bending and shear strength and hardness. Sites differed significantly (P &lt; 0.001) in standing tree AWV, log AWV and all wood properties of the butt logs, with the 13–15-y age class displaying higher AWV, wood basic density, stiffness and hardness than the 8-y age class. Log AWV2 explained 54% of the variance in board static modulus of elasticity (MoEstat) for the pooled data. At the age class level, 47% of the variance was explained for the 13–15-y logs, but the correlation was much poorer—explaining only 6% of variance in MoEstat—for the subset of 56 logs from the two 8-y-old sites. Dynamic MOE (MoEdyn, the product of green density and log AWV2), gave useful predictions of MoEstat for the younger age class. MoEdyn calculated using log AWV2 explained 56% of the variation in MoEstat, facilitating the segregation of logs into three stiffness classes. Tree AWV2 explained 40% of the variation in MoEstat for the pooled data, similarly enabling segregation of boards into three stiffness classes. Relative to segregation at the log level, a similar percentage of low-stiffness material was identified; but ability to identify higher-stiffness material was reduced. A significant (P &lt; 0.01) positive correlation of 0.30 was found between board stiffness and hardness, indicating that segregation based on increasing stiffness would also improve hardness. © 2012 Taylor &amp; Francis Group, LLC.",Scopus,2-s2.0-84859237440
English,Article,2011,"Carroll T.D., Matzinger S.R., Barro M., Fritts L., McChesney M.B., Miller C.J., Johnston R.E.",Alphavirus replicon-based adjuvants enhance the immunogenicity and effectiveness of Fluzone® in rhesus macaques,Vaccine,29,5,,931,940,,25,10.1016/j.vaccine.2010.11.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751580569&doi=10.1016%2fj.vaccine.2010.11.024&partnerID=40&md5=9f3f210d4e5c967b0ce46801d1f1921a,"Venezuelan equine encephalitis virus replicon particles (VRP) without a transgene (null VRP) have been used to adjuvant effective humoral [1], cellular [2], and mucosal [3] immune responses in mice. To assess the adjuvant activity of null VRP in the context of a licensed inactivated influenza virus vaccine, rhesus monkeys were immunized with Fluzone® alone or Fluzone® mixed with null VRP and then challenged with a human seasonal influenza isolate, A/Memphis/7/2001 (H1N1). Compared to Fluzone® alone, Fluzone®+null VRP immunized animals had stronger influenza-specific CD4+ T cell responses (4.4 fold) with significantly higher levels of virus-specific IFN-γ (7.6 fold) and IL-2 (5.3 fold) producing CD4+ T cells. Fluzone®+null VRP immunized animals also had significantly higher plasma anti-influenza IgG (p&lt;0.0001, 1.3 log) and IgA (p&lt;0.05, 1.2 log) levels. In fact, the mean plasma anti-influenza IgG titers after one Fluzone®+null VRP immunization was 1.2 log greater (p&lt;0.04) than after two immunizations with Fluzone® alone. After virus challenge, only Fluzone®+null VRP immunized monkeys had a significantly lower level of viral replication (p&lt;0.001) relative to the unimmunized control animals. Although little anti-influenza antibody was detected in the respiratory secretions after immunization, strong anamnestic anti-influenza IgG and IgA responses were present in secretions of the Fluzone®+null VRP immunized monkeys immediately after challenge. There were significant inverse correlations between influenza RNA levels in tracheal lavages and plasma anti-influenza HI and IgG anti-influenza antibody titers prior to challenge. These results demonstrate that null VRP dramatically improve both the immunogenicity and protection elicited by a licensed inactivated influenza vaccine. © 2010 Elsevier Ltd.",Scopus,2-s2.0-78751580569
English,Article,2005,"Agardh C.-D., Cilio C.M., Lethagen A., Lynch K., Leslie R.D.G., Palmér M., Harris R.A., Robertson J.A., Lernmark A.",Clinical evidence for the safety of GAD65 immunomodulation in adult-onset autoimmune diabetes,Journal of Diabetes and its Complications,19,4,,238,246,,169,10.1016/j.jdiacomp.2004.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21344461521&doi=10.1016%2fj.jdiacomp.2004.12.003&partnerID=40&md5=336a287bd0c005f4f175baef80719e01,"The purpose of this Phase II study was to evaluate if alum-formulated human recombinant GAD65 is safe and does not compromise beta cell function. The study was conducted as a randomized, double blind, placebo-controlled, dose-escalation clinical trial in a total of 47 Latent Autoimmune Diabetes in Adults (LADA) patients who received either placebo or 4, 20, 100, or 500 μg Diamyd subcutaneously at Weeks 1 and 4. Safety evaluations, including neurology, beta cell function tests, diabetes status assessment, hematology, biochemistry, and cellular and humoral immunological markers, were repeatedly assessed over 24 weeks. None of the patients had significant study-related adverse events (AE). Fasting c-peptide levels at 24 weeks were increased compared with placebo (P=.0015) in the 20 μg but not in the other dose groups. In addition, both fasting (P=.0081) and stimulated (P=.0236) c-peptide levels increased from baseline to 24 weeks in the 20 μg dose group. GADA log levels clearly increased (P=.0002) in response to 500 μg Diamyd. The CD4 +CD25+/CD4+CD25- cell ratio increased (P=.0128) at 24 weeks in the 20 μg group. No sudden increase in HbA1c or plasma glucose or decrease in beta cell function was observed in any of the dose groups. These positive findings for clinical safety further support the clinical development of Diamyd as a therapeutic to prevent autoimmune diabetes. © 2005 Elsevier Inc. All rights reserved.",Scopus,2-s2.0-21344461521
English,Article,1989,"Formelli F., Carsana R., Costa A., Buranelli F., Campa T., Dossena G., Magni A., Pizzichetta M.",Plasma Retinol Level Reduction by the Synthetic Retinoid Fenretinide: A One Year Follow-up Study of Breast Cancer Patients,Cancer Research,49,21,,6149,6152,,130,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024452570&partnerID=40&md5=d550e2125bdc4bacb9e70e920d0be15d,"Fenretinide (HPR) is a synthetic retinoid which has been shown to cause a reduction in the incidence of carcinogen-induced epithelial tumors in experimental animals, and it has been chosen to be tested as a chemopreventive agent in humans. A study on plasma concentrations of HPR, of its metabolite N-(4-methoxyphenyl)retinamide (MPR), and on its effects on endogenous retinol was performed in groups of 14 to 18 breast cancer patients who received p.o. daily doses of placebo or 100, 200, and 300 mg of HPR for 6 mo and subsequently 200 mg for an additional 6 mo. After the first 5 mo of treatment, there was a linear relationship between doses of HPR administered and HPR, MPR, and retinol levels. HPR and MPR levels increased with the increase in dose, whereas retinol levels decreased, and the reduction was statistically significant compared with the placebo group after all the doses tested. Plasma retinol binding proteins (RBP) decreased proportionally to retinol (r = 0.96). The effect of HPR on retinol and RBP occurred early, since retinol and RBP levels had already been decreased, compared with the initial levels, by 38% and 26%, respectively, 24 h after a 200-mg HPR dose. After 12 mo of treatment, in patients treated with 200 mg daily, the dose chosen for a chemopreventive trial, HPR and retinol levels were similar to those found at 5 mo, suggesting no drug accumulation and no further retinol reduction, whereas MPR levels were higher. Following interruption of treatment, as HPR decreased, retinol increased with a linear relationship between log levels (r = 0.78); after about 50 days, HPR was present in trace amounts, and retinol levels were in the range of those of the placebo group. These data show that HPR treatment lowers retinol and RBP plasma concentrations. This effect is related to HPR levels and is reversible on cessation of HPR administration. © 1989, American Association for Cancer Research. All rights reserved.",Scopus,2-s2.0-0024452570
English,Article,2017,"Roopsind A., Wortel V., Hanoeman W., Putz F.E.",Quantifying uncertainty about forest recovery 32-years after selective logging in Suriname,Forest Ecology and Management,391,,,246,255,,16,10.1016/j.foreco.2017.02.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013250241&doi=10.1016%2fj.foreco.2017.02.026&partnerID=40&md5=03434551a7829da9a6e0306e040160ed,"The inclusion of managed tropical forests in climate change mitigation has made it important to find the sustainable sweet-spot for timber production, carbon retention, and the quick recovery of both. Here we focus on recovery of aboveground carbon and timber stocks over the first 32 years after selective logging with the CELOS Harvest System in Suriname. Our data are from twelve 1-ha permanent sample plots in which growth, survival, and recruitment of trees ≥15 cm diameter were monitored between 1978 and 2012. We evaluate plot-level changes in basal area, stem density, aboveground carbon, and timber stock in response to average timber harvests of 15, 23, and 46 m3 ha−1. We use a linear mixed-effects model in a Bayesian framework to quantify recovery time for aboveground carbon and timber stock, as well as annualized increments for both. Our statistical models accounted for the uncertainty associated with the height and biomass allometries used to estimate aboveground carbon and increased precision of annualized aboveground carbon increments by including data from forty-one plots located elsewhere on the Guiana Shield. The probabilities of aboveground carbon recovery to pre-logging levels 32 years after harvests of 15, 23 and 46 m3 ha−1 were 45%, 40%, and 24%, respectively. Net aboveground carbon increment for logged forests across all harvest intensities was 0.64 Mg C ha−1 yr−1, more than twice the rate observed in unlogged forests (0.26 Mg C ha−1 yr−1). The probabilities of timber stock recovery at the end of the 32-year period were highest after harvest intensities of 15 and 23 m3 ha−1 (with 80% probability) and lowest after the harvest of 46 m3 ha−1 (with 70% probability). Timber stock recovery across all harvest intensities was driven primarily by residual tree growth. Application of the legal cutting limit of 25 m3 ha−1 will require more than 70 and 40 years to recover aboveground carbon and timber stocks, respectively, with 90% probability. Based on the low recruitment rates of the twelve species harvested, the 25 year cutting cycle currently implemented in Suriname is too short for long-term timber stock sustainability. We highlight the value of propagating uncertainty from individual tree measurements to statistical predictions of carbon stock recovery. Ultimately, our study reveals the trade-offs that must be made between timber and carbon services as well as the opportunity to use carbon payments to enable longer cutting rotations to capture carbon from forest regrowth. © 2017 Elsevier B.V.",Scopus,2-s2.0-85013250241
English,Article,2017,"de Avila A.L., Schwartz G., Ruschel A.R., Lopes J.D.C., Silva J.N.M., Carvalho J.O.P.D., Dormann C.F., Mazzei L., Soares M.H.M., Bauhus J.","Recruitment, growth and recovery of commercial tree species over 30 years following logging and thinning in a tropical rain forest",Forest Ecology and Management,385,,,225,235,,45,10.1016/j.foreco.2016.11.039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005992411&doi=10.1016%2fj.foreco.2016.11.039&partnerID=40&md5=454e50ca0af6bc2d00a7c312ea265a46,"Sustainable production of timber from commercial species across felling cycles is a core challenge for tropical silviculture. In this study, we analysed how the intensity and type (harvesting and thinning) of silvicultural interventions affect: (a) recruitment of small stems (5 cm ⩽ DBH &lt; 15 cm), (b) increment of future crop trees (15 cm ⩽ DBH &lt; 50 cm) and (c) recovery of harvestable growing stocks (DBH ⩾ 50 cm) of 52 commercial timber species in the Tapajós National Forest, Brazil. Intervention intensities comprised logging (on average 61 m3 ha−1) and associated damage to remaining trees (1982) and thinning (refinement) to reduce basal area at the stand level (1993/1994). These interventions together resulted in a gradient of reduction in basal-area from 19 to 53% relative to pre-logging stocks. Trees (DBH ⩾ 5 cm) were measured on eight occasions in 41 permanent sample plots of 0.25 ha each. The dynamics were analysed at the stand level over 30 years and compared among treatments (including unlogged forest) and to pre-logging stands. Recruitment and growth temporarily increased following interventions and recovery of harvestable growing stock decreased with intervention intensity. Harvesting substantially increased recruitment of small stems relative to the unlogged forest, but recruitment rates decreased over time and did not increase following thinning. Gross increment of future crop trees was higher in logged than in unlogged forest and increased over time with high intensity of follow-up thinning, where it remained significantly higher than in control plots over time. Increased recruitment rates and volume increments were mainly driven by long-lived pioneer species, changing the composition of the growing stock. In 2012, recovery of harvestable growing stock of the 22 species harvested in 1982 varied between 19% and 57% in logged treatments relative to pre-logging levels. When considering an additional group of 30 species that were not harvested in the permanent sample plots but are now potentially commercial, relative recovery increased enough to support a second harvest under the present regulations (maximum harvest of 30 m3 ha−1), except for treatment with high thinning intensity where stocks were still less than 30% relative to pre-harvest levels. In contrast, light and medium thinning intensity promoted recovery of harvestable growing stock. These findings indicate that intensive thinning should be avoided and silvicultural interventions oriented towards future crop trees of target species should be adopted. This may enhance recovery and reduce unintended changes in composition of the commercial growing stock. © 2016 Elsevier B.V.",Scopus,2-s2.0-85005992411
English,Article,2006,"Kariuki M., Kooyman R.M., Smith R.G.B., Wardell-Johnson G., Vanclay J.K.","Regeneration changes in tree species abundance, diversity and structure in logged and unlogged subtropical rainforest over a 36-year period",Forest Ecology and Management,236,2-3,,162,176,,32,10.1016/j.foreco.2006.09.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751329949&doi=10.1016%2fj.foreco.2006.09.021&partnerID=40&md5=71c0e17dc2b2203fd5df66f18aba5906,"The long-term effects of logging treatments on rainforest regeneration are difficult to quantify due to compounding interactions with natural dynamics, site characteristics and tree species. The aim of this study was to examine regeneration differences over a 36-year period in stands subjected to various levels of disturbance ranging from natural, through an increasing intensity of individual tree removal to intensive logging. Multivariate and univariate analyses of trees ≥10 cm diameter at 1.3 m above the ground (dbh) showed that regeneration responses were generally correlated with disturbance gradient. In the undisturbed controls there were gradual changes that had no significant effects on tree species richness and diversity, stem density, or diameter distribution. Gradual changes were also observed during the early stages of regeneration following logging. However, in logged sites changes in tree species richness and diversity, stem density and diameter distribution became more rapid with time, and significant changes were observed. Similar regeneration events across site and disturbance levels resulted in three identifiable stages. In the first stage, lasting about 10 years, stem density of abundant shade tolerant trees decreased with no discernable changes in tree species richness. In the second stage, also lasting about 10 years, tree species richness and diversity, as well as stem density decreased to minima due to localised species turnover and net mortality. In the third stage, recruitment surpassed mortality and reversed the net loss of both species and stems, as tree species assemblages began to return to pre-disturbance levels. Sites subjected to individual tree selective logging returned to their pre-logging states in all aspects within 35 years of logging, but diameter distribution of trees ≥40 cm dbh showed low density compared to that observed in the controls. After 15-30 years, sites subjected to more intensive logging returned to their pre-logging levels of stem densities, species abundance and richness, but after 35-44 years of regeneration this sites had low species diversity and high densities of both the small sized stems and shade intolerant tree species. More intensively logged sites also had a low density of shade tolerant tree species compared to the controls. This suggests that the restoration of forest structure takes considerably longer than the restoration of tree species richness and abundance following logging in these forests. A high rate of stand basal area growth and a modest diameter distribution of lager trees ≥40 cm dbh were observed in moderate tree selection logging. This indicates high timber production potential at moderate tree selection rate in this type of forest. However, if the stem size distribution of larger trees is to be maintained, a logging cycle longer than 50 years is necessary. © 2006 Elsevier B.V. All rights reserved.",Scopus,2-s2.0-33751329949
English,Conference Paper,2016,"Menon P., Ali A., Guergour M.N., Ebeid M., Jeong J., Darous C.","Petrophysical modeling based on porosity partitioning, a case study in thamama formation",Society of Petroleum Engineers - Abu Dhabi International Petroleum Exhibition and Conference 2016,2016-January,,,,,,,10.2118/183391-ms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044259582&doi=10.2118%2f183391-ms&partnerID=40&md5=6cf5944d6bc4cff702464acb7dea796c,"What makes a petrophysical model robust is the understanding of the relationships between its petrophysical properties, typically porosity, permeability, and saturation. In carbonates, and particularly in limestones, the pore geometry and connectivity are often not uniform which can lead to lack of correlation between porosity, permeability, and saturation. This study explains how the combination of core analyses, Nuclear Magnetic Resonance (NMR), and borehole image logs was used to build a permeability transform that considers the pore size distribution and connectivity. The permeability model is based on a dual medium pore network concept that combines the macro pore network with the micro and meso pore network according to their relative pore volume. The saturation height functions are defined based on the rock characteristics that honor the capillary pressure (Pc) core data behavior. The comparison of the pore size partitioning from NMR logs or borehole image processing with the core description and digital core photos provided important information on how to process and interpret both log and core data. The pore size distribution was associated with depositional and diagenetic processes that were used during the 3D modeling of the macro porosity. After the porosity and the macro-porosity were modelled using geological concepts and geostatistics, the permeability in the 3D static model is directly computed using the function defined from core and logs. The saturation height functions (SHF) are defined from routine and capillary pressure (Pc) core data to be consistent with the pore size classification used to define the permeability model. The saturation height modeling is ultimately adjusted by combining the free water level (FWL) from pressure data, SHF from Pc data, and log-derived water saturation. All the properties of the model are consistent between each other and can be updated with new wells very easily since only porosity and its macro porosity volume need to be re-populated with the data from the new wells. The construction of empirical permeability models in particular in carbonates that use the pore size distribution has been established in the past generally at the core to log level. This work revisits the construction of those analytical permeability models by using a dual-media pore network concept and illustrates the advantages to use those functions directly in the 3D modeling construction and update. Copyright 2016, Society of Petroleum Engineers.",Scopus,2-s2.0-85044259582
English,Article,2015,"Feys P., Coninx K., Kerkhofs L., De Weyer T., Truyens V., Maris A., Lamers I.",Robot-supported upper limb training in a virtual learning environment: A pilot randomized controlled trial in persons with MS,Journal of NeuroEngineering and Rehabilitation,12,1,60,,,,31,10.1186/s12984-015-0043-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937798422&doi=10.1186%2fs12984-015-0043-3&partnerID=40&md5=93c4fe524eaf7fd51dafb9773403ccce,"Background: Despite the functional impact of upper limb dysfunction in multiple sclerosis (MS), effects of intensive exercise programs and specifically robot-supported training have been rarely investigated in persons with advanced MS. Aim: To investigate the effects of additional robot-supported upper limb training in persons with MS compared to conventional treatment only. Methods: Seventeen persons with MS (pwMS) (median Expanded Disability Status Scale of 8, range 3.5-8.5) were included in a pilot RCT comparing the effects of additional robot-supported training to conventional treatment only. Additional training consisted of 3 weekly sessions of 30 min interacting with the HapticMaster robot within an individualised virtual learning environment (I-TRAVLE). Clinical measures at body function (Hand grip strength, Motricity Index, Fugl-Meyer) and activity (Action Research Arm test, Motor Activity Log) level were administered before and after an intervention period of 8 weeks. The intervention group were also evaluated on robot-mediated movement tasks in three dimensions, providing active range of motion, movement duration and speed and hand-path ratio as indication of movement efficiency in the spatial domain. Non-parametric statistics were applied. Results: PwMS commented favourably on the robot-supported virtual learning environment and reported functional training effects in daily life. Movement tasks in three dimensions, measured with the robot, were performed in less time and for the transporting and reaching movement tasks more efficiently. There were however no significant changes for any clinical measure in neither intervention nor control group although observational analyses of the included cases indicated large improvements on the Fugl-Meyer in persons with more marked upper limb dysfunction. Conclusion: Robot-supported training lead to more efficient movement execution which was however, on group level, not reflected by significant changes on standard clinical tests. Persons with more marked upper limb dysfunction may benefit most from additional robot-supported training, but larger studies are needed. Trial registration: This trial is registered within the registry Clinical Trials GOV (NCT02257606). © 2015 Feys et al.",Scopus,2-s2.0-84937798422
English,Article,2013,"Gurtler J.B., Marks H.M., Bailey R.B., Juneja V., Jones D.R.",Kinetics model comparison for the inactivation of salmonella serotypes enteritidis and oranienburg in 10% salted liquid whole egg,Foodborne Pathogens and Disease,10,6,,492,499,,5,10.1089/fpd.2012.1366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879402203&doi=10.1089%2ffpd.2012.1366&partnerID=40&md5=4ff31ead1da63a9085a6dd6b8dd5fad6,"The goal of this study was to determine the inactivation kinetics of Salmonella in commercial 10% salted liquid whole egg (LWE) to assist the U.S. Department of Agriculture in writing new liquid egg pasteurization guidelines. Current data are not sufficient for predicting thermal inactivation kinetics of Salmonella spp. for use in updating pasteurization guidelines for many types of liquid egg products, including salted LWE (SLWE). This is, in part, due to variations in Salmonella strains and changes in the processing of liquid egg products that have arisen in the past 40 years. Pasteurization guidelines are currently being reevaluated in light of recent risk assessments. Heat-resistant Salmonella serovars Enteritidis and Oranienburg were composited and mixed into 10% SLWE, resulting in final populations of approximately 5.7-7.8 log colony-forming units (CFU)/mL. Inoculated egg was injected into glass capillary tubes, flame-sealed, and heated in a water bath at 60, 62.2, 63.3, 64.3, or 66 C. Contents were surface-plated and incubated at 37 C for 24 h. Survival curves were not log-linear (log levels versus time), but decreased rapidly, and after initial periods became linear. Asymptotic decimal reduction values at each temperature were calculated from survivor curves with a minimum inactivation of 5.0 log CFU/mL. The asymptotic thermal D-values for SLWE were 3.47, 2.23, 1.79, 1.46, and 1.04 min at 60, 62.2, 63.3, 64.3, or 66 C, respectively. The calculated thermal z-value was 11.5 C. A model that predicts lethality for given times and temperatures that was developed predicted that the current pasteurization requirements for 10% SLWE (i.e., 63.3 C for 3.5 min, or 62.2 C for 6.2 min) are not sufficient to inactivate 7 log CFU/mL of Salmonella and only achieve approximately 4 log CFU/mL inactivation. This model will assist egg-products manufacturers and regulatory agencies in designing pasteurization processes to ensure product safety. © Mary Ann Liebert, Inc.",Scopus,2-s2.0-84879402203
English,Article,2011,"Tauchi T., Kizaki M., Okamoto S., Tanaka H., Tanimoto M., Inokuchi K., Murayama T., Saburi Y., Hino M., Tsudo M., Shimomura T., Isobe Y., Oshimi K., Dan K., Ohyashiki K., Ikeda Y.",Seven-year follow-up of patients receiving imatinib for the treatment of newly diagnosed chronic myelogenous leukemia by the TARGET system,Leukemia Research,35,5,,585,590,,34,10.1016/j.leukres.2010.10.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954592021&doi=10.1016%2fj.leukres.2010.10.027&partnerID=40&md5=15ad6adb7d52aeca29c6c90e47c865af,"The TARGET system is an online database that can be easily accessed by physicians. The registration of one's own chronic myeloid leukemia (CML) patients in the TARGET system makes it possible to share experiences among physicians, and, thus, may facilitate appropriate treatment for patients. Patients were registered in the TARGET system from October 2003 to March 2010 in Japan. A total of 1236 patients from 176 hospitals were registered in Japan. We analyzed data from 639 CML chronic phase patients not receiving prior therapy registered in this system. After 90 months follow-up, high survival rates were demonstrated for imatinib-treated newly diagnosed CML patients, with event-free survival (EFS), progression-free survival (PFS), and overall survival (OS) rates of 79.1, 94.8, and 95.1%, respectively. A landmark analysis of 296 patients who showed a complete cytogenetic response (CCyR) at 12 months after the initiation of imatinib treatment revealed that, at 90 months, 99% of patients (95% CI, 98-100) had not progressed to accelerated phase (AP) or blastic crisis (BC). The patients showing a CCyR and a reduction of at least 3. log levels of BCR-ABL transcripts after 18 months of treatment had an estimated survival rate without CML progression of 100% at 84 months. The probability of achieving undetectable BCR-ABL in patients by 72 months with an major molecular response (MMR) at 12 months was 86.5%, compared with 64.7% for those without an MMR (p<0.0001). There were no new safety issues. In summary, based on this 7-year TARGET analysis, imatinib showed a continual clinical benefit as first-line therapy for newly diagnosed CML. The TARGET system may represent a more practical and general feature compared with the IRIS study. © 2010 Elsevier Ltd.",Scopus,2-s2.0-79954592021
English,Conference Paper,2006,"DeCarolis J., Adham S.A., Hirani Z.M., Pearce W.R.",Integrity evaluation of new generation reverse osmosis membranes during municipal wastewater reclamation,American Water Works Association - Water Quality Technology Conference and Exposition 2006: Taking Water Quality to New Heights,,,,1830,1845,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871519202&partnerID=40&md5=3038358b210be495757e68679d68e3bb,"MWH and the City of San Diego have recently completed a pilot testing program at the North City Water Reclamation Plant (NCWRP) located in San Diego, CA. The purpose of the testing program was to evaluate various aspects of RO membrane integrity and integrity monitoring. During this study the integrity of new generation RO membranes offered for water reuse applications was assessed using traditional methods such as vacuum decay testing, vessel probing, on-line conductivity and sulfate monitoring and challenge testing with soluble dye and MS2 phage. In addition, a new method of RO integrity monitoring developed by Nalco Company which uses TRASAR® chemical was evaluated. The ability of the RO membranes to remove select endocrine disrupting compounds (EDCs) and pharmaceuticals and personal care products (PPCPs) was assessed by sampling feed and permeate of each system and evaluating them for a target list of 29 compounds commonly found in municipal wastewater. Lastly, the sensitivity of the tested integrity methods at detecting integrity breaches and the impact of integrity on EDC/PPCP passage were determined by purposely breaching one of the RO membrane systems and repeating testing. Results of this study showed the removal of MS2 phage by new generation RO membranes varied between 2-4 log (i. e. 99%-99. 99%). In addition, results of the various monitoring methods tested on each membrane correlated to MS2 removal; however, the sensitivity of the methods varied. For instance, conductivity monitoring provided ~2 log removal, sulfate monitoring ~3 log removal and soluble dye testing ~ 4 log removal. Challenge testing with TRASAR® showed that the proposed method has the potential to provide a highly sensitive (>6 log) level of integrity monitoring. EDC/PPCP analysis showed all intact new generation RO membrane systems tested removed select compounds present in the feed wastewater to levels near or below MDL (ng/L). Results also suggest that certain EDC/PPCP compounds have different removal mechanisms and therefore, the increase in passage related to compromises in integrity may vary by compound and the type of breach. © 2006 American Water Works Association WQTC Conference All Rights Reserved.",Scopus,2-s2.0-84871519202
Japanese,Article,1979,"Hara M., Sakai M.",A single-dose study of valproate sodium in Japanese previously untreated epileptic patients and healthy volunteers,No To Hattatsu,11,6,,92,101,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018675304&partnerID=40&md5=b3f0edd17beff6c4ea5c0f436156b6d7,"Serum concentrations of valproate sodium (VPA, Depakene) after a single dose were determined in 25 previously untreated epileptic patients (Ep group) and 8 healthy volunteers (N group). All but 3 cases were given no medication at least for 1 month prior to the study. 3 Epileptic patients were given monosodium trichlorethyl phosphate (Trichloryl) or pentobarbital calcium (Ravona) at the time of EEG examination for sleep activation. 6 healthy volunteers were given either a single dose of 200 mg or 400 mg VPA for crossover test. Plots of peak level (Pl, μg/ml) versus dose (d, mg/kg) were linear in the Ep group and N group (calculated by the method of least squares). Ep group: Pl = 9.41d-7.71 (n = 25, r = 0.90, p < 0.001) N group: Pl = 7.02 d + 1.82 (n = 14, r = 0.94, p < 0.001) Plots of log [level (1, μg/ml) dose (d, mg/kg) ratio after peak level] versus time after single dose administration in the following 4 groups were linear (calculated by the method of least squares). Differences between these 4 groups were not statistically significant. Ep 200 mg group (epileptic patients who were given a single dose of 200 mg VPA): log (1/d)=-0.0304 t+0.9420(n=12, r=-0.9979, p<0.001). Ep 400 mg group: log (1/d)=-0.0178t+0.9029(n=13,r=-0.9948, p<0.001). N 200 mg group: log (1/d)=-0.0256t+0.9111 (n=7, r=-0.9961, p<0.001). N 400 mg group:log (1/d)=-0.0230t+0.9058 (n=7, r=-0.9842, p<0.001). Peak level time for Ep 200 mg group was 1.99 ± 0.76 hr; for Ep 400 mg group, 2.32 ± 1.26 hr; for N 200 mg group, 1.87 ± 0.52 hr; and for N 400 mg group, 2.22 ± 0.51 hr. Differences between these 4 groups were not statistically significant. Biological half-life for Ep 200 mg group was 14.32 ± 3.08 hr; for Ep 400 mg group, 17.18 ± 7.71 hr; for N 200 mg group, 14.06 ± 3.51 hr; and for N 400 mg group, 17.28 ± 6.12 hr. Differences between these 4 groups were not statistically significant. No differences of pharmacokinetics valproate sodium between Japanese previously untreated epileptic patients and healthy volunteers were found.",Scopus,2-s2.0-0018675304
English,Article,2018,"Gupta I., Rai C., Devegowda D., Sondergeld C.",Use of data analytics to optimize hydraulic fracture locations along borehole,Petrophysics,59,6,,811,825,,4,10.30632/PJV59N6-2018a6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063045343&doi=10.30632%2fPJV59N6-2018a6&partnerID=40&md5=4e36cc935986ab4569812260e792895a,"The study focuses on the use of data analytics to determine optimal locations to place fractures along the wellbore. The petrophysical data of the formation, such as mineralogy, total organic carbon (TOC), ultrasonic velocity, Young's modulus, Poisson's ratio, and creep displacements, are used to define different clusters. These clusters were then ranked in order of increasing fracturing potential to characterize the formation. The data available to this study included petrophysical measurements at 660 sampled locations in 20 wells in the Eagle Ford, Wolfcamp, and Woodford formations. In the first step, different clusters are identified based on four key variables, namely TOC, clays, Young's modulus and Poisson's ratio using unsupervised clustering algorithms like K-means and hierarchical clustering. Four different clusters were identified and ranked in order of their fracturing potential. The other measurements, such anisotropy parameters (e and y) and creep parameters, not directly used in the clustering process were also incorporated in ranking the clusters. Cluster 1 was rich in clays (51 wt%), had a high Poisson's ratio (0.29), and anisotropy (e = 0.32). This cluster was deemed ductile and unsuitable for fracturing. Cluster 2 was rich in TOC (7.2 wt%), had a high Poisson's ratio (0.26) and anisotropy (e = 0.25). This cluster was also deemed ductile and unsuitable for fracturing. Cluster 3 had low TOC (1.6 wt%) and clays (11 wt%) but high Poisson's ratio (0.26) and a very high Young's modulus (72 GPa). This cluster was also rich in carbonate minerals. This cluster would require high energy to break and therefore deemed unsuitable for fracturing. Cluster 4 had high quartz content (36 wt%), but low TOC (2.2 wt%), Young's modulus (54 GPa), Poisson's ratio (0.19), anisotropy (e = 0.1) and creep. This cluster was deemed most suitable for fracturing. Next, since core data are generally available in a select few wells while log data are available in a much larger set of wells, the clusters were upscaled using the available logs (gamma ray, neutron, and density). Three different classification techniques, namely decision trees, gradient boosting and support vector machines (SVM), were applied and compared. The gradient-boosting technique gave the minimum error and was finally selected to predict the clusters at the log level. This study also shows examples from both vertical and horizontal wells where this workflow was applied to identify optimal fracture locations. © 2019 Society of Well Log Analystists Inc. All rights reserved.",Scopus,2-s2.0-85063045343
English,Article,2016,"Todoroki C.L., Lowell E.C.","Validation of models predicting modulus of elasticity in Douglas-fir trees, boles, and logs",New Zealand Journal of Forestry Science,46,1,11,,,,5,10.1186/s40490-016-0067-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975807178&doi=10.1186%2fs40490-016-0067-x&partnerID=40&md5=96ac920d60914adfad792bcd439344ce,"Background: Acoustic tools have simplified estimation of wood modulus of elasticity (MOE). Strong relationships between acoustic velocity and MOE of logs have encouraged use of acoustics at earlier points in the value chain, culminating in the development of acoustic harvesting systems. With accurate estimates of MOE of individual trees, improvements in efficiency along the value chain and increased value recovery will result. Our aim was to quantify the accuracy of MOE estimates at three distinct points: pre-harvest (standing trees), during harvest (merchantable boles), and post-harvest (5-m logs). We hypothesised that: (1) MOE estimated from acoustic velocity and wood density would provide greatest accuracy; and (2) bole estimates with a resonance tool would be more accurate than tree estimates with a time-of-flight tool. Methods: A sample of 168 Douglas-fir (Pseudotsuga menziesii [Mirb. Franco]) trees, representing the variability in acoustic velocity of 700 standing 36–51-year-old trees, was harvested from three sites. Prior to harvest, time-of-flight and breast-height diameter were recorded. After felling, resonance velocities of boles and subsequent 5-m logs were recorded. Discs, cut from log ends, were immersed, and green wood density determined. Half the logs were processed into boards, the other half into veneer sheets, and all products (in excess of 6000) non-destructively tested for MOE. MOE of parent trees, boles, and logs was then calculated from the mean MOE of derived products. Predictive mixed-effects models of tree, bole, and log MOE were developed using data from 139 trees. Fixed effects comprised combinations of velocity squared, wood density, acoustic MOE (derived from the wave equation), diameter, height, taper, and age. Random effects comprised site, plot, and, at the log level, tree. The models were validated using data from the remaining trees and compared using multiple performance metrics. Results: For estimating tree MOE, a model with velocity squared, wood density, and taper as predictors is recommended. For estimating MOE of boles and logs, models with velocity squared and wood density are recommended. The models have an accuracy, as determined by RMSE, of about ± 2 GPa. Conclusions: For accurate MOE estimation, velocity alone is insufficient. Knowledge of wood density is necessary for improved accuracy. © 2016, The Author(s).",Scopus,2-s2.0-84975807178
English,Article,1998,"Léonard B.M., Hétu F., Busque L., Gyger M., Bélanger R., Perreault C., Roy D.-C.",Lymphoma cell burden in progenitor cell grafts measured by competitive polymerase chain reaction: Less than one log difference between bone marrow and peripheral blood sources,Blood,91,1,,331,339,,57,10.1182/blood.v91.1.331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2642665511&doi=10.1182%2fblood.v91.1.331&partnerID=40&md5=6b59c6d8f0a72b7f1aaebe14a8e6a01a,"A controversy persists in autologous transplantation as to which source of progenitor cells, bone marrow (BM) or peripheral blond (PB), contains the lowest number of contaminating lymphoma cells, and how mobilization procedures affect these numbers. To accurately measure the number of non- Hodgkin's lymphoma (NHL) cells harboring the bcl-2/ immunoglobulin H (IgH) rearrangement in progenitor cell grafts, we developed a nested quantitative competitive polymerase chain reaction assay (QC-PCR). DNA from lymph nodes of four patients with NHL were cloned into the pSK(+) vectors to generate four internal controls (ICs) (two with major breakpoint region [MBR] and two with minor cluster region [mcr] rearrangements). The kinetics of amplification of ICs paralleled those of bcl-2/IgH rearranged genomic DNA. When used in a QC- PCR assay, these ICs were accurate at a 0.2-log level and provided reproducible results, as shown by low intrarun and interrun variability. An excellent correlation between predicted and observed lymphoma cell content (r = .99) was observed over a range of at least 5 logs of rearranged calls. This approach was used to measure involvement by NHL cells at the time of progenitor cell harvest in 37 autologous transplant patients. The number of bcl-2/IgH rearranged cells in BM, PB, and mobilized PB (mPB) was found to vary from I to 1.1 x 105 per million cells. The number of lymphoma cells present in BM was significantly higher than in PB (P = .0001), with a median difference in lymphoma cell content between BM and PB of 0.48 log of cells (range, -0.7 to 5 logs). In contrast, we found no difference in the concentration of bcl-2/IgH rearranged cells present in BM versus PB progenitor calls mobilized with cyclophosphamide and granulocyte colony- stimulating factor (G-CSF) (mPB) (P = .57). In conclusion, the QC-PCR assay described in this study could measure accurately and reproducibly the number of bcl-2/IgH rearranged calls among normal cells. Differences in levels of contamination by lymphoma cells between BM and PB were of less than one log (10-fold), and no differences in lymphoma cell concentrations were observed between BM and mobilized PB. As more cells are usually infused with mPB than with BM grafts, mPB progenitor cell grafts may actually be associated with higher levels of contamination by lymphoma cells. Furthermore, this QC-PCR assay should provide an important tool to assess the prognostic impact of lymphoma cell burden both in progenitor cell grafts and in vivo.",Scopus,2-s2.0-2642665511
English,Article,2018,"Tonnelier E., Baskiotis N., Guigue V., Gallinari P.",Anomaly detection in smart card logs and distant evaluation with Twitter: a robust framework,Neurocomputing,298,,,109,121,,10,10.1016/j.neucom.2017.12.067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042663069&doi=10.1016%2fj.neucom.2017.12.067&partnerID=40&md5=145e08e83c222d55675984b005ac486d,"Smart card logs constitute a valuable source of information to model a public transportation network and characterize normal or abnormal events; however, this source of data is associated to a high level of noise and missing data, thus, it requires robust analysis tools. First, we define an anomaly as any perturbation in the transportation network with respect to a typical day: temporary interruption, intermittent habit shifts, closed stations, unusual high/low number of entrances in a station. The Parisian metro network with 300 stations and millions of daily trips is considered as a case study. In this paper, we present four approaches for the task of anomaly detection in a transportation network using smart card logs. The first three approaches involve the inference of a daily temporal prototype of each metro station and the use of a distance denoting the compatibility of a particular day and its inferred prototype. We introduce two simple and strong baselines relying on a differential modeling between stations and prototypes in the raw-log space. We implemented a raw version (sensitive to volume change) as well as a normalized version (sensitive to behavior changes). The third approach is an original matrix factorization algorithm that computes a dictionary of typical behaviors shared across stations and the corresponding weights allowing the reconstruction of denoised station profiles. We propose to measure the distance between stations and prototypes directly in the latent space. The main advantage resides in its compactness allowing to describe each station profile and the inherent variability within a few parameters. The last approach is a user-based model in which abnormal behaviors are first detected for each user at the log level and then aggregated spatially and temporally; as a consequence, this approach is heavier and requires to follow users, at the opposite of the previous ones that operate on anonymous log data. On top of that, our contribution regards the evaluation framework: we listed particular days but we also mined RATP1 Twitter account to obtain (partial) ground truth information about operating incidents. Experiments show that matrix factorization is very robust in various situations while the last user-based model is particularly efficient to detect small incidents reported in the twitter dataset. © 2018 Elsevier B.V.",Scopus,2-s2.0-85042663069
English,Conference Paper,2005,"Sianturi P., Kanninen M.",Determination of sustainable management of natural tropical forests using SYMFOR modelling framework,"MODSIM05 - International Congress on Modelling and Simulation: Advances and Applications for Management and Decision Making, Proceedings",,,,1977,1984,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053102941&partnerID=40&md5=52c2f2686fffa41a5a9dbb13d44f0014,"A permanent sample plot (PSP) data set was enumerated from primary forest in Jambi province, Indonesia. No silvicultural treatment (harvesting, thinning, poisoning) was applied to the plot during the period in which measurements were taken. The data set including individual tree information such as tree identity, diameter and position within the plot, was input into the Sustainable Yield Management for Tropical Forests (SYMFOR) model. The model has been calibrated for a Kalimantan PSP data set. Prior to the simulation, the Jambi data set was compared to the Kalimantan data set in terms of diameter class distribution, the dominant tree species, and tree family distribution. It was concluded that both data sets are not significantly different. Moreover, both data sets are lowland tropical primary forest types. Hence, the model was readily applicable to this Jambi data set. Using the SYMFOR computer model several harvesting methods were simulated. These were the conventional method TPTI (Tebang Pilih dan Tanam Indonesia), RIL (Reduced Impact Logging) and a set of harvesting methods derived from the RIL namely RIL8, RIL50 and RIL60. The RIL8 means the maximum number of trees allowable to cut is 8 per hectare, while the RIL50 and RIL60 means the maximum volume of the allowable cut is 50 m3 ha-1 and 60 m3 ha-1 respectively. The RIL60 associated with the production forests category while the RIL50 with the limited production forests category. The simulation was run over a long period of time in order to cover multiple harvest cycles, and were repeated for several times to capture the variability among the runs. These output results were manipulated to obtain the evolution of timber extracted as time went on. Under a 35 year cutting cycle, the RIL50 and RIL60 performed better than other methods - the amount of timber extracted per hectare in the first harvest were successfully attained before the next harvest. The simulation was also conducted by altering the length of cutting cycle within the TPTI, RIL and RIL8. The results showed that timber production increased with the cutting cycle. In particular, under the RIL8 on 45 years cycle, the quantity of timber extracted reached back its pre-first harvest level. This might be due to maximum allowable cut assigned for the RIL8 was less severe. The effect of logging on residual stands was also simulated. It was found that both the RIL50 and RIL60 were consistently better than the other harvesting methods; the forest system could be revived almost to the condition of pre-first harvest level. Upon the cutting cycle extension, it was found that under RIL50 and RIL60 methods, the level of residual stand beyond the 35 years cycle, were successfully returned to its pre-first harvest level. In contrast, both the TPTI and the RIL methods failed to reach its respective pre-first harvest level despite substantial extension of the cutting cycle applied. Notably, the RIL8 which is considered to be less severe logging compared to the RIL, still failed to reach the pre-first harvest level particularly if the cutting cycle less than 45 years. This suggests that in order to reach forest sustainability, careful logging operations as assigned within the RIL methods should be in conjunction with the reduction of logging level. This study consistently suggests that the current forest management guidelines in Indonesia (the TPTI) would not lead our forest sustainably. Both RIL50 and RIL60 harvesting methods on 35 year cycle could be good alternatives for the TPTI.",Scopus,2-s2.0-80053102941
English,Article,2014,"Haaken D., Dittmar T., Schmalz V., Worch E.",Disinfection of biologically treated wastewater and prevention of biofouling by UV/electrolysis hybrid technology: Influence factors and limits for domestic wastewater reuse,Water Research,52,,,20,28,,35,10.1016/j.watres.2013.12.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892525134&doi=10.1016%2fj.watres.2013.12.029&partnerID=40&md5=72508b0f5f73e5ef77b5a2165c637b98,"Reuse of wastewater contributes significantly to an efficient and sustainable water usage. However, due to the presence of a multitude of pathogens (e.g. bacteria, viruses, worms, protozoa) in secondary effluents, disinfection procedures are indispensable. In decentralized wastewater treatment, UV irradiation represents one of the most common disinfection methods in addition to membrane processes and to a certain extent electrochemical procedures. However, the usage of UV disinfected secondary effluents for domestic (sanitary) or irrigation purposes bears a potential health risk due to the possible photo and dark repair of reversibly damaged bacteria. Against this background, the application of the UV/electrolysis hybrid technology for disinfection and prevention of bacterial reactivation in biologically treated wastewater was investigated in view of relevant influence factors and operating limits. Furthermore, the influence of electrochemically generated total oxidants on the formation of biofilms on quartz glass surfaces was examined, since its preventive avoidance contributes to an enhanced operational safety of the hybrid reactor. It was found that reactivation of bacteria in UV irradiated, biologically treated wastewater can be prevented by electrochemically produced total oxidants. In this regard, the influence of the initial concentration of the microbiological indicator organism Escherichia coli (E. coli) (9.3*102-2.2*105 per 100mL) and the influence of total suspended solids (TSS) in the range of 11-75mgL-1 was examined. The concentration of total oxidants necessary for prevention of bacterial regrowth increases linearly with the initial E. coli and TSS concentration. At an initial concentration of 933 E. coli per 100mL, a total oxidants concentration of 0.4mgL-1 is necessary to avoid photo reactivation (at 4200Lux), whereas 0.67mgL-1 is required if the E. coli concentration is enhanced by 2.4 log levels (cTSS = constant = 13 mg L-1). The prevention of dark repair is ensured with 25-50% lower concentration of total oxidants. An increase of the TSS concentration from 11mgL-1 to 75mgL-1 leads to a triplication of the need of total oxidants from 0.6mgL-1 to 1.8mgL-1 (3*105E. coli per 100 mL). The energy consumption of the hybrid reactor varies from 0.17kWhm-3 to 0.94kWhm-3 depending on the TSS concentration (11-75mgL-1).Furthermore, biofilm formation on quartz glass surfaces, of which the sleeves of UV lamps consist, can be suppressed by electrochemically produced total oxidants at a concentration of at least 1mgL-1 which ensures high operational safety of the hybrid reactor combined with large maintenance intervals. © 2014 Elsevier Ltd.",Scopus,2-s2.0-84892525134
English,Article,2009,Meyler A.,The pass through of oil prices into euro area consumer liquid fuel prices in an environment of high and volatile oil prices,Energy Economics,31,6,,867,881,,38,10.1016/j.eneco.2009.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349810537&doi=10.1016%2fj.eneco.2009.07.002&partnerID=40&md5=f14294b3978cb9a76dbc3cf227d18139,"Crude and refined oil prices have been relatively high and volatile on a sustained basis since 1999. This paper considers the pass through of oil prices into consumer liquid (i.e. petrol, diesel and heating) fuel prices in such an environment. The pass through of oil prices into consumer liquid fuel prices has already been addressed extensively in the literature. Nonetheless much of this literature has either focused on the United States or on a time period when oil prices were relatively stable, or has used monthly data. The main contribution of this paper is a comprehensive combination of many features that have been considered before but rarely jointly. These features include: (1) the analysis of the euro area as an aggregate and a large number of countries (the initial 12 member states); (2) the consideration of different time periods; (3) the modelling of the data in raw levels rather than in log levels. This turns out to have important implications for our findings; (4) the use of high frequency (weekly) data, which, as results will suggest, are the lowest frequency one should consider; (5) the investigation of the different stages of the production chain from crude oil prices to retail distribution - refining costs and margins, distribution and retailing costs and margins; (6) the examination of prices including and excluding taxes - excise and value-added; (7) the modelling of prices for three fuel types - passenger car petrol and diesel separately and home heating fuel oil; (8) lastly we also address the issue of possible asymmetries, allowing for the pass through to vary according to (a) whether price are increasing or decreasing and (b) whether price levels are above or below their equilibrium level. The main findings are as follows: First, as distribution and retailing costs and margins have been broadly stable on average, the modelling of the relationship between consumer prices excluding taxes and upstream prices in raw levels rather than in logarithms has important implications for the stability of estimates of pass through when oil price levels rise significantly. Second, considering spot prices for refined prices improves significantly the fit of the estimated models relative to using crude oil prices. It also results in more economically meaningful results concerning the extent of pass through. Third, oil price pass through occurs quickly, with 90% occurring within three to five weeks. Fourth, using a relatively broad specification allowing for asymmetry in the pass through from upstream to downstream prices, there is little evidence of statistically significant asymmetries. Furthermore, even where asymmetry is found to be statistically significant, it is generally not economically significant. Lastly, these results generally hold across most euro area countries with few exceptions. © 2009 Elsevier B.V. All rights reserved.",Scopus,2-s2.0-70349810537