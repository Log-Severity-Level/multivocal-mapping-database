ID,Status,Title,Abstract,Year,Authors
264,ACCEPTED,A Qualitative Study of the Benefits and Costs of Logging from Developers&#x0027; Perspectives,"Software developers insert logging statements in their source code to collect important runtime information of software systems. In practice, logging appropriately is a challenge for developers. Prior studies aimed to improve logging by proactively inserting logging statements in certain code snippets or by learning where to log from existing logging code. However, there exists no work that systematically studies developers&#x0027; logging considerations, i.e., the benefits and costs of logging from developers&#x0027; perspectives. Without understanding developers&#x0027; logging considerations, automated approaches for logging decisions are based primarily on researchers&#x0027; intuition which may not be convincing to developers. In order to fill the gap between developers&#x0027; logging considerations and researchers&#x0027; intuition, we performed a qualitative study that combines a survey of 66 developers and a case study of 223 logging-related issue reports. The findings of our qualitative study draw a comprehensive picture of the benefits and costs of logging from developers&#x0027; perspectives. We observe that developers consider a wide range of logging benefits and costs, while most of the uncovered benefits and costs have never been observed nor discussed in prior work. We also observe that developers use ad hoc strategies to balance the benefits and costs of logging. Developers need to be fully aware of the benefits and costs of logging, in order to better benefit from logging (e.g., leveraging logging to enable users to solve problems by themselves) and avoid unnecessary negative impact (e.g., exposing users&#x0027; sensitive information). Future research needs to consider such a wide range of logging benefits and costs when developing automated logging strategies. Our findings also inspire opportunities for researchers and logging library providers to help developers balance the benefits and costs of logging, for example, to support different log levels for different parts of a logging statement, or to help developers estimate and reduce the negative impact of logging statements. IEEE",2020,"Li H., Shang W., Adams B., Sayagh M., Hassan A.E."
191,ACCEPTED,An approach to cloud execution failure diagnosis based on exception logs in openstack,"Cloud is getting ubiquitous and scales up rapidly. It is critical to effectively detect and efficiently repair system anomalies for a robust cloud. Many efforts have been made to facilitate analysis of system problems with the readily-available and massive cloud logs. However, most tools can still not automatically recognize failures related to a specific cloud operating system task. To diagnose execution failures of a cloud, it is inevitable to monitor corresponding system tasks. In this paper, we propose a lightweight approach to identify cloud behaviors related to failed executions of the cloud operating system for failure diagnosis, by exploiting logs of ERROR logging level in a cloud. Instead of working on execution sequences extracted from logs for all system tasks, we focus on automated recognition of exception logs generated by a system task. These logs are critical snippets of execution traces for failure diagnosis of a cloud. In our work, exception logs are extracted and associated with the respective system task. Efforts can be reduced by comparing patterns of new error cloud behaviors with cloud behaviors met before. With experiments on OpenStack, a popular open source cloud operating system, we demonstrate that our work is effective and efficient for execution failure diagnosis of a cloud. Our approach can also be used as a complementary method for log-based troubleshooting tools concentrating on execution sequences. © 2019 IEEE.",2019,"Yuan Y., Shi W., Liang B., Qin B."
1,ACCEPTED,An Approach to Recommendation of Verbosity Log Levels Based on Logging Intention,"Verbosity levels of logs are designed to discriminate highly diverse runtime events, which facilitates system failure identification through simple keyword search (e.g., fatal, error). Verbosity levels should be properly assigned to logging statements, as inappropriate verbosity levels would confuse users and cause a lot of redundant maintenance effort. However, to achieve such a goal is not an easy task due to the lack of practical specifications and guidelines towards verbosity log level usages. The existing research has built a classification model on log related quantitative metrics such as log density to improve logging level practice. Though such quantitative metrics can reveal logging characteristics, their contributions on logging level decision are limited, since valuable logging intention information buried in logging code context can not be captured. In this paper, we propose an automatic approach to help developers determine the appropriate verbosity log levels. More specially, our approach discriminates different verbosity log level usages based on code context features that contain underlying logging intention. To validate our approach, we implement a prototype tool, VerbosityLevelDirector, and perform a case study to measure its effectiveness on four well-known open source software projects. Evaluation results show that VerbosityLevelDirector achieves high performance on verbosity level discrimination and outperforms the baseline approaches on all those projects. Furthermore, through applying noise handling technique, our approach can detect previously unknown inappropriate verbosity level configurations in the code repository. We have reported 21 representative logging level errors with modification advice to issue tracking platforms of the examined software projects and received positive feedback from their developers. The above results confirm that our work can help developers make a better logging level decision in real-world engineering. © 2019 IEEE.",2019,"Anu H., Chen J., Shi W., Hou J., Liang B., Qin B."
7,ACCEPTED,Automatic recommendation to appropriate log levels,"A log statement is one of the key tactics for a developer to record and monitor important run-time behaviors of our system in a development phase and a maintenance phase. It composes of a message for stating log contents, and a log level (eg, debug or warn) to denote the severity of a message and controlling its visibility at run time. In spite of its usefulness, a developer does not tend to deeply consider which log level is appropriate in writing source code, which causes the system to be unmaintainable. To address this issue, this paper proposes an automatic approach to validating the appropriateness of the log level in consideration of the semantic and syntactic features and recommending a proper alternative log level. We first build the semantic feature vector to quantify the semantic similarity among application log messages using the word vector space, and the syntactic feature vector to capture the application context that surrounds the log statement. Based on the feature vectors and machine learning techniques, the log level is automatically validated, and an alternative log level is recommended if the log level is invalid. For the evaluation, we collected 22 open-source projects from three application domains, and obtained the 77% of precision and 75% of recall in validating the log levels. Also, our approach showed 6% higher accuracy than that of the developer group who has 7 to 8 years of work experience, and 72% of the developers accepted our recommendation. © 2019 John Wiley & Sons, Ltd.",2020,"Kim T., Kim S., Park S., Park Y."
5,ACCEPTED,Design Log Management System of Computer Network Devices Infrastructures Based on ELK Stack,"Device monitoring is an important thing to manage networks. Information-related network state or condition can be gathered through the device monitoring for administrators to take decisions regarding occurred events. Logs can be useful information to monitor network devices. Network administrator of Diponegoro University needs a centralized the logs, so that can receive, manage, and analyze logs. This research identifies functional requirements of the log management system. DSR Method was used to design topology and software of the log management system. The next step is implementation of the topology, software, and application. The last step is testing the system and log management application. The results show that collecting centralized logs and processing these logs into information in the form of dashboards using ELK Stack application successfully implemented. The dashboard resulted by ELK Stack Application will be implemented on the web application using PHP programming language and Code Igniter framework. The test results show that system can receive logs and group the log according to the device location and the severity level of the log. © 2019 IEEE.",2019,"Rochim A.F., Aziz M.A., Fauzi A."
113,ACCEPTED,Developing an error logging framework for ruby on rails application using AOP,"A framework for detecting and recording the flaws that happen during the usage of web applications is designed and a library functionality to perform this is discussed in this paper. The recorded information can be stored at different levels of detail, commonly called the logging levels. For some modules more than others, it may be required to store more detailed information about any error that arises during its usage according to its importance. A Web Application also needs to print the stack trace containing the error information on the web page when an error occurs for the user to understand the nature of the error. When dealing with legacy web applications, it is difficult to insert code. The proposed and designed framework is tested with a web application called Kic Kart. © 2014 IEEE.",2014,"Gomathy M., Devi V.K., Meenakshi D."
56,ACCEPTED,How is logging practice implemented in open source software projects? A preliminary exploration,"Background: Logs are the footprints that software systems produce during runtime, which can be used to understand the dynamic behavior of these software systems. To generate logs, logging practice is accepted by developers to place logging statements in the source code of software systems. Compared to the great number of studies on log analysis, the research on logging practice is relatively scarce, which raises a very critical question, i.e. as the original intention, can current logging practice support capturing the behavior of software systems effectively? Aims: To answer this question, we first need to understand how logging practices are implemented these software projects. Method: In this paper, we carried out an empirical study to explore the logging practice in open source software projects so as to establish a basic understanding on how logging practice is applied in real world software projects. The density, log level (what to log?) and context (where to log?) are measured for our study. Results: Based on the evidence we collected in 28 top open source projects, we find the logging practice is adopted highly inconsistently among different developers both across projects and even within one project in terms of the density and log levels of logging statements. However, the choice of what context the logging statements to place is consistent to a fair degree. Conclusion: Both the inconsistency in density and log level and the convergence of context have forced us to question whether it is a reliable means to understand the runtime behavior of software systems via analyzing the logs produced by the current logging practice. © 2018 IEEE.",2018,"Rong G., Gu S., Zhang H., Shao D., Liu W."
24,ACCEPTED,LogGAN: a Log-level Generative Adversarial Network for Anomaly Detection using Permutation Event Modeling,"System logs that trace system states and record valuable events comprise a significant component of any computer system in our daily life. Each log contains sufficient information (i.e., normal and abnormal instances) that assist administrators in diagnosing and maintaining the operation of systems. If administrators cannot detect and eliminate diverse and complex anomalies (i.e., bugs and failures) efficiently, running workflows and transactions, even systems, would break down. Therefore, the technique of anomaly detection has become increasingly significant and attracted a lot of research attention. However, current approaches concentrate on the anomaly detection analyzing a high-level granularity of logs (i.e., session) instead of detecting log-level anomalies which weakens the efficiency of responding anomalies and the diagnosis of system failures. To overcome the limitation, we propose an LSTM-based generative adversarial network for anomaly detection based on system logs using permutation event modeling named LogGAN, which detects log-level anomalies based on patterns (i.e., combinations of latest logs). On the one hand, the permutation event modeling mitigates the strong sequential characteristics of LSTM for solving the out-of-order problem caused by the arrival delays of logs. On the other hand, the generative adversarial network-based model mitigates the impact of imbalance between normal and abnormal instances to improve the performance of detecting anomalies. To evaluate LogGAN, we conduct extensive experiments on two real-world datasets, and the experimental results show the effectiveness of our proposed approach on the task of log-level anomaly detection. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",2021,"Xia B., Bai Y., Yin J., Li Y., Xu J."
200,ACCEPTED,Optimizing Root Cause Analysis Time Using Smart Logging Framework for Unix and GNU/Linux Based Operating System,"The computer activity records are used for statistical purposes, backup, recovery, and root cause analysis of failure on application. These records are referred as a log. The log files are written for recording incoming dialogs, debug, error, status of an application and certain transaction details, by the operating system or other control program. The logs generated by an application that can be referred by user that may be helpful in the event of failure. For example, in a file transfer, FTP program generates a log file consist of date, time, source and destination, etc. In this number of logs generated by the application uses too much disk space. If the logging is tuned down (e.g., by lowering the log level) then the disk space usage is less, but then not enough information is available for debugging issue. To address this problem we proposed a Smart Logging Framework. The Smart Logging Framework provides the feature such as In-memory logging, In-memory packet capturing and Zoom-in log viewer. © 2020, Springer Nature Singapore Pte Ltd.",2020,"Bharkad V.S., Chavan M.K."
18,ACCEPTED,PADLA: A dynamic log level adapter using online phase detection,"Logging is an important feature for a software system to record its run-time information. Although detailed logs are helpful to identify the cause of a failure in a program execution, constantly recording detailed logs of a long-running system is challenging because of its performance overhead and storage cost. To solve the problem, we propose PADLA (Phase-Aware Dynamic Log Level Adapter) that dynamically adjusts the log level of a running system so that the system can record irregular events such as performance anomalies in detail while recording regular events concisely. PADLA is an extension of Apache Log4j, one of the most popular logging framework for Java. It employs an online phase detection algorithm to recognize irregular events. It monitors run-time performance of a system and learns regular execution phases of a program. If it recognizes a performance anomalies, it automatically changes the log level of a system to record the detailed behavior. In the case study, PADLA successfully recorded a detailed log for performance analysis of a server system under high load while suppressing the amount of log data and performance overhead. © 2019 IEEE.",2019,"Mizouchi T., Shimari K., Ishio T., Inoue K."
125,ACCEPTED,ReCon - Aspect oriented remotely reconfigurable error logging framework for web applications,"Web applications need an error logging framework as they can adequately record and store the errors that are encountered either in the web applications or in the usage of them. In the existing MVC(Model-View-Controller) based frameworks, the log settings such as log location, mechanism and logging level are specified for all the modules in a single file which cannot be changed dynamically. Further, for individual modules, the log settings have to be hard coded into application code, confusing the role of the application developers and log administrators. A remotely configurable error logging framework has been proposed in this paper. This paper proposes to use the aspect oriented programming paradigm to weave the code for error logging into the legacy web application. The proposed configurable error logging framework has been tested on an e-Shopping web application built based on the 'Ruby on Rails' framework. © 2013 IEEE.",2013,"Krishnamurthy V., Babu C., Krishnan. P.M., Aravindan. C., Balamurugan. S."
97,ACCEPTED,Studying the characteristics of logging practices in mobile apps: a case study on F-Droid,"Logging is a common practice in software engineering. Prior research has investigated the characteristics of logging practices in system software (e.g., web servers or databases) as well as desktop applications. However, despite the popularity of mobile apps, little is known about their logging practices. In this paper, we sought to study logging practices in mobile apps. In particular, we conduct a case study on 1,444 open source Android apps in the F-Droid repository. Through a quantitative study, we find that although mobile app logging is less pervasive than server and desktop applications, logging is leveraged in almost all studied apps. However, we find that there exist considerable differences between the logging practices of mobile apps and the logging practices in server and desktop applications observed by prior studies. In order to further understand such differences, we conduct a firehouse email interview and a qualitative annotation on the rationale of using logs in mobile app development. By comparing the logging level of each logging statement with developers’ rationale of using the logs, we find that all too often (35.4%), the chosen logging level and the rationale are inconsistent. Such inconsistency may prevent the useful runtime information to be recorded or may generate unnecessary logs that may cause performance overhead. Finally, to understand the magnitude of such performance overhead, we conduct a performance evaluation between generating all the logs and not generating any logs in eight mobile apps. In general, we observe a statistically significant performance overhead based on various performance metrics (response time, CPU and battery consumption). In addition, we find that if the performance overhead of logging is significantly observed in an app, disabling the unnecessary logs indeed provides a statistically significant performance improvement. Our results show the need for a systematic guidance and automated tool support to assist in mobile logging practices. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",2019,"Zeng Y., Chen J., Shang W., Chen T.-H.P."
121,ACCEPTED,Web log data analysis and mining,"Log files contain information about User Name, IP Address, Time Stamp, Access Request, number of Bytes Transferred, Result Status, URL that Referred and User Agent. The log files are maintained by the web servers. By analysing these log files gives a neat idea about the user. This paper gives a detailed discussion about these log files, their formats, their creation, access procedures, their uses, various algorithms used and the additional parameters that can be used in the log files which in turn gives way to an effective mining. It also provides the idea of creating an extended log file. © Springer-Verlag Berlin Heidelberg 2011.",2011,"Joshila Grace L.K., Maheswari V., Nagamalai D."
11,ACCEPTED,Which log level should developers choose for a new logging statement?,"Logging statements are used to record valuable runtime information about applications. Each logging statement is assigned a log level such that users can disable some verbose log messages while allowing the printing of other important ones. However, prior research finds that developers often have difficulties when determining the appropriate level for their logging statements. In this paper, we propose an approach to help developers determine the appropriate log level when they add a new logging statement. We analyze the development history of four open source projects (Hadoop, Directory Server, Hama, and Qpid), and leverage ordinal regression models to automatically suggest the most appropriate level for each newly-added logging statement. First, we find that our ordinal regression model can accurately suggest the levels of logging statements with an AUC (area under the curve; the higher the better) of 0.75 to 0.81 and a Brier score (the lower the better) of 0.44 to 0.66, which is better than randomly guessing the appropriate log level (with an AUC of 0.50 and a Brier score of 0.80 to 0.83) or naively guessing the log level based on the proportional distribution of each log level (with an AUC of 0.50 and a Brier score of 0.65 to 0.76). Second, we find that the characteristics of the containing block of a newly-added logging statement, the existing logging statements in the containing source code file, and the content of the newly-added logging statement play important roles in determining the appropriate log level for that logging statement. © 2016, Springer Science+Business Media New York.",2017,"Li H., Shang W., Hassan A.E."
288,REJECTED,Anomaly detection in smart card logs and distant evaluation with Twitter: a robust framework,"Smart card logs constitute a valuable source of information to model a public transportation network and characterize normal or abnormal events; however, this source of data is associated to a high level of noise and missing data, thus, it requires robust analysis tools. First, we define an anomaly as any perturbation in the transportation network with respect to a typical day: temporary interruption, intermittent habit shifts, closed stations, unusual high/low number of entrances in a station. The Parisian metro network with 300 stations and millions of daily trips is considered as a case study. In this paper, we present four approaches for the task of anomaly detection in a transportation network using smart card logs. The first three approaches involve the inference of a daily temporal prototype of each metro station and the use of a distance denoting the compatibility of a particular day and its inferred prototype. We introduce two simple and strong baselines relying on a differential modeling between stations and prototypes in the raw-log space. We implemented a raw version (sensitive to volume change) as well as a normalized version (sensitive to behavior changes). The third approach is an original matrix factorization algorithm that computes a dictionary of typical behaviors shared across stations and the corresponding weights allowing the reconstruction of denoised station profiles. We propose to measure the distance between stations and prototypes directly in the latent space. The main advantage resides in its compactness allowing to describe each station profile and the inherent variability within a few parameters. The last approach is a user-based model in which abnormal behaviors are first detected for each user at the log level and then aggregated spatially and temporally; as a consequence, this approach is heavier and requires to follow users, at the opposite of the previous ones that operate on anonymous log data. On top of that, our contribution regards the evaluation framework: we listed particular days but we also mined RATP1 Twitter account to obtain (partial) ground truth information about operating incidents. Experiments show that matrix factorization is very robust in various situations while the last user-based model is particularly efficient to detect small incidents reported in the twitter dataset. © 2018 Elsevier B.V.",2018,"Tonnelier E., Baskiotis N., Guigue V., Gallinari P."
6,REJECTED,Applying data analytic techniques for fault detection,"Monitoring events in communication and computing systems becomes more and more challenging due to the increasing complexity and diversity of these systems. Several supporting tools have been created to assist system administrators in monitoring an enormous number of events daily. The main function of these tools is to filter as many as possible events and present highly suspected events to the administrators for fault analysis, detection and report. While these suspected events appear regularly on large and complex systems, such as cloud computing systems, analyzing them consumes much time and effort. In this study, we propose an approach for evaluating the severity level of events using a classification decision tree. The approach exploits existing fault datasets and features, such as bug reports and log events to construct a decision tree that can be used to classify the severity level of other events. The administrators refer to the result of classification to determine proper actions for the suspected events with a high severity level. We have implemented and experimented the approach for various bug report and log event datasets. The experimental results reveal that the accuracy of classifying severity levels by using the decision trees is above 80%, and some detailed analyses are also provided. © Springer-Verlag GmbH Germany 2017.",2017,"Tran H.M., Van Nguyen S., Le S.T., Vu Q.T."
166,REJECTED,AutoCorrel: A neural network event correlation approach,"Intrusion detection analysts are often swamped by multitudes of alerts originating from installed intrusion detection systems (IDS) as well as logs from routers and firewalls on the networks. Properly managing these alerts and correlating them to previously seen threats is critical in the ability to effectively protect a network from attacks. Manually correlating events can be a slow tedious task prone to human error. We present a two-stage alert correlation approach involving an artificial neural network (ANN) autoassociator and a single parameter decision threshold-setting unit. By clustering closely matched alerts together, this approach would be beneficial to the analyst. In this approach, alert attributes are extracted from each alert content and used to train an autoassociator. Based on the reconstruction error determined by the autoassociator, closely matched alerts are grouped together. Whenever a new alert is received, it is automatically categorised into one of the alert clusters which identify the type of attack and its severity level as previously known by the analyst. If the attack is entirely new and there is no match to the existing clusters, this would be appropriately reflected to the analyst. There are several advantages to using an ANN based approach. First, ANNs acquire knowledge straight from the data without the need for a human expert to build sets of domain rules and facts. Second, once trained, ANNs can be very fast, accurate and have high precision for near real-time applications. Finally, while learning, ANNs perform a type of dimensionality reduction allowing a user to input large amounts of information without fearing an efficiency bottleneck. Thus, rather than storing the data in TCP Quad format (which stores only seven event attributes) and performing a multi-stage query on reduced information, the user can input all the relevant information available and instead allow the neural network to organise and reduce this knowledge in an adaptive and goal-oriented fashion.",2006,"Dondo M.G., Japkowicz N., Smith R."
52,REJECTED,Categorization of cyber security deception events for measuring the severity level of advanced targeted breaches,"Advanced attackers have become more sophisticated in their target selection, evasion of detection and monetization of breached data. Cyber deception is used for gathering information about botnets and spreading worms, and to detect persistent external attackers hidden into the systems as well as insider threats. Decoys are resources that should not be normally accessed. They raise alerts and provide information when systems have been compromised. Decoys can be used for learning about automated malicious tools and behavior of the adversaries, as well as to slow down the attacks. This paper tries to solve the following challenges. Deception tools usually raise only certain severity level alerts, which have been selected manually or hard coded into implementations. This means that telling the difference in severity between two alerts coming from different decoys may be difficult. However, on the other hand the second challenge is that alerts coming from decoys may tell too much information for malicious administrators (insider threats). In fact, many times it would be not necessary to tell the type or actual location of decoys at all. Third challenge is difficulty of monitoring the attack phases during time. For giving solutions for all three challenges, this paper proposes an automated categorization for severity of information coming from decoys. The proposed categorization can be used together with existing cyber security deception tools (such as honeypots, honeynets or honeytokens) to provide addition information for alerts. The categorization uses a decoy severity level, which is calculated from the criticality of locations of the actual decoy, a bait leading to it and a key enabling the access to the bait or the decoy. Usually external attacks start against the easiest targets, but insider threat may in fact access the most critical information right away. In addition to this, presented categorization wants to improve the situational awareness by giving more information for measuring the level of the adversaries in advanced targeted attacks, and thus helping with the third challenge. The proposed approach and categorization have been tested with propotype including a combination of webpage type of honeytokens, URL type of baits leading to them, and encryption keys and user credentials enabling access to the baits. Two different implementation approaches have been demonstrated. The results show that combining additional severity measurement information together with security alerts indeed improves the situational awareness. The results of the research can be used to improve existing deception tools and ways of logging of events, or to create new deception tools, as well as to improve information that would be shown in various visualization tools. © 2017 ACM.",2017,Väisänen T.
242,REJECTED,"General, Efficient, and Real-Time Data Compaction Strategy for APT Forensic Analysis","The damage caused by Advanced Persistent Threat (APT) attacks to governments and large enterprises is gradually escalating. Once an attack event is detected, forensic analysis will use the dependencies between system audit logs to rapidly locate intrusion points and determine the impact of the attacks. Due to the high persistence of APT attacks, huge amounts of data will be stored to meet the needs of forensic analysis, which not only brings great storage overhead, but also sharply increases the computing costs. To compact data without affecting forensic analysis, several methods have been proposed. However, in real-world scenarios, we meet the problems of weak cross-platform capability, large data processing overhead, and poor real-time performance, rendering existing data compaction methods difficult to meet the usability and universality requirements jointly. To overcome these difficulties, this paper proposes a general, efficient, and real-time data compaction method at the system log level; it does not involve internal analysis of the program or depend on the specific operating system type, and it includes two strategies: 1) data compaction of maintaining global semantics (GS), which determines and deletes redundant events that do not affect global dependencies, and 2) data compaction based on suspicious semantics (SS). Given that the purpose of forensic analysis is to restore the attack chain, SS performs context analysis on the remaining events from GS and further deletes the parts that are not related to the attack. The results of the real-world experiments show that the compaction ratios of our method to system events are as high as 4.36× to 13.18× and 7.86× to 26.99× on GS and SS, respectively, which is better than state-of-the-art studies. © 2005-2012 IEEE.",2021,"Zhu T., Wang J., Ruan L., Xiong C., Yu J., Li Y., Chen Y., Lv M., Chen T."
205,REJECTED,Identifying faults in large-scale distributed systems by filtering noisy error logs,"Extracting fault features with the error logs of fault injection tests has been widely studied in the area of large scale distributed systems for decades. However, the process of extracting features is severely affected by a large amount of noisy logs. While the existing work tries to solve the problem by compressing logs in temporal and spatial views or removing the semantic redundancy between logs, they fail to consider the co-existence of other noisy faults that generate error logs instead of injected faults, for example, random hardware faults, unexpected bugs of softwares, system configuration faults or the error rank of a log severity. During a fault feature extraction process, those noisy faults generate error logs that are not related to a target fault, and will strongly mislead the resulted fault features. We call an error log that is not related to a target fault a noisy error log. To filter out noisy error logs, we present a similarity-based error log filtering method SBF, which consists of three integrated steps: (1) model error logs into time series and use haar wavelet transform to get the approximate time series; (2) divide the approximate time series into sub time series by valleys; (3) identify noisy error logs by comparing the similarity between the sub time series of target error logs and the template of noisy error logs. We apply our log filtering method in an enterprise cloud system and show its effectiveness. Compared with the existing work, we successfully filter out noisy error logs and increase the precision and the recall rate of fault feature extraction.1 © 2011 IEEE.",2011,"Rao X., Wang H., Shi D., Chen Z., Cai H., Zhou Q., Sun T."
32,REJECTED,LogGAN: A Sequence-Based Generative Adversarial Network for Anomaly Detection Based on System Logs,"System logs which trace system states and record valuable events comprise a significant component of any computer system in our daily life. There exist abundant information (i.e., normal and abnormal instances) involved in logs which assist administrators in diagnosing and maintaining the operation of the system. If diverse and complex anomalies (i.e., bugs and failures) cannot be detected and eliminated efficiently, the running workflows and transactions, even the system, would break down. Therefore, anomaly detection has become increasingly significant and attracted a lot of research attention. However, current approaches concentrate on the anomaly detection in a high-level granularity of logs (i.e., session) instead of detecting log-level anomalies which weakens the efficiency of responding anomalies and the diagnosis of system failures. To overcome the limitation, we propose a sequence-based generative adversarial network for anomaly detection based on system logs named LogGAN which detects log-level anomalies based on the patterns (i.e., the combination of latest logs). In addition, the generative adversarial network-based model relieves the effect of imbalance between normal and abnormal instances to improve the performance of capturing anomalies. To evaluate LogGAN, we conduct extensive experiments on two real-world datasets, and the experimental results show the effectiveness of our proposed approach to log-level anomaly detection. © Springer Nature Switzerland AG 2019.",2019,"Xia B., Yin J., Xu J., Li Y."
68,REJECTED,Logging in C++,Logging is a critical technique for troubleshooting and maintaining software systems and provides information without requiring knowledge of programming language. Good logging mechanisms can save long debugging sessions and can increase the maintainability of applications. It is believed that indentation makes the logging more readable. Logging will have a cost only if it actually produces output. This lets the user to control the trade-off between fast execution and detailed logging. User can add logging liberally to code without serious efficiency concerns. The only thing to remember is to pass higher logging levels to code that is more heavily executed. Using policy-based design for logging is justified as the communication can be done in an efficient manner.,2007,Marginean P.
2,REJECTED,QLLog: A log anomaly detection method based on Q-learning algorithm,"Most of the existing log anomaly detection methods suffer from scalability and numerous false positives. Besides, they cannot rank the severity level of abnormal events. This paper proposes a log anomaly detection based on Q-learning, namely QLLog, which can detect multiple types of system anomalies and rank the severity level of abnormal events. We first build a mathematical model of log anomaly detection, proving that log anomaly detection is a sequential decision problem. Second, we use the Q-learning algorithm to build the core of the anomaly detection model. This allows QLLog to automatically learn directed acyclic graph log patterns from normal execution and adjust the training model according to the reward value. Then, QLLog combines the advantages of the Q-learning algorithm and the specially designed rules to detect anomalies when log patterns deviate from the model trained from log data under normal execution. Besides, we provide a feedback mechanism and build an abnormal level table. Therefore, QLLog can adapt to new log states and log patterns. Experiments on real datasets show that the method can quickly and effectively detect system anomalies. Compared with the state of the art, QLLog can detect numerous real problems with high accuracy 95%, and its scalability outperforms other existing log-based anomaly detection methods. © 2021 Elsevier Ltd",2021,"Duan X., Ying S., Yuan W., Cheng H., Yin X."
185,REJECTED,"Triad: Creating synergies between memory, disk and log in log structured key-value stores","We present TRIAD, a new persistent key-value (KV) store based on Log-Structured Merge (LSM) trees. TRIAD improves LSM KV throughput by reducing the write amplification arising in the maintenance of the LSM tree structure. Although occurring in the background, write amplification consumes significant CPU and I/O resources. By reducing write amplification, TRIAD allows these resources to be used instead to improve user-facing throughput. TRIAD uses a holistic combination of three techniques. At the LSM memory component level, TRIAD leverages skew in data popularity to avoid frequent I/O operations on the most popular keys. At the storage level, TRIAD amortizes management costs by deferring and batching multiple I/O operations. At the commit log level, TRIAD avoids duplicate writes to storage. We implement TRIAD as an extension of Facebook's RocksDB and evaluate it with production and synthetic workloads. With these workloads, TRIAD yields up to 193% improvement in throughput. It reduces write amplification by a factor of up to 4x, and decreases the amount of I/O by an order of magnitude. © USENIX Annual Technical Conference, USENIX ATC 2017. All rights reserved.",2019,"Balmau O., Yuan H., Didona D., Guerraoui R., Arora A., Gupta K., Zwaenepoel W., Konka P."