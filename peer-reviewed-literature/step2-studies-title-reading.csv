ID,Status,Title,Year,Authors,Abstract
264,ACCEPTED,A Qualitative Study of the Benefits and Costs of Logging from Developers&#x0027; Perspectives,2020,"Li H., Shang W., Adams B., Sayagh M., Hassan A.E.","Software developers insert logging statements in their source code to collect important runtime information of software systems. In practice, logging appropriately is a challenge for developers. Prior studies aimed to improve logging by proactively inserting logging statements in certain code snippets or by learning where to log from existing logging code. However, there exists no work that systematically studies developers&#x0027; logging considerations, i.e., the benefits and costs of logging from developers&#x0027; perspectives. Without understanding developers&#x0027; logging considerations, automated approaches for logging decisions are based primarily on researchers&#x0027; intuition which may not be convincing to developers. In order to fill the gap between developers&#x0027; logging considerations and researchers&#x0027; intuition, we performed a qualitative study that combines a survey of 66 developers and a case study of 223 logging-related issue reports. The findings of our qualitative study draw a comprehensive picture of the benefits and costs of logging from developers&#x0027; perspectives. We observe that developers consider a wide range of logging benefits and costs, while most of the uncovered benefits and costs have never been observed nor discussed in prior work. We also observe that developers use ad hoc strategies to balance the benefits and costs of logging. Developers need to be fully aware of the benefits and costs of logging, in order to better benefit from logging (e.g., leveraging logging to enable users to solve problems by themselves) and avoid unnecessary negative impact (e.g., exposing users&#x0027; sensitive information). Future research needs to consider such a wide range of logging benefits and costs when developing automated logging strategies. Our findings also inspire opportunities for researchers and logging library providers to help developers balance the benefits and costs of logging, for example, to support different log levels for different parts of a logging statement, or to help developers estimate and reduce the negative impact of logging statements. IEEE"
191,ACCEPTED,An approach to cloud execution failure diagnosis based on exception logs in openstack,2019,"Yuan Y., Shi W., Liang B., Qin B.","Cloud is getting ubiquitous and scales up rapidly. It is critical to effectively detect and efficiently repair system anomalies for a robust cloud. Many efforts have been made to facilitate analysis of system problems with the readily-available and massive cloud logs. However, most tools can still not automatically recognize failures related to a specific cloud operating system task. To diagnose execution failures of a cloud, it is inevitable to monitor corresponding system tasks. In this paper, we propose a lightweight approach to identify cloud behaviors related to failed executions of the cloud operating system for failure diagnosis, by exploiting logs of ERROR logging level in a cloud. Instead of working on execution sequences extracted from logs for all system tasks, we focus on automated recognition of exception logs generated by a system task. These logs are critical snippets of execution traces for failure diagnosis of a cloud. In our work, exception logs are extracted and associated with the respective system task. Efforts can be reduced by comparing patterns of new error cloud behaviors with cloud behaviors met before. With experiments on OpenStack, a popular open source cloud operating system, we demonstrate that our work is effective and efficient for execution failure diagnosis of a cloud. Our approach can also be used as a complementary method for log-based troubleshooting tools concentrating on execution sequences. © 2019 IEEE."
1,ACCEPTED,An Approach to Recommendation of Verbosity Log Levels Based on Logging Intention,2019,"Anu H., Chen J., Shi W., Hou J., Liang B., Qin B.","Verbosity levels of logs are designed to discriminate highly diverse runtime events, which facilitates system failure identification through simple keyword search (e.g., fatal, error). Verbosity levels should be properly assigned to logging statements, as inappropriate verbosity levels would confuse users and cause a lot of redundant maintenance effort. However, to achieve such a goal is not an easy task due to the lack of practical specifications and guidelines towards verbosity log level usages. The existing research has built a classification model on log related quantitative metrics such as log density to improve logging level practice. Though such quantitative metrics can reveal logging characteristics, their contributions on logging level decision are limited, since valuable logging intention information buried in logging code context can not be captured. In this paper, we propose an automatic approach to help developers determine the appropriate verbosity log levels. More specially, our approach discriminates different verbosity log level usages based on code context features that contain underlying logging intention. To validate our approach, we implement a prototype tool, VerbosityLevelDirector, and perform a case study to measure its effectiveness on four well-known open source software projects. Evaluation results show that VerbosityLevelDirector achieves high performance on verbosity level discrimination and outperforms the baseline approaches on all those projects. Furthermore, through applying noise handling technique, our approach can detect previously unknown inappropriate verbosity level configurations in the code repository. We have reported 21 representative logging level errors with modification advice to issue tracking platforms of the examined software projects and received positive feedback from their developers. The above results confirm that our work can help developers make a better logging level decision in real-world engineering. © 2019 IEEE."
288,ACCEPTED,Anomaly detection in smart card logs and distant evaluation with Twitter: a robust framework,2018,"Tonnelier E., Baskiotis N., Guigue V., Gallinari P.","Smart card logs constitute a valuable source of information to model a public transportation network and characterize normal or abnormal events; however, this source of data is associated to a high level of noise and missing data, thus, it requires robust analysis tools. First, we define an anomaly as any perturbation in the transportation network with respect to a typical day: temporary interruption, intermittent habit shifts, closed stations, unusual high/low number of entrances in a station. The Parisian metro network with 300 stations and millions of daily trips is considered as a case study. In this paper, we present four approaches for the task of anomaly detection in a transportation network using smart card logs. The first three approaches involve the inference of a daily temporal prototype of each metro station and the use of a distance denoting the compatibility of a particular day and its inferred prototype. We introduce two simple and strong baselines relying on a differential modeling between stations and prototypes in the raw-log space. We implemented a raw version (sensitive to volume change) as well as a normalized version (sensitive to behavior changes). The third approach is an original matrix factorization algorithm that computes a dictionary of typical behaviors shared across stations and the corresponding weights allowing the reconstruction of denoised station profiles. We propose to measure the distance between stations and prototypes directly in the latent space. The main advantage resides in its compactness allowing to describe each station profile and the inherent variability within a few parameters. The last approach is a user-based model in which abnormal behaviors are first detected for each user at the log level and then aggregated spatially and temporally; as a consequence, this approach is heavier and requires to follow users, at the opposite of the previous ones that operate on anonymous log data. On top of that, our contribution regards the evaluation framework: we listed particular days but we also mined RATP1 Twitter account to obtain (partial) ground truth information about operating incidents. Experiments show that matrix factorization is very robust in various situations while the last user-based model is particularly efficient to detect small incidents reported in the twitter dataset. © 2018 Elsevier B.V."
6,ACCEPTED,Applying data analytic techniques for fault detection,2017,"Tran H.M., Van Nguyen S., Le S.T., Vu Q.T.","Monitoring events in communication and computing systems becomes more and more challenging due to the increasing complexity and diversity of these systems. Several supporting tools have been created to assist system administrators in monitoring an enormous number of events daily. The main function of these tools is to filter as many as possible events and present highly suspected events to the administrators for fault analysis, detection and report. While these suspected events appear regularly on large and complex systems, such as cloud computing systems, analyzing them consumes much time and effort. In this study, we propose an approach for evaluating the severity level of events using a classification decision tree. The approach exploits existing fault datasets and features, such as bug reports and log events to construct a decision tree that can be used to classify the severity level of other events. The administrators refer to the result of classification to determine proper actions for the suspected events with a high severity level. We have implemented and experimented the approach for various bug report and log event datasets. The experimental results reveal that the accuracy of classifying severity levels by using the decision trees is above 80%, and some detailed analyses are also provided. © Springer-Verlag GmbH Germany 2017."
166,ACCEPTED,AutoCorrel: A neural network event correlation approach,2006,"Dondo M.G., Japkowicz N., Smith R.","Intrusion detection analysts are often swamped by multitudes of alerts originating from installed intrusion detection systems (IDS) as well as logs from routers and firewalls on the networks. Properly managing these alerts and correlating them to previously seen threats is critical in the ability to effectively protect a network from attacks. Manually correlating events can be a slow tedious task prone to human error. We present a two-stage alert correlation approach involving an artificial neural network (ANN) autoassociator and a single parameter decision threshold-setting unit. By clustering closely matched alerts together, this approach would be beneficial to the analyst. In this approach, alert attributes are extracted from each alert content and used to train an autoassociator. Based on the reconstruction error determined by the autoassociator, closely matched alerts are grouped together. Whenever a new alert is received, it is automatically categorised into one of the alert clusters which identify the type of attack and its severity level as previously known by the analyst. If the attack is entirely new and there is no match to the existing clusters, this would be appropriately reflected to the analyst. There are several advantages to using an ANN based approach. First, ANNs acquire knowledge straight from the data without the need for a human expert to build sets of domain rules and facts. Second, once trained, ANNs can be very fast, accurate and have high precision for near real-time applications. Finally, while learning, ANNs perform a type of dimensionality reduction allowing a user to input large amounts of information without fearing an efficiency bottleneck. Thus, rather than storing the data in TCP Quad format (which stores only seven event attributes) and performing a multi-stage query on reduced information, the user can input all the relevant information available and instead allow the neural network to organise and reduce this knowledge in an adaptive and goal-oriented fashion."
7,ACCEPTED,Automatic recommendation to appropriate log levels,2020,"Kim T., Kim S., Park S., Park Y.","A log statement is one of the key tactics for a developer to record and monitor important run-time behaviors of our system in a development phase and a maintenance phase. It composes of a message for stating log contents, and a log level (eg, debug or warn) to denote the severity of a message and controlling its visibility at run time. In spite of its usefulness, a developer does not tend to deeply consider which log level is appropriate in writing source code, which causes the system to be unmaintainable. To address this issue, this paper proposes an automatic approach to validating the appropriateness of the log level in consideration of the semantic and syntactic features and recommending a proper alternative log level. We first build the semantic feature vector to quantify the semantic similarity among application log messages using the word vector space, and the syntactic feature vector to capture the application context that surrounds the log statement. Based on the feature vectors and machine learning techniques, the log level is automatically validated, and an alternative log level is recommended if the log level is invalid. For the evaluation, we collected 22 open-source projects from three application domains, and obtained the 77% of precision and 75% of recall in validating the log levels. Also, our approach showed 6% higher accuracy than that of the developer group who has 7 to 8 years of work experience, and 72% of the developers accepted our recommendation. © 2019 John Wiley & Sons, Ltd."
52,ACCEPTED,Categorization of cyber security deception events for measuring the severity level of advanced targeted breaches,2017,Väisänen T.,"Advanced attackers have become more sophisticated in their target selection, evasion of detection and monetization of breached data. Cyber deception is used for gathering information about botnets and spreading worms, and to detect persistent external attackers hidden into the systems as well as insider threats. Decoys are resources that should not be normally accessed. They raise alerts and provide information when systems have been compromised. Decoys can be used for learning about automated malicious tools and behavior of the adversaries, as well as to slow down the attacks. This paper tries to solve the following challenges. Deception tools usually raise only certain severity level alerts, which have been selected manually or hard coded into implementations. This means that telling the difference in severity between two alerts coming from different decoys may be difficult. However, on the other hand the second challenge is that alerts coming from decoys may tell too much information for malicious administrators (insider threats). In fact, many times it would be not necessary to tell the type or actual location of decoys at all. Third challenge is difficulty of monitoring the attack phases during time. For giving solutions for all three challenges, this paper proposes an automated categorization for severity of information coming from decoys. The proposed categorization can be used together with existing cyber security deception tools (such as honeypots, honeynets or honeytokens) to provide addition information for alerts. The categorization uses a decoy severity level, which is calculated from the criticality of locations of the actual decoy, a bait leading to it and a key enabling the access to the bait or the decoy. Usually external attacks start against the easiest targets, but insider threat may in fact access the most critical information right away. In addition to this, presented categorization wants to improve the situational awareness by giving more information for measuring the level of the adversaries in advanced targeted attacks, and thus helping with the third challenge. The proposed approach and categorization have been tested with propotype including a combination of webpage type of honeytokens, URL type of baits leading to them, and encryption keys and user credentials enabling access to the baits. Two different implementation approaches have been demonstrated. The results show that combining additional severity measurement information together with security alerts indeed improves the situational awareness. The results of the research can be used to improve existing deception tools and ways of logging of events, or to create new deception tools, as well as to improve information that would be shown in various visualization tools. © 2017 ACM."
5,ACCEPTED,Design Log Management System of Computer Network Devices Infrastructures Based on ELK Stack,2019,"Rochim A.F., Aziz M.A., Fauzi A.","Device monitoring is an important thing to manage networks. Information-related network state or condition can be gathered through the device monitoring for administrators to take decisions regarding occurred events. Logs can be useful information to monitor network devices. Network administrator of Diponegoro University needs a centralized the logs, so that can receive, manage, and analyze logs. This research identifies functional requirements of the log management system. DSR Method was used to design topology and software of the log management system. The next step is implementation of the topology, software, and application. The last step is testing the system and log management application. The results show that collecting centralized logs and processing these logs into information in the form of dashboards using ELK Stack application successfully implemented. The dashboard resulted by ELK Stack Application will be implemented on the web application using PHP programming language and Code Igniter framework. The test results show that system can receive logs and group the log according to the device location and the severity level of the log. © 2019 IEEE."
113,ACCEPTED,Developing an error logging framework for ruby on rails application using AOP,2014,"Gomathy M., Devi V.K., Meenakshi D.","A framework for detecting and recording the flaws that happen during the usage of web applications is designed and a library functionality to perform this is discussed in this paper. The recorded information can be stored at different levels of detail, commonly called the logging levels. For some modules more than others, it may be required to store more detailed information about any error that arises during its usage according to its importance. A Web Application also needs to print the stack trace containing the error information on the web page when an error occurs for the user to understand the nature of the error. When dealing with legacy web applications, it is difficult to insert code. The proposed and designed framework is tested with a web application called Kic Kart. © 2014 IEEE."
242,ACCEPTED,"General, Efficient, and Real-Time Data Compaction Strategy for APT Forensic Analysis",2021,"Zhu T., Wang J., Ruan L., Xiong C., Yu J., Li Y., Chen Y., Lv M., Chen T.","The damage caused by Advanced Persistent Threat (APT) attacks to governments and large enterprises is gradually escalating. Once an attack event is detected, forensic analysis will use the dependencies between system audit logs to rapidly locate intrusion points and determine the impact of the attacks. Due to the high persistence of APT attacks, huge amounts of data will be stored to meet the needs of forensic analysis, which not only brings great storage overhead, but also sharply increases the computing costs. To compact data without affecting forensic analysis, several methods have been proposed. However, in real-world scenarios, we meet the problems of weak cross-platform capability, large data processing overhead, and poor real-time performance, rendering existing data compaction methods difficult to meet the usability and universality requirements jointly. To overcome these difficulties, this paper proposes a general, efficient, and real-time data compaction method at the system log level; it does not involve internal analysis of the program or depend on the specific operating system type, and it includes two strategies: 1) data compaction of maintaining global semantics (GS), which determines and deletes redundant events that do not affect global dependencies, and 2) data compaction based on suspicious semantics (SS). Given that the purpose of forensic analysis is to restore the attack chain, SS performs context analysis on the remaining events from GS and further deletes the parts that are not related to the attack. The results of the real-world experiments show that the compaction ratios of our method to system events are as high as 4.36× to 13.18× and 7.86× to 26.99× on GS and SS, respectively, which is better than state-of-the-art studies. © 2005-2012 IEEE."
56,ACCEPTED,How is logging practice implemented in open source software projects? A preliminary exploration,2018,"Rong G., Gu S., Zhang H., Shao D., Liu W.","Background: Logs are the footprints that software systems produce during runtime, which can be used to understand the dynamic behavior of these software systems. To generate logs, logging practice is accepted by developers to place logging statements in the source code of software systems. Compared to the great number of studies on log analysis, the research on logging practice is relatively scarce, which raises a very critical question, i.e. as the original intention, can current logging practice support capturing the behavior of software systems effectively? Aims: To answer this question, we first need to understand how logging practices are implemented these software projects. Method: In this paper, we carried out an empirical study to explore the logging practice in open source software projects so as to establish a basic understanding on how logging practice is applied in real world software projects. The density, log level (what to log?) and context (where to log?) are measured for our study. Results: Based on the evidence we collected in 28 top open source projects, we find the logging practice is adopted highly inconsistently among different developers both across projects and even within one project in terms of the density and log levels of logging statements. However, the choice of what context the logging statements to place is consistent to a fair degree. Conclusion: Both the inconsistency in density and log level and the convergence of context have forced us to question whether it is a reliable means to understand the runtime behavior of software systems via analyzing the logs produced by the current logging practice. © 2018 IEEE."
205,ACCEPTED,Identifying faults in large-scale distributed systems by filtering noisy error logs,2011,"Rao X., Wang H., Shi D., Chen Z., Cai H., Zhou Q., Sun T.","Extracting fault features with the error logs of fault injection tests has been widely studied in the area of large scale distributed systems for decades. However, the process of extracting features is severely affected by a large amount of noisy logs. While the existing work tries to solve the problem by compressing logs in temporal and spatial views or removing the semantic redundancy between logs, they fail to consider the co-existence of other noisy faults that generate error logs instead of injected faults, for example, random hardware faults, unexpected bugs of softwares, system configuration faults or the error rank of a log severity. During a fault feature extraction process, those noisy faults generate error logs that are not related to a target fault, and will strongly mislead the resulted fault features. We call an error log that is not related to a target fault a noisy error log. To filter out noisy error logs, we present a similarity-based error log filtering method SBF, which consists of three integrated steps: (1) model error logs into time series and use haar wavelet transform to get the approximate time series; (2) divide the approximate time series into sub time series by valleys; (3) identify noisy error logs by comparing the similarity between the sub time series of target error logs and the template of noisy error logs. We apply our log filtering method in an enterprise cloud system and show its effectiveness. Compared with the existing work, we successfully filter out noisy error logs and increase the precision and the recall rate of fault feature extraction.1 © 2011 IEEE."
24,ACCEPTED,LogGAN: a Log-level Generative Adversarial Network for Anomaly Detection using Permutation Event Modeling,2021,"Xia B., Bai Y., Yin J., Li Y., Xu J.","System logs that trace system states and record valuable events comprise a significant component of any computer system in our daily life. Each log contains sufficient information (i.e., normal and abnormal instances) that assist administrators in diagnosing and maintaining the operation of systems. If administrators cannot detect and eliminate diverse and complex anomalies (i.e., bugs and failures) efficiently, running workflows and transactions, even systems, would break down. Therefore, the technique of anomaly detection has become increasingly significant and attracted a lot of research attention. However, current approaches concentrate on the anomaly detection analyzing a high-level granularity of logs (i.e., session) instead of detecting log-level anomalies which weakens the efficiency of responding anomalies and the diagnosis of system failures. To overcome the limitation, we propose an LSTM-based generative adversarial network for anomaly detection based on system logs using permutation event modeling named LogGAN, which detects log-level anomalies based on patterns (i.e., combinations of latest logs). On the one hand, the permutation event modeling mitigates the strong sequential characteristics of LSTM for solving the out-of-order problem caused by the arrival delays of logs. On the other hand, the generative adversarial network-based model mitigates the impact of imbalance between normal and abnormal instances to improve the performance of detecting anomalies. To evaluate LogGAN, we conduct extensive experiments on two real-world datasets, and the experimental results show the effectiveness of our proposed approach on the task of log-level anomaly detection. © 2020, Springer Science+Business Media, LLC, part of Springer Nature."
32,ACCEPTED,LogGAN: A Sequence-Based Generative Adversarial Network for Anomaly Detection Based on System Logs,2019,"Xia B., Yin J., Xu J., Li Y.","System logs which trace system states and record valuable events comprise a significant component of any computer system in our daily life. There exist abundant information (i.e., normal and abnormal instances) involved in logs which assist administrators in diagnosing and maintaining the operation of the system. If diverse and complex anomalies (i.e., bugs and failures) cannot be detected and eliminated efficiently, the running workflows and transactions, even the system, would break down. Therefore, anomaly detection has become increasingly significant and attracted a lot of research attention. However, current approaches concentrate on the anomaly detection in a high-level granularity of logs (i.e., session) instead of detecting log-level anomalies which weakens the efficiency of responding anomalies and the diagnosis of system failures. To overcome the limitation, we propose a sequence-based generative adversarial network for anomaly detection based on system logs named LogGAN which detects log-level anomalies based on the patterns (i.e., the combination of latest logs). In addition, the generative adversarial network-based model relieves the effect of imbalance between normal and abnormal instances to improve the performance of capturing anomalies. To evaluate LogGAN, we conduct extensive experiments on two real-world datasets, and the experimental results show the effectiveness of our proposed approach to log-level anomaly detection. © Springer Nature Switzerland AG 2019."
68,ACCEPTED,Logging in C++,2007,Marginean P.,Logging is a critical technique for troubleshooting and maintaining software systems and provides information without requiring knowledge of programming language. Good logging mechanisms can save long debugging sessions and can increase the maintainability of applications. It is believed that indentation makes the logging more readable. Logging will have a cost only if it actually produces output. This lets the user to control the trade-off between fast execution and detailed logging. User can add logging liberally to code without serious efficiency concerns. The only thing to remember is to pass higher logging levels to code that is more heavily executed. Using policy-based design for logging is justified as the communication can be done in an efficient manner.
200,ACCEPTED,Optimizing Root Cause Analysis Time Using Smart Logging Framework for Unix and GNU/Linux Based Operating System,2020,"Bharkad V.S., Chavan M.K.","The computer activity records are used for statistical purposes, backup, recovery, and root cause analysis of failure on application. These records are referred as a log. The log files are written for recording incoming dialogs, debug, error, status of an application and certain transaction details, by the operating system or other control program. The logs generated by an application that can be referred by user that may be helpful in the event of failure. For example, in a file transfer, FTP program generates a log file consist of date, time, source and destination, etc. In this number of logs generated by the application uses too much disk space. If the logging is tuned down (e.g., by lowering the log level) then the disk space usage is less, but then not enough information is available for debugging issue. To address this problem we proposed a Smart Logging Framework. The Smart Logging Framework provides the feature such as In-memory logging, In-memory packet capturing and Zoom-in log viewer. © 2020, Springer Nature Singapore Pte Ltd."
18,ACCEPTED,PADLA: A dynamic log level adapter using online phase detection,2019,"Mizouchi T., Shimari K., Ishio T., Inoue K.","Logging is an important feature for a software system to record its run-time information. Although detailed logs are helpful to identify the cause of a failure in a program execution, constantly recording detailed logs of a long-running system is challenging because of its performance overhead and storage cost. To solve the problem, we propose PADLA (Phase-Aware Dynamic Log Level Adapter) that dynamically adjusts the log level of a running system so that the system can record irregular events such as performance anomalies in detail while recording regular events concisely. PADLA is an extension of Apache Log4j, one of the most popular logging framework for Java. It employs an online phase detection algorithm to recognize irregular events. It monitors run-time performance of a system and learns regular execution phases of a program. If it recognizes a performance anomalies, it automatically changes the log level of a system to record the detailed behavior. In the case study, PADLA successfully recorded a detailed log for performance analysis of a server system under high load while suppressing the amount of log data and performance overhead. © 2019 IEEE."
2,ACCEPTED,QLLog: A log anomaly detection method based on Q-learning algorithm,2021,"Duan X., Ying S., Yuan W., Cheng H., Yin X.","Most of the existing log anomaly detection methods suffer from scalability and numerous false positives. Besides, they cannot rank the severity level of abnormal events. This paper proposes a log anomaly detection based on Q-learning, namely QLLog, which can detect multiple types of system anomalies and rank the severity level of abnormal events. We first build a mathematical model of log anomaly detection, proving that log anomaly detection is a sequential decision problem. Second, we use the Q-learning algorithm to build the core of the anomaly detection model. This allows QLLog to automatically learn directed acyclic graph log patterns from normal execution and adjust the training model according to the reward value. Then, QLLog combines the advantages of the Q-learning algorithm and the specially designed rules to detect anomalies when log patterns deviate from the model trained from log data under normal execution. Besides, we provide a feedback mechanism and build an abnormal level table. Therefore, QLLog can adapt to new log states and log patterns. Experiments on real datasets show that the method can quickly and effectively detect system anomalies. Compared with the state of the art, QLLog can detect numerous real problems with high accuracy 95%, and its scalability outperforms other existing log-based anomaly detection methods. © 2021 Elsevier Ltd"
125,ACCEPTED,ReCon - Aspect oriented remotely reconfigurable error logging framework for web applications,2013,"Krishnamurthy V., Babu C., Krishnan. P.M., Aravindan. C., Balamurugan. S.","Web applications need an error logging framework as they can adequately record and store the errors that are encountered either in the web applications or in the usage of them. In the existing MVC(Model-View-Controller) based frameworks, the log settings such as log location, mechanism and logging level are specified for all the modules in a single file which cannot be changed dynamically. Further, for individual modules, the log settings have to be hard coded into application code, confusing the role of the application developers and log administrators. A remotely configurable error logging framework has been proposed in this paper. This paper proposes to use the aspect oriented programming paradigm to weave the code for error logging into the legacy web application. The proposed configurable error logging framework has been tested on an e-Shopping web application built based on the 'Ruby on Rails' framework. © 2013 IEEE."
97,ACCEPTED,Studying the characteristics of logging practices in mobile apps: a case study on F-Droid,2019,"Zeng Y., Chen J., Shang W., Chen T.-H.P.","Logging is a common practice in software engineering. Prior research has investigated the characteristics of logging practices in system software (e.g., web servers or databases) as well as desktop applications. However, despite the popularity of mobile apps, little is known about their logging practices. In this paper, we sought to study logging practices in mobile apps. In particular, we conduct a case study on 1,444 open source Android apps in the F-Droid repository. Through a quantitative study, we find that although mobile app logging is less pervasive than server and desktop applications, logging is leveraged in almost all studied apps. However, we find that there exist considerable differences between the logging practices of mobile apps and the logging practices in server and desktop applications observed by prior studies. In order to further understand such differences, we conduct a firehouse email interview and a qualitative annotation on the rationale of using logs in mobile app development. By comparing the logging level of each logging statement with developers’ rationale of using the logs, we find that all too often (35.4%), the chosen logging level and the rationale are inconsistent. Such inconsistency may prevent the useful runtime information to be recorded or may generate unnecessary logs that may cause performance overhead. Finally, to understand the magnitude of such performance overhead, we conduct a performance evaluation between generating all the logs and not generating any logs in eight mobile apps. In general, we observe a statistically significant performance overhead based on various performance metrics (response time, CPU and battery consumption). In addition, we find that if the performance overhead of logging is significantly observed in an app, disabling the unnecessary logs indeed provides a statistically significant performance improvement. Our results show the need for a systematic guidance and automated tool support to assist in mobile logging practices. © 2019, Springer Science+Business Media, LLC, part of Springer Nature."
185,ACCEPTED,"Triad: Creating synergies between memory, disk and log in log structured key-value stores",2019,"Balmau O., Yuan H., Didona D., Guerraoui R., Arora A., Gupta K., Zwaenepoel W., Konka P.","We present TRIAD, a new persistent key-value (KV) store based on Log-Structured Merge (LSM) trees. TRIAD improves LSM KV throughput by reducing the write amplification arising in the maintenance of the LSM tree structure. Although occurring in the background, write amplification consumes significant CPU and I/O resources. By reducing write amplification, TRIAD allows these resources to be used instead to improve user-facing throughput. TRIAD uses a holistic combination of three techniques. At the LSM memory component level, TRIAD leverages skew in data popularity to avoid frequent I/O operations on the most popular keys. At the storage level, TRIAD amortizes management costs by deferring and batching multiple I/O operations. At the commit log level, TRIAD avoids duplicate writes to storage. We implement TRIAD as an extension of Facebook's RocksDB and evaluate it with production and synthetic workloads. With these workloads, TRIAD yields up to 193% improvement in throughput. It reduces write amplification by a factor of up to 4x, and decreases the amount of I/O by an order of magnitude. © USENIX Annual Technical Conference, USENIX ATC 2017. All rights reserved."
121,ACCEPTED,Web log data analysis and mining,2011,"Joshila Grace L.K., Maheswari V., Nagamalai D.","Log files contain information about User Name, IP Address, Time Stamp, Access Request, number of Bytes Transferred, Result Status, URL that Referred and User Agent. The log files are maintained by the web servers. By analysing these log files gives a neat idea about the user. This paper gives a detailed discussion about these log files, their formats, their creation, access procedures, their uses, various algorithms used and the additional parameters that can be used in the log files which in turn gives way to an effective mining. It also provides the idea of creating an extended log file. © Springer-Verlag Berlin Heidelberg 2011."
11,ACCEPTED,Which log level should developers choose for a new logging statement?,2017,"Li H., Shang W., Hassan A.E.","Logging statements are used to record valuable runtime information about applications. Each logging statement is assigned a log level such that users can disable some verbose log messages while allowing the printing of other important ones. However, prior research finds that developers often have difficulties when determining the appropriate level for their logging statements. In this paper, we propose an approach to help developers determine the appropriate log level when they add a new logging statement. We analyze the development history of four open source projects (Hadoop, Directory Server, Hama, and Qpid), and leverage ordinal regression models to automatically suggest the most appropriate level for each newly-added logging statement. First, we find that our ordinal regression model can accurately suggest the levels of logging statements with an AUC (area under the curve; the higher the better) of 0.75 to 0.81 and a Brier score (the lower the better) of 0.44 to 0.66, which is better than randomly guessing the appropriate log level (with an AUC of 0.50 and a Brier score of 0.80 to 0.83) or naively guessing the log level based on the proportional distribution of each log level (with an AUC of 0.50 and a Brier score of 0.65 to 0.76). Second, we find that the characteristics of the containing block of a newly-added logging statement, the existing logging statements in the containing source code file, and the content of the newly-added logging statement play important roles in determining the appropriate log level for that logging statement. © 2016, Springer Science+Business Media New York."
61,REJECTED,Accurate systematic hot-spot scoring method and score-based fixing guidance generation,2009,"Park Y., Choi J., Hong J., Lee S., Yoo M., Cho J.","The researches on predicting and removing of lithographic hot-spots have been prevalent in recent semiconductor industries, and known to be one of the most difficult challenges to achieve high quality detection coverage. To provide physical design implementation with designer's favors on fixing hot-spots, in this paper, we present a noble and accurate hot-spot detection method, so-called ""leveling and scoring"" algorithm based on weighted combination of image quality parameters (i.e., normalized image log-slope (NILS), mask error enhancement factor (MEEF), and depth of focus (DOF)) from lithography simulation. In our algorithm, firstly, hot-spot scoring function considering severity level is calibrated with process window qualification, and then least-square regression method is used to calibrate weighting coefficients for each image quality parameter. In this way, after we obtain the scoring function with wafer results, our method can be applied to future designs of using the same process. Using this calibrated scoring function, we can successfully generate fixing guidance and rule to detect hot-spot area by locating edge bias value which leads to a hot-spot-free score level. Finally, we integrate the hotspot fixing guidance information into layout editor to facilitate the user-favorable design environment. Applying our method to memory devices of 60 nm node and below, we could successfully attain sufficient process window margin to yield high mass production. Copyright © 2009 The Institute of Electronics, Information and Communication Engineers."
163,REJECTED,An Automated Grading and Diagnosis System for Evaluation of Dry Eye Syndrome,2018,"Bağbaba A., Şen B., Delen D., Uysal B.S.","This article describes methods used to determine the severity of Dry Eye Syndrome (DES) based on Oxford Grading Schema (OGS) automatically by developing and applying a decider model. The number of dry punctate dots occurred on corneal surface after corneal fluorescein staining can be used as a diagnostic indicator of DES severity according to OGS; however, grading of DES severity exactly by carefully assessing these dots is a rather difficult task for humans. Taking into account that current methods are also subjectively dependent on the perception of the ophtalmologists coupled with the time and resource intensive requirements, enhanced diagnosis techniques would greatly contribute to clinical assessment of DES. Automated grading system proposed in this study utilizes image processing methods in order to provide more objective and reliable diagnostic results for DES. A total of 70 fluorescein-stained cornea images from 20 patients with mild, moderate, or severe DES (labeled by an ophthalmologist in the Keratoconus Center of Yildirim Beyazit University Ataturk Training and Research Hospital) used as the participants for the study. Correlations between the number of dry punctate dots and DES severity levels were determined. When automatically created scores and clinical scores were compared, the following measures were observed: Pearson’s correlation value between the two was 0.981; Lin’s Concordance Correlation Coefficients (CCC) was 0.980; and 95% confidence interval limites were 0.963 and 0.989. The automated DES grade was estimated from the regression fit and accordingly the unknown grade is calculated with the following formula: Gpred = 1.3244 log(Ndots) - 0.0612. The study has shown the viability and the utility of a highly successful automated DES diagnostic system based on OGS, which can be developed by working on the fluorescein-stained cornea images. Proper implemention of a computationally savvy and highly accurate classification system, can assist investigators to perform more objective and faster DES diagnoses in real-world scenerios. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
10,REJECTED,An Automatic Approach to Validating Log Levels in Java,2018,"Kim T., Kim S., Yoo C.-J., Cho S., Park S.","A log statement is used to record important runtime behavior of software systems for diverse reasons, which is inevitable to develop most of the software systems. However, developers do not tend to deeply consider an appropriate log level in their source code. In order to address the issues, this paper proposes an automatic approach to validating log levels in Java in consideration of the syntactic as well as semantic features. We first build up the Word2Vec model and generate semantic and syntactic log feature vectors, then train the machine learning classifiers to automatically validate the log levels. For the evaluation, we collected six open source projects of the message-oriented middleware domain, and obtained the 88% precision and the 87% recall respectively. © 2018 IEEE."
164,REJECTED,Anomaly detection and severity prediction of air leakage in train braking pipes,2017,Lee W.-J.,"Air leakage in braking pipes is a commonly encountered mechanical defect on trains. A severe air leakage will lead to braking issues and therefore decrease the reliability and cause train delays or stranding. However, air leakage is difficult to be detected via visual inspection and therefore most air leakage defects are run to fail. In this research we present a framework that not only can detect air leakages but also predicts the severity of air leakages so that action plans can be determined based on the severity level. The proposed contextual anomaly detection method detects air leakages based on the on/off logs of a compressor. Air leakage causes failure in the context when the compressor idle time is short than the compressor run time, that is, the speed of air consumption is faster than air generation. In our method the logistic regression classifier is adopted to model two different classes of compressor behavior for each train separately. The logistic regression classifier defines the boundary separating the two classes under normal situations and models the distribution of the compressor idle time and run time separately using logistic functions. The air leakage anomaly is further detected in the context that when a compressor idle time is erroneously classified as a compressor run time. To distinguish anomalies from outliers and detect anomalies based on the severity degree, a density-based clustering method with a dynamic density threshold is developed for anomaly detection. The results have demonstrated that most air leakages can be detected one to four weeks before the braking failure and therefore can be prevented in time. Most importantly, the contextual anomaly detection method can pre-filter anomaly candidates and therefore avoid generating false alarms. To facilitate the decisionmaking process, the logistic function built on the compressor run time is further used together with the duration of an air leakage to model the severity of the air leakage. By building the prediction model on the severity, the remaining useful life of the air braking pipe until it reaches a certain level of severity can be estimated. © 2017, Prognostics and Health Management Society. All rights reserved."
196,REJECTED,Development of a Method for Constructing Linguistic Standards for Multi-Criteria Assessment of Honeypot Efficiency,2021,"Korchenko A., Breslavskyi V., Yevseiev S., Zhumangalieva N., Zvarych A., Kazmirchuk S., Kurchenko O., Laptiev O., Sievierinov O., Tkachuk S.","One of the pressing areas that is developing in the field of information security is associated with the use of Honeypots (virtual decoys, online traps), and the selection of criteria for determining the most effective Honeypots and their further classification is an urgent task. The main products that implement virtual decoy technologies are presented. They are often used to study the behavior, approaches and methods that an unauthorized party uses to gain unauthorized access to information system resources. Online hooks can simulate any resource, but more often they look like real production servers and workstations. A number of fairly effective developments are known that are used to solve the problems of detecting attacks on information system resources, which are based on the apparatus of fuzzy sets. They showed the effectiveness of the appropriate mathematical apparatus, the use of which, for example, to formalize the approach to the formation of a set of reference values that will improve the process of determining the most effective Honeypots. For this purpose, many characteristics have been formed (installation and configuration process, usage and support process, data collection, logging level, simulation level, interaction level) that determine the properties of online traps. These characteristics became the basis for developing a method for the formation of standards of linguistic variables for further selection of the most effective Honeypots. The method is based on the formation of a Honeypots set, subsets of characteristics and identifier values of linguistic estimates of the Honeypot characteristics, a base and derived frequency matrix, as well as on the construction of fuzzy terms and reference fuzzy numbers with their visualization. This will allow classifying and selecting the most effective virtual baits in the future © 2021, A. Korchenko, V. Breslavskyi, S. Yevseiev, N. Zhumangalieva, A. Zvarych, S. Kazmirchuk, O. Kurchenko, O. Laptiev, O. Sievierinov, S. Tkachuk This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0)"
118,REJECTED,Hyper-parameterised dynamic regressions for nowcasting Spanish GDP growth in real time,2017,"De Antonio Liedo D., Muñoz E.F.","This paper analyses the nowcasting performance of hyperparameterised dynamic regression models with a large number of variables in log levels, and compares it with state-of-the-art methods for nowcasting.We deal with the 'curse of dimensionality' by exploiting prior information originating in the Bayesian VAR literature. The real-time forecast simulation conducted over the most severe phase of the Great Recession shows that our method yields reliable GDP predictions almost one and a half months before the official figures are published. The usefulness of our approach is confirmed in a genuine out-of-sample evaluation over the European sovereign debt crisis and subsequent recovery. Copyright © 2017 Inderscience Enterprises Ltd."
154,REJECTED,MDC-DFA: A multi-dimensional cube deterministic finite automata-based feature matching algorithm,2015,"Li Y., Luo X., Shao X., Wei D.","The regular expression matching algorithm (REM) is widely applied in the deep packet inspection(DPI), which is more flexible and efficient compared with conventional exact matching algorithm. The REM based on the deterministic finite automaton (DFA) is applicable in the high speed networks due to its linear matching speed. However, it faces the problem of the state explosion simultaneously. Therefore, we propose a multi-dimensional cube deterministic finite automata algorithm (MDC-DFA) for anomaly feature matching. The algorithm divides and compresses redundant states by each dimension, and achieves equivalent state transition by constructing dynamic nodes. Theory and simulation results show that, compared with conventional deterministic finite automata algorithm, the algorithm achieves a logarithm-level compression to the number of states and the storage space while maintaining the time complexity. © 2015 IEEE."
243,REJECTED,MOOC Performance Prediction by Deep Learning from Raw Clickstream Data,2020,"Kőrösi G., Farkas R.","Student performance prediction is a challenging problem in online education. One of the key issues relating to the quality Massive Open Online Courses (MOOC) teaching is the issue of how to foretell student performance in the future during the initial phases of education. While the fame of MOOCs has been rapidly increasing, there is a growing interest in scalable automated support technologies for student learning. Researchers have implemented numerous different Machine Learning algorithms in order to find suitable solutions to this problem. The main concept was to manually design features through cumulating daily, weekly or monthly user log data and use standard Machine Learners, like SVM, LOGREG or MLP. Deep learning algorithms could give us new opportunities, as we can apply them directly on raw input data, and we could spare the most time-consuming process of feature engineering. Based on our extensive literature survey, recent deep learning publications on MOOC sequences are based on cumulated data, i.e. on fine-engineered features. The main contribution of this paper is using raw log-line-level data as our input without any feature engineering and Recurrent Neural Networks (RNN) to predict student performance at the end of the MOOC course. We used the Stanford Lagunita’s dataset, consisting of log-level data of 130000 students and compared the RNN model based on raw data to standard classifiers using hand-crafted commulated features. The experimental results presented in this paper indicate the RNN’s dominance given its dependably superior performance as compared with the standard method. As far as we know, this will be the first work to use deep learning to predict student performance from raw log-line level students’ clickstream sequences in an online course. © 2020, Springer Nature Singapore Pte Ltd."
46,REJECTED,Nontechnical Deterrence Effects of Mild and Severe Internet Use Policy Reminders in Reducing Employee Internet Abuse,2016,"Shepherd M.M., Mejias R.J.","ABSTRACT: This two-stage longitudinal study examines how employee Internet abuse may be reduced by nontechnical deterrence methods, specifically via organizational acceptable use policies (AUPs). This study used actual employee usage and audit logs (not self-reporting survey measures) to monitor the web activity of employees. In stage 1, a mild AUP reminder sent to company employees resulted in a 12% decrease in employee Internet abuse. In stage 2, a more severe AUP reminder resulted in a 33% decrease in employee Internet abuse. For both stages, the AUP warning (regardless of severity level) resulted in an immediate and significant decrease in employee nonwork Internet use. Results indicate that the severe AUP treatment was more effective in reducing and maintaining lower levels of employee nonwork Internet use than the mild AUP treatment. Under the mild AUP treatment, employee nonwork Internet use levels returned to their pretreatment levels after only one week. However, under the severe AUP treatment, employee nonwork Internet use levels were lower than the mild AUP treatment and remained consistently lower than their pretreatment levels even after three weeks. These results suggest that nontechnical deterrence methods in the form of organizational IT use policies may constitute an effective approach to reducing employee Internet abuse, particularly if AUP policies are clear with regard to related sanctions and penalties for employee noncompliance. © 2016 Taylor & Francis."
203,REJECTED,Performance testing of super fast application,2012,"Bag A., Rodi S., Pillai K.","Performance Testing (both unit and system) is useful to understand application and sub-component(s) performance characteristics. System/Application performance testing gives insight into the overall performance of the application and is useful to debug performance issues before and during production phase. For High Performance Computing (HPC) applications, unit performance testing/proof-of-concept (POC) is very important to help choose from the different options (technology, design, data structures etc). For HPC applications, doing system performance testing becomes very complex as simulating very high input throughputs may necessitate construction of custom load injectors. Also, in HPC applications with complex workflows, simulating proper transaction mix and maintaining stable backend database size becomes complex. Monitoring different data points (latency, throughput) and debugging (log levels) is also tricky because of the overheads those incur. Unit performance testing of critical components involves complex workload and throughput modeling to determine achievable performance. This paper outlines two examples, one for POC through unit performance testing of a very high throughput application at architecture and design phase. The other is of system performance testing of a very high throughput application for SLA certification and tuning purpose."
133,REJECTED,Process Mining Algorithms for Clinical Workflow Analysis,2018,"Tibeme B., Shahriar H., Zhang C.","Process Mining focuses on the extraction of knowledge from data generated and stored by information systems. Log level data contains the signatures of executed processes. Many case studies have used process mining to discover the processes for compliance or identifying anomalies in business workflow. In this paper, we apply workflow analysis for a possible clinical setting by leveraging an open source data mining tool named ProM. We apply four available mining algorithms (Alpha, Heuristic, Inductive and Fuzzy miners) and evaluate the outputs that describe a workflow. The work provides a genesis for clinical practitioners on the advantages and disadvantages of applying various algorithms towards a dataset based on event logs. © 2018 IEEE."
74,REJECTED,Score-Based Fixing Guidance Generation With Accurate Hot-Spot Detection Method,2009,"Park Y.-H., Kim D.-H., Choi J.-H., Hong J.-S., Park C.-H., Lee S.-H., Yoo M.-H., Cho J.-D.","While predicting and removing of lithographic hot-spots are a matured practice in recent semiconductor industry, it is one of the most difficult challenges to achieve high quality detection coverage and to provide designer-friendly fixing guidance for effective physical design implementation. In this paper, we present an accurate hot-spot detection method through leveling and scoring algorithm using weighted combination of image quality parameters, i.e., normalized image log-slope (NILS), mask error enhancement factor (MEEF), and depth of focus (DOF) which can be obtained through lithography simulation. Hot-spot scoring function and severity level are calibrated with process window qualification results. Least-square regression method is used to calibrate weighting coefficients for each image quality parameter. Once scoring function is obtained with wafer results, it can be applied to various designs with the same process. Using this calibrated scoring function, we generate fixing guidance and rule for the detected hot-spot area by locating edge bias value which can lead to a hot-spot free score level. Fixing guidance is generated by considering dissections information of OPC recipe. Finally, we integrated hot-spot fixing guidance display into layout editor for the effective design implementation. Applying hot-spot scoring and fixing method to memory devices of the 50nm node and below, we could achieve a sufficient process window margin for high yield mass production. © 2009 SPIE."
214,REJECTED,Simulating fibrin clotting time,2006,Marx G.,"The clotting time (CT) of fibrinogen mixed with thrombin decreased, then increased with increasing fibrinogen levels. By contrast, log CT decreased monotonically with respect to the log level of activating enzyme (thrombin or reptilase). Here, the CT was determined over a large range of fibrinogen concentration (to 100 mg ml-1) at a fixed level of enzyme. A new parameter, [Fib]min, the minimal fibrinogen concentration required for thrombin or reptilase-instigated phase change (coagulation), was determined as [Fib]min =0.2±0.05 μM fibrinogen. A dynamic simulation program (Stella) was employed to organize simulations based on simple and complex coagulation mechanisms, which generated CT values. The successful simulation aimed at forming [Fib]minand ""recognized"" the binding of unreacted fibrinogen with intermediate fibrin protofibrils. The ""virtual data"" mimicked the biphasic experimental CT values over a wide range of concentrations. Fibrinogen appeared to act in three modalities: as a thrombin substrate; as a precursor of fibrin; and as a competitor for fibrin protofibrils. The optimized simulation may provide a basis for predicting CT in more complex systems, such as pathological plasmas or whole blood or at high concentrations encountered with fibrin sealant. © International Federation for Medical and Biological Engineering 2006."
153,REJECTED,Smart Features for Dynamic Vision Sensors,2020,"Friedel Z.P., Leishman R.C.","This paper presents a semi-supervised procedure for training a fully convolutional neural network for interest point detection and description. Contrary to previously trained networks utilized for the same purpose, our network was tailored to work with event-based imagery from a downward facing camera aboard a fixed-wing unmanned aerial vehicle. Event-based cameras are a novel type of visual sensor that operate under a unique paradigm, providing asynchronous data on the log-level changes in light intensity for individual pixels. This hardware-level approach to change detection allows these cameras to achieve ultra-wide dynamic range and high temporal resolution. The final system produces state-of-the-art repeatability and homography estimation results on an aerial event-based image dataset when compared to traditional interest point detector and descriptor algorithms. © 2020 IEEE."
134,REJECTED,Where is the user? Filtering bots from the edurep query logs,2010,Muskee W.,"Edurep indexes learning object metadata from several repositories, offering a webservice interface on which portals can build their own search implementation. At Edurep query log level, no obvious distinction can be made between human users and webcrawlers visiting these portal sites. This makes it impossible to gather any meaningful data on user search behaviour. Four query types, distinguished from the six largest portals' websites were related to one month of query logs. For two query types a distinction between human and automatic generated traffic could be found. However, these results can only be used to advise connected portals on their interface implementations. More research is needed to actually perform any reliable filtering."
13,REJECTED,Which log level should developers choose for a new logging statement? (journal-first abstract),2018,"Li H., Shang W., Hassan A.E.","This is an extended abstract of a paper published in the Empirical Software Engineering journal. The original paper is communicated by Mark Grechanik. The paper empirically studied how developers assign log levels to their logging statements and proposed an automated approach to help developers determine the most appropriate log level when they add a new logging statement. We analyzed the development history of four open source projects (Hadoop, Directory Server, Hama, and Qpid). We found that our automated approach can accurately suggest the levels of logging statements with an AUC of 0.75 to 0.81. We also found that the characteristics of the containing block of a newly-added logging statement, the existing logging statements in the containing source code file, and the content of the newly-added logging statement play important roles in determining the appropriate log level for that logging statement. © 2018 IEEE."